{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rjuQY9f2mdS"
   },
   "source": [
    "## 과제 1\n",
    "ReLu activation function과 derivative function을 구현해보세요\n",
    "- Hint : np.maximum 함수 사용하면 편리합니다\n",
    "- 다른 방법 사용하셔도 무방합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "puH0YVGI2uLz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def relu(x):\n",
    "    return np.maximum(x,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Esm4jmTVijro"
   },
   "outputs": [],
   "source": [
    "def d_relu(x):\n",
    "    temp=0\n",
    "    if x<0 :\n",
    "        temp=0\n",
    "    else :\n",
    "        temp =x\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz8Hi0Rc2-yJ"
   },
   "source": [
    "## 과제 2\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
    "- Hint : 코드 파일의 예시는 Two layer MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fusEy49j3uhs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[43mhidden_size\u001b[49m, x_size) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m x_size),\n\u001b[0;32m      2\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((hidden_size, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m x_size),\n\u001b[0;32m      3\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(num_class, hidden_size) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m hidden_size),\n\u001b[0;32m      4\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((num_class, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m hidden_size)\n\u001b[0;32m      5\u001b[0m           }\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_pass\u001b[39m(x, y_true, params):\n\u001b[0;32m      9\u001b[0m   dS2 \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m y_true\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hidden_size' is not defined"
     ]
    }
   ],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "\n",
    "  dS3 = params[\"A3\"] - y_true\n",
    "  # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
    "  # dS2 is softmax + CE loss derivative\n",
    "\n",
    "  grads = {}\n",
    "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]  \n",
    "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
    "  dS2 = dA2 * d_sigmoid(params[\"S2\"])    \n",
    "    \n",
    "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
    "\n",
    "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twf-R8s-34zT"
   },
   "source": [
    "## 과제 3\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
    "\n",
    "hyperparameter는 다음과 같이 설정\n",
    "\n",
    "- epochs : 100\n",
    "- hiddensize : 128, 64 (two layer)\n",
    "- learning_rate : 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "bxJO249A3jhk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\jocha\\anaconda3\\lib\\site-packages (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Assignment 3 구현은 여기서 ()\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jocha\\AppData\\Local\\Temp\\ipykernel_18464\\1472909150.py\", line 1, in <cell line: 1>\n",
      "    from torchvision import transforms, datasets\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\torchvision\\__init__.py\", line 5, in <module>\n",
      "    from torchvision import datasets, io, models, ops, transforms, utils\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\__init__.py\", line 1, in <module>\n",
      "    from ._optical_flow import FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\_optical_flow.py\", line 12, in <module>\n",
      "    from .utils import _read_pfm, verify_str_arg\n",
      "ImportError: cannot import name '_read_pfm' from 'torchvision.datasets.utils' (C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\utils.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1982, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\jocha\\anaconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor()\n\u001b[0;32m      3\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[0;32m      2\u001b[0m     root      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./.data/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m     train     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     download  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m testset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\n\u001b[0;32m      8\u001b[0m     root      \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./.data/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      9\u001b[0m     train     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m     download  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = True,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = False,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
    "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mNet\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Net, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(784,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mNet\u001b[49m()\n\u001b[0;32m      2\u001b[0m model\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Net' is not defined"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n",
    "                         # require_true 얘는 학습되는 애구나 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    # 배치 당 loss 값을 담을 리스트 생성\n",
    "    batch_losses = []\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        # 옵티마이저의 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # y pred 값 산출\n",
    "        output = model(data)\n",
    "        # loss 계산\n",
    "        # 정답 데이터와의 cross entropy loss 계산\n",
    "        # 이 loss를 배치 당 loss로 보관\n",
    "        loss = criterion(output, target)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "        # 기울기 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 가중치 업데이트!\n",
    "        optimizer.step()\n",
    "        \n",
    "    # 배치당 평균 loss 계산\n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    # 모델을 평가 모드로 전환\n",
    "    model.eval()\n",
    "\n",
    "    batch_losses = []\n",
    "    correct = 0 \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            # 예측값 생성\n",
    "            output = model(data)\n",
    "\n",
    "            # loss 계산 (이전과 동일)\n",
    "            loss = criterion(output, target)\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "           # Accuracy 계산\n",
    "           # y pred와 y가 일치하면 correct에 1을 더해주기\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "\n",
    "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # 배치 당 평균 loss 계산 \n",
    "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
    "\n",
    "    #정확도 계산\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
    "          epoch, train_loss, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jocha\\AppData\\Local\\Temp\\ipykernel_18464\\3700766821.py:2: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  get_ipython().magic('reset -sf')\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "num_train = 60000\n",
    "num_class = 10\n",
    "\n",
    "x_train = np.float32(mnist.data[:num_train]).T\n",
    "y_train_index = np.int32(mnist.target[:num_train]).T\n",
    "x_test = np.float32(mnist.data[num_train:]).T\n",
    "y_test_index = np.int32(mnist.target[num_train:]).T\n",
    "\n",
    "# Normalization\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_size = x_train.shape[0]\n",
    "\n",
    "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
    "for idx in range(y_train_index.shape[0]):\n",
    "  y_train[y_train_index[idx], idx] = 1\n",
    "\n",
    "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
    "for idx in range(y_test_index.shape[0]):\n",
    "  y_test[y_test_index[idx], idx] = 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter initialization\n",
    "\n",
    "hidden_size1 = 128 # hidden unit size\n",
    "hidden_size2 = 64\n",
    "# two-layer neural network\n",
    "\n",
    "params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n",
    "          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/ x_size),\n",
    "          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/ hidden_size1),\n",
    "          \"b2\": np.zeros((hidden_size1, 1)) * np.sqrt(1/ hidden_size2)\n",
    "          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n",
    "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n",
    "          }\n",
    "# Xavier initialization: https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "  # derivative of sigmoid\n",
    "  exp = np.exp(-x)\n",
    "  return (exp)/((1+exp)**2)\n",
    "\n",
    "def softmax(x):\n",
    "  exp = np.exp(x)\n",
    "  return exp/np.sum(exp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "  # loss calculation\n",
    "\n",
    "  num_sample = y_true.shape[1]\n",
    "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
    "  \n",
    "  return Li/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(x, params):\n",
    "  \n",
    "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
    "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
    "  params[\"A2\"] = softmax(params[\"S2\"])\n",
    "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
    "  params[\"A3\"] = softmax(params[\"S3\"])  \n",
    "    \n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass_test(x, params):\n",
    "\n",
    "  params_test = {}\n",
    "  \n",
    "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
    "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
    "  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
    "\n",
    "  return params_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "  y_true_idx = np.argmax(y_true, axis = 0)\n",
    "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
    "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
    "\n",
    "  accuracy = num_correct / y_true.shape[1] * 100\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "\n",
    "  dS3 = params[\"A3\"] - y_true\n",
    "  # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
    "  # dS2 is softmax + CE loss derivative\n",
    "\n",
    "  grads = {}\n",
    "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]  \n",
    "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
    "  dS2 = dA2 * d_sigmoid(params[\"S2\"])    \n",
    "    \n",
    "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
    "\n",
    "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  if i == 0:\n",
    "    params = foward_pass(x_train, params)\n",
    "    \n",
    "  grads = backward_pass(x_train, y_train, params)\n",
    "\n",
    "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
    "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
    "\n",
    "\n",
    "  params = foward_pass(x_train, params)\n",
    "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
    "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
    "\n",
    "  params_test = foward_pass_test(x_test, params)\n",
    "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
    "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
    "\n",
    "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
    "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaqqRzF73oBu"
   },
   "source": [
    "## 과제 4\n",
    "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
    "\n",
    "- Hint : Activation function, hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6b82DZG6W3j"
   },
   "outputs": [],
   "source": [
    "미제출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pboMIBQq7onH"
   },
   "source": [
    "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux7mPf6E78d4"
   },
   "outputs": [],
   "source": [
    "며칠 개념서를 보고 공부해 보고, 코드 구성을 나름 생각한대로 고쳐보긴 했지만 \n",
    "설치 과정에서 오류로(transforms 인식이 안됨) 실 구동을 못해봤습니다.\n",
    "하이퍼 파라미터 값 중 학습률, 미니 배치 크기 등의 조정을 해보는 것이 \n",
    "성능 향상의 주안점이 되리라 생각합니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[0818] Deep Learning Basic_과제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
