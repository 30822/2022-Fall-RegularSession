{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0818]_Deep_Learning_Basic_과제_8기_도승범.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3639d156f0464ea6ab57d09ae725d7ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18791f3bb6f4408a9d890dbc9995b8dc","IPY_MODEL_aa6dbe6e10ac4253b9bc229c8b3e751e","IPY_MODEL_7c9e2c6a233b4a5f96376678aecbed3c"],"layout":"IPY_MODEL_d4cc475595c646e0be732c131caa435a","tabbable":null,"tooltip":null}},"18791f3bb6f4408a9d890dbc9995b8dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5fa4d87c44bd4d409c81b602db2479d4","placeholder":"​","style":"IPY_MODEL_7e834a691d724eb4ba7ef75bf7973a48","tabbable":null,"tooltip":null,"value":"100%"}},"aa6dbe6e10ac4253b9bc229c8b3e751e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_0817fc4714c642038ed5c7ea8dad6e73","max":9912422,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4e1a5979b2014004b68c42146b21c921","tabbable":null,"tooltip":null,"value":9912422}},"7c9e2c6a233b4a5f96376678aecbed3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_9e5544ed1dec42f89777fccc76096008","placeholder":"​","style":"IPY_MODEL_9d6f661f901749fd8aabbcd7996a60c3","tabbable":null,"tooltip":null,"value":" 9912422/9912422 [00:00&lt;00:00, 29386974.40it/s]"}},"d4cc475595c646e0be732c131caa435a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fa4d87c44bd4d409c81b602db2479d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e834a691d724eb4ba7ef75bf7973a48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0817fc4714c642038ed5c7ea8dad6e73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e1a5979b2014004b68c42146b21c921":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e5544ed1dec42f89777fccc76096008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d6f661f901749fd8aabbcd7996a60c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8f64d1baae1f4336b4f15c171d3a896d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e7d7e90663b49f897dbe6fcd0c80df0","IPY_MODEL_88738bfa10df45e1babe1fca6602802a","IPY_MODEL_e72717fe58604d3cac389173feebef74"],"layout":"IPY_MODEL_edaee299ca2d4893966adc1ec3738fa7","tabbable":null,"tooltip":null}},"2e7d7e90663b49f897dbe6fcd0c80df0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_1a432f040f5143e4925e3538daed27a3","placeholder":"​","style":"IPY_MODEL_7af7a877e4154fc28553326969b01699","tabbable":null,"tooltip":null,"value":"100%"}},"88738bfa10df45e1babe1fca6602802a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_9c155099f66843209cdc8863a42fd52b","max":28881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48aac96209634fc7bc126a1da2df7584","tabbable":null,"tooltip":null,"value":28881}},"e72717fe58604d3cac389173feebef74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_4a08bc98e43048379e483d7497b19127","placeholder":"​","style":"IPY_MODEL_20ea656bb04a43b3b35fa45b8f00a18e","tabbable":null,"tooltip":null,"value":" 28881/28881 [00:00&lt;00:00, 211942.43it/s]"}},"edaee299ca2d4893966adc1ec3738fa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a432f040f5143e4925e3538daed27a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7af7a877e4154fc28553326969b01699":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9c155099f66843209cdc8863a42fd52b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48aac96209634fc7bc126a1da2df7584":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a08bc98e43048379e483d7497b19127":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20ea656bb04a43b3b35fa45b8f00a18e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"03c0c97c77104155832b71c814f112b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b58489e4cf74b4da6aff67b0e3dc31d","IPY_MODEL_f1d515a5a1f24ad7a920013797210539","IPY_MODEL_e4cb850e554243f5b398526164623e5a"],"layout":"IPY_MODEL_8dc186318be445d1acbaca5758c32695","tabbable":null,"tooltip":null}},"9b58489e4cf74b4da6aff67b0e3dc31d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_048d8a1ba984412fb2f35a9db4e48e75","placeholder":"​","style":"IPY_MODEL_838776a1aea4460f852d8162c4f674ac","tabbable":null,"tooltip":null,"value":"100%"}},"f1d515a5a1f24ad7a920013797210539":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_dd9846f020404e28bd6db442c248422b","max":1648877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_671a42d873c3451bb1857684ac093c94","tabbable":null,"tooltip":null,"value":1648877}},"e4cb850e554243f5b398526164623e5a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_86b90ce128544e4e9e49c2bf0d982c0f","placeholder":"​","style":"IPY_MODEL_a7f9df9dbe85432693381d8b3c7539b7","tabbable":null,"tooltip":null,"value":" 1648877/1648877 [00:00&lt;00:00, 1200707.23it/s]"}},"8dc186318be445d1acbaca5758c32695":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"048d8a1ba984412fb2f35a9db4e48e75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"838776a1aea4460f852d8162c4f674ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"dd9846f020404e28bd6db442c248422b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"671a42d873c3451bb1857684ac093c94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86b90ce128544e4e9e49c2bf0d982c0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f9df9dbe85432693381d8b3c7539b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8a96f19f1f5a4276b9292d6f0bccf950":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f6c3589aaba46e9ab0276606ea59830","IPY_MODEL_43af43c20b0c47c485b9eac92d92d83a","IPY_MODEL_3a331df16fd54d94be4ab3461bcf0a3a"],"layout":"IPY_MODEL_f301d10c56f4430c93381d5a8b7a38f9","tabbable":null,"tooltip":null}},"5f6c3589aaba46e9ab0276606ea59830":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_13a533746a494d1d810f54d2611f6e46","placeholder":"​","style":"IPY_MODEL_eb4cf59f67d1403e9510466939564337","tabbable":null,"tooltip":null,"value":"100%"}},"43af43c20b0c47c485b9eac92d92d83a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_399fed8b53594d54a518ca6b129b31dc","max":4542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b06984eecb54e509630b445fa515115","tabbable":null,"tooltip":null,"value":4542}},"3a331df16fd54d94be4ab3461bcf0a3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"2.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_2cc485f09bcd44ce91ef598acaf4f5b2","placeholder":"​","style":"IPY_MODEL_492f32f6ccd046f5be6cdc0e88575d50","tabbable":null,"tooltip":null,"value":" 4542/4542 [00:00&lt;00:00, 55126.57it/s]"}},"f301d10c56f4430c93381d5a8b7a38f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13a533746a494d1d810f54d2611f6e46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb4cf59f67d1403e9510466939564337":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"399fed8b53594d54a518ca6b129b31dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b06984eecb54e509630b445fa515115":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2cc485f09bcd44ce91ef598acaf4f5b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"492f32f6ccd046f5be6cdc0e88575d50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLStyleModel","model_module_version":"2.0.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660912364957,"user_tz":-540,"elapsed":8,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"e5e5eec4-b169-4cf8-f3b5-2cc09f4377f3"},"source":["import numpy as np\n","\n","def relu(x):\n","\n","  output = np.maximum(x, 0)\n","\n","  return output\n","\n","print(relu(5))\n","print(relu(-4))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","0\n"]}]},{"cell_type":"code","source":["def d_relu(x):\n","\n","  output = 0\n","\n","  if x >= 0:\n","    output = 1\n","  else:\n","    pass\n","\n","  return output\n","\n","print(d_relu(2.5))\n","print(d_relu(-3))"],"metadata":{"id":"Esm4jmTVijro","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660912633818,"user_tz":-540,"elapsed":325,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"f4cd70af-bf53-4b0e-ad91-bc4b8c7e7853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","0\n"]}]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","source":["from IPython import get_ipython\n","get_ipython().magic('reset -sf')"],"metadata":{"id":"gmo1glCpQoGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"],"metadata":{"id":"n4hnLTlTQugm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data preprocessing\n","\n","num_train = 60000 #Train 샘플의 크기\n","num_class = 10 #label이 가질 수 있는 클래스의 수\n","\n","x_train = np.float32(mnist.data[:num_train]).T \n","y_train_index = np.int32(mnist.target[:num_train]).T \n","x_test = np.float32(mnist.data[num_train:]).T \n","y_test_index = np.int32(mnist.target[num_train:]).T \n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1    "],"metadata":{"id":"4MggeQ5FQv1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"id":"weIXxzK-Qw0M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660915358495,"user_tz":-540,"elapsed":9,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"3de5fedc-56c8-46f8-cfd8-1097d46c64db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784, 60000)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden_size1 = 64 # hidden unit size\n","hidden_size2 = 32 # hidden unit size\n","\n","# two-layer neural network\n","\n","params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/ hidden_size1),\n","          \"b2\": np.zeros((hidden_size2, 1)) * np.sqrt(1/ x_size),\n","          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n","          }\n","# Xavier initialization: https://reniew.github.io/13/"],"metadata":{"id":"9L6daoh_QxO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def d_sigmoid(x):\n","  # derivative of sigmoid\n","  exp = np.exp(-x)\n","  return (exp)/((1+exp)**2)\n","\n","def softmax(x):\n","  exp = np.exp(x)\n","  return exp/np.sum(exp, axis=0)"],"metadata":{"id":"KCTXiSU3Qyqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_loss(y_true, y_pred):\n","  # loss calculation\n","\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"metadata":{"id":"CDFcWEkhQz53"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = sigmoid(params[\"S1\"])\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = sigmoid(params[\"S2\"])\n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = softmax(params[\"S3\"])\n","\n","  return params"],"metadata":{"id":"YvVCUZKNQ1oV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def foward_pass_test(x, params):\n","\n","  params_test = {}\n","\n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = sigmoid(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n","\n","  return params_test"],"metadata":{"id":"dUbAjGqGQ2J7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"metadata":{"id":"R6fm4MzAQ3cF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def backward_pass(x, y_true, params):\n","\n","  dS3 = params[\"A3\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads\n"],"metadata":{"id":"yoMWVMr6Q7Kl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.5\n","\n","for i in range(epochs):\n","\n","  if i == 0:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A3\"])\n","  train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A3\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, test acuracy = {}%\"\n","  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"metadata":{"id":"spjWjDrvQ8yN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660922342371,"user_tz":-540,"elapsed":120842,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"6f8c929b-1a14-4e55-8e70-10616f867eb4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = 2.371842, training acuracy = 9.9%, test loss = 2.375679, training acuracy = 10.27%\n","Epoch 2: training loss = 2.332802, training acuracy = 11.24%, test loss = 2.33546, training acuracy = 11.36%\n","Epoch 3: training loss = 2.313462, training acuracy = 11.24%, test loss = 2.315108, training acuracy = 11.35%\n","Epoch 4: training loss = 2.303396, training acuracy = 11.24%, test loss = 2.304251, training acuracy = 11.35%\n","Epoch 5: training loss = 2.298107, training acuracy = 11.24%, test loss = 2.298388, training acuracy = 11.35%\n","Epoch 6: training loss = 2.295128, training acuracy = 11.24%, test loss = 2.295014, training acuracy = 11.35%\n","Epoch 7: training loss = 2.293149, training acuracy = 11.24%, test loss = 2.292772, training acuracy = 11.35%\n","Epoch 8: training loss = 2.29156, training acuracy = 11.24%, test loss = 2.291008, training acuracy = 11.35%\n","Epoch 9: training loss = 2.290109, training acuracy = 11.25%, test loss = 2.289435, training acuracy = 11.35%\n","Epoch 10: training loss = 2.288698, training acuracy = 11.26%, test loss = 2.287935, training acuracy = 11.37%\n","Epoch 11: training loss = 2.287292, training acuracy = 11.27%, test loss = 2.286459, training acuracy = 11.37%\n","Epoch 12: training loss = 2.285879, training acuracy = 11.28%, test loss = 2.284985, training acuracy = 11.4%\n","Epoch 13: training loss = 2.284452, training acuracy = 11.31%, test loss = 2.283503, training acuracy = 11.44%\n","Epoch 14: training loss = 2.283008, training acuracy = 11.38%, test loss = 2.282007, training acuracy = 11.52%\n","Epoch 15: training loss = 2.281544, training acuracy = 11.48%, test loss = 2.280493, training acuracy = 11.61%\n","Epoch 16: training loss = 2.280058, training acuracy = 11.63%, test loss = 2.278958, training acuracy = 11.79%\n","Epoch 17: training loss = 2.278549, training acuracy = 11.78%, test loss = 2.277398, training acuracy = 12.0%\n","Epoch 18: training loss = 2.277013, training acuracy = 12.02%, test loss = 2.275812, training acuracy = 12.34%\n","Epoch 19: training loss = 2.275449, training acuracy = 12.28%, test loss = 2.274197, training acuracy = 12.8%\n","Epoch 20: training loss = 2.273855, training acuracy = 12.6%, test loss = 2.27255, training acuracy = 13.23%\n","Epoch 21: training loss = 2.272227, training acuracy = 12.98%, test loss = 2.270868, training acuracy = 13.68%\n","Epoch 22: training loss = 2.270564, training acuracy = 13.44%, test loss = 2.269151, training acuracy = 14.2%\n","Epoch 23: training loss = 2.268863, training acuracy = 13.96%, test loss = 2.267394, training acuracy = 14.89%\n","Epoch 24: training loss = 2.267121, training acuracy = 14.61%, test loss = 2.265595, training acuracy = 15.7%\n","Epoch 25: training loss = 2.265336, training acuracy = 15.31%, test loss = 2.263751, training acuracy = 16.4%\n","Epoch 26: training loss = 2.263505, training acuracy = 16.03%, test loss = 2.261859, training acuracy = 17.44%\n","Epoch 27: training loss = 2.261625, training acuracy = 16.86%, test loss = 2.259917, training acuracy = 18.5%\n","Epoch 28: training loss = 2.259693, training acuracy = 17.82%, test loss = 2.257921, training acuracy = 19.38%\n","Epoch 29: training loss = 2.257705, training acuracy = 18.89%, test loss = 2.255868, training acuracy = 20.54%\n","Epoch 30: training loss = 2.255659, training acuracy = 19.92%, test loss = 2.253755, training acuracy = 21.83%\n","Epoch 31: training loss = 2.253552, training acuracy = 21.01%, test loss = 2.251578, training acuracy = 22.99%\n","Epoch 32: training loss = 2.251379, training acuracy = 22.2%, test loss = 2.249333, training acuracy = 24.29%\n","Epoch 33: training loss = 2.249136, training acuracy = 23.45%, test loss = 2.247017, training acuracy = 25.64%\n","Epoch 34: training loss = 2.246821, training acuracy = 24.66%, test loss = 2.244625, training acuracy = 27.03%\n","Epoch 35: training loss = 2.244429, training acuracy = 25.95%, test loss = 2.242154, training acuracy = 28.37%\n","Epoch 36: training loss = 2.241955, training acuracy = 27.22%, test loss = 2.239599, training acuracy = 29.57%\n","Epoch 37: training loss = 2.239396, training acuracy = 28.48%, test loss = 2.236955, training acuracy = 30.81%\n","Epoch 38: training loss = 2.236747, training acuracy = 29.65%, test loss = 2.234219, training acuracy = 32.19%\n","Epoch 39: training loss = 2.234003, training acuracy = 30.83%, test loss = 2.231384, training acuracy = 33.35%\n","Epoch 40: training loss = 2.23116, training acuracy = 31.96%, test loss = 2.228447, training acuracy = 34.38%\n","Epoch 41: training loss = 2.228211, training acuracy = 33.16%, test loss = 2.225402, training acuracy = 35.59%\n","Epoch 42: training loss = 2.225152, training acuracy = 34.31%, test loss = 2.222242, training acuracy = 36.68%\n","Epoch 43: training loss = 2.221978, training acuracy = 35.31%, test loss = 2.218964, training acuracy = 37.78%\n","Epoch 44: training loss = 2.218682, training acuracy = 36.32%, test loss = 2.21556, training acuracy = 38.64%\n","Epoch 45: training loss = 2.215258, training acuracy = 37.35%, test loss = 2.212024, training acuracy = 39.46%\n","Epoch 46: training loss = 2.211701, training acuracy = 38.21%, test loss = 2.208351, training acuracy = 40.21%\n","Epoch 47: training loss = 2.208003, training acuracy = 39.12%, test loss = 2.204533, training acuracy = 40.98%\n","Epoch 48: training loss = 2.204159, training acuracy = 39.92%, test loss = 2.200564, training acuracy = 41.66%\n","Epoch 49: training loss = 2.200162, training acuracy = 40.75%, test loss = 2.196438, training acuracy = 42.67%\n","Epoch 50: training loss = 2.196005, training acuracy = 41.49%, test loss = 2.192146, training acuracy = 43.34%\n","Epoch 51: training loss = 2.19168, training acuracy = 42.31%, test loss = 2.187682, training acuracy = 44.03%\n","Epoch 52: training loss = 2.187181, training acuracy = 43.0%, test loss = 2.183038, training acuracy = 44.81%\n","Epoch 53: training loss = 2.182499, training acuracy = 43.73%, test loss = 2.178207, training acuracy = 45.46%\n","Epoch 54: training loss = 2.177629, training acuracy = 44.34%, test loss = 2.173182, training acuracy = 45.83%\n","Epoch 55: training loss = 2.172561, training acuracy = 44.91%, test loss = 2.167954, training acuracy = 46.38%\n","Epoch 56: training loss = 2.16729, training acuracy = 45.47%, test loss = 2.162515, training acuracy = 46.74%\n","Epoch 57: training loss = 2.161806, training acuracy = 45.98%, test loss = 2.15686, training acuracy = 47.22%\n","Epoch 58: training loss = 2.156104, training acuracy = 46.5%, test loss = 2.150979, training acuracy = 47.57%\n","Epoch 59: training loss = 2.150175, training acuracy = 47.01%, test loss = 2.144866, training acuracy = 48.07%\n","Epoch 60: training loss = 2.144014, training acuracy = 47.48%, test loss = 2.138514, training acuracy = 48.77%\n","Epoch 61: training loss = 2.137612, training acuracy = 47.96%, test loss = 2.131915, training acuracy = 49.15%\n","Epoch 62: training loss = 2.130964, training acuracy = 48.32%, test loss = 2.125064, training acuracy = 49.52%\n","Epoch 63: training loss = 2.124064, training acuracy = 48.76%, test loss = 2.117955, training acuracy = 49.82%\n","Epoch 64: training loss = 2.116907, training acuracy = 49.16%, test loss = 2.110582, training acuracy = 50.17%\n","Epoch 65: training loss = 2.109487, training acuracy = 49.58%, test loss = 2.102941, training acuracy = 50.52%\n","Epoch 66: training loss = 2.101802, training acuracy = 49.94%, test loss = 2.095027, training acuracy = 50.92%\n","Epoch 67: training loss = 2.093847, training acuracy = 50.29%, test loss = 2.086838, training acuracy = 51.26%\n","Epoch 68: training loss = 2.085621, training acuracy = 50.69%, test loss = 2.078372, training acuracy = 51.59%\n","Epoch 69: training loss = 2.077122, training acuracy = 51.0%, test loss = 2.069627, training acuracy = 51.97%\n","Epoch 70: training loss = 2.06835, training acuracy = 51.34%, test loss = 2.060604, training acuracy = 52.23%\n","Epoch 71: training loss = 2.059306, training acuracy = 51.68%, test loss = 2.051302, training acuracy = 52.65%\n","Epoch 72: training loss = 2.049991, training acuracy = 52.01%, test loss = 2.041726, training acuracy = 52.92%\n","Epoch 73: training loss = 2.040409, training acuracy = 52.35%, test loss = 2.031877, training acuracy = 53.22%\n","Epoch 74: training loss = 2.030563, training acuracy = 52.68%, test loss = 2.02176, training acuracy = 53.32%\n","Epoch 75: training loss = 2.020459, training acuracy = 52.98%, test loss = 2.011382, training acuracy = 53.57%\n","Epoch 76: training loss = 2.010104, training acuracy = 53.3%, test loss = 2.000748, training acuracy = 53.91%\n","Epoch 77: training loss = 1.999503, training acuracy = 53.56%, test loss = 1.989865, training acuracy = 54.25%\n","Epoch 78: training loss = 1.988665, training acuracy = 53.84%, test loss = 1.978744, training acuracy = 54.64%\n","Epoch 79: training loss = 1.9776, training acuracy = 54.14%, test loss = 1.967392, training acuracy = 55.05%\n","Epoch 80: training loss = 1.966315, training acuracy = 54.49%, test loss = 1.955819, training acuracy = 55.34%\n","Epoch 81: training loss = 1.954821, training acuracy = 54.77%, test loss = 1.944036, training acuracy = 55.66%\n","Epoch 82: training loss = 1.943128, training acuracy = 55.07%, test loss = 1.932052, training acuracy = 56.03%\n","Epoch 83: training loss = 1.931246, training acuracy = 55.41%, test loss = 1.91988, training acuracy = 56.41%\n","Epoch 84: training loss = 1.919184, training acuracy = 55.72%, test loss = 1.907529, training acuracy = 56.72%\n","Epoch 85: training loss = 1.906954, training acuracy = 56.06%, test loss = 1.895009, training acuracy = 56.94%\n","Epoch 86: training loss = 1.894566, training acuracy = 56.38%, test loss = 1.882332, training acuracy = 57.37%\n","Epoch 87: training loss = 1.882027, training acuracy = 56.69%, test loss = 1.869507, training acuracy = 57.76%\n","Epoch 88: training loss = 1.869348, training acuracy = 57.0%, test loss = 1.856544, training acuracy = 58.04%\n","Epoch 89: training loss = 1.856538, training acuracy = 57.33%, test loss = 1.843451, training acuracy = 58.31%\n","Epoch 90: training loss = 1.843604, training acuracy = 57.63%, test loss = 1.830238, training acuracy = 58.52%\n","Epoch 91: training loss = 1.830555, training acuracy = 57.94%, test loss = 1.816913, training acuracy = 58.83%\n","Epoch 92: training loss = 1.817397, training acuracy = 58.23%, test loss = 1.803484, training acuracy = 59.08%\n","Epoch 93: training loss = 1.804139, training acuracy = 58.54%, test loss = 1.789958, training acuracy = 59.29%\n","Epoch 94: training loss = 1.790788, training acuracy = 58.8%, test loss = 1.776343, training acuracy = 59.55%\n","Epoch 95: training loss = 1.777349, training acuracy = 59.06%, test loss = 1.762646, training acuracy = 59.79%\n","Epoch 96: training loss = 1.763829, training acuracy = 59.33%, test loss = 1.748875, training acuracy = 60.22%\n","Epoch 97: training loss = 1.750236, training acuracy = 59.59%, test loss = 1.735035, training acuracy = 60.49%\n","Epoch 98: training loss = 1.736575, training acuracy = 59.9%, test loss = 1.721135, training acuracy = 60.85%\n","Epoch 99: training loss = 1.722854, training acuracy = 60.15%, test loss = 1.707181, training acuracy = 61.15%\n","Epoch 100: training loss = 1.709079, training acuracy = 60.46%, test loss = 1.693181, training acuracy = 61.42%\n"]}]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","source":["from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"aYMJi8AoPwhz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지를 텐서로 변경\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"6PI2vub8PyK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = True,\n","    download  = True,\n","    transform = transform\n",")\n","testset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = False,\n","    download  = True,\n","    transform = transform\n",")"],"metadata":{"id":"b_AedUXiPzEL","colab":{"base_uri":"https://localhost:8080/","height":311,"referenced_widgets":["3639d156f0464ea6ab57d09ae725d7ce","18791f3bb6f4408a9d890dbc9995b8dc","aa6dbe6e10ac4253b9bc229c8b3e751e","7c9e2c6a233b4a5f96376678aecbed3c","d4cc475595c646e0be732c131caa435a","5fa4d87c44bd4d409c81b602db2479d4","7e834a691d724eb4ba7ef75bf7973a48","0817fc4714c642038ed5c7ea8dad6e73","4e1a5979b2014004b68c42146b21c921","9e5544ed1dec42f89777fccc76096008","9d6f661f901749fd8aabbcd7996a60c3","8f64d1baae1f4336b4f15c171d3a896d","2e7d7e90663b49f897dbe6fcd0c80df0","88738bfa10df45e1babe1fca6602802a","e72717fe58604d3cac389173feebef74","edaee299ca2d4893966adc1ec3738fa7","1a432f040f5143e4925e3538daed27a3","7af7a877e4154fc28553326969b01699","9c155099f66843209cdc8863a42fd52b","48aac96209634fc7bc126a1da2df7584","4a08bc98e43048379e483d7497b19127","20ea656bb04a43b3b35fa45b8f00a18e","03c0c97c77104155832b71c814f112b4","9b58489e4cf74b4da6aff67b0e3dc31d","f1d515a5a1f24ad7a920013797210539","e4cb850e554243f5b398526164623e5a","8dc186318be445d1acbaca5758c32695","048d8a1ba984412fb2f35a9db4e48e75","838776a1aea4460f852d8162c4f674ac","dd9846f020404e28bd6db442c248422b","671a42d873c3451bb1857684ac093c94","86b90ce128544e4e9e49c2bf0d982c0f","a7f9df9dbe85432693381d8b3c7539b7","8a96f19f1f5a4276b9292d6f0bccf950","5f6c3589aaba46e9ab0276606ea59830","43af43c20b0c47c485b9eac92d92d83a","3a331df16fd54d94be4ab3461bcf0a3a","f301d10c56f4430c93381d5a8b7a38f9","13a533746a494d1d810f54d2611f6e46","eb4cf59f67d1403e9510466939564337","399fed8b53594d54a518ca6b129b31dc","5b06984eecb54e509630b445fa515115","2cc485f09bcd44ce91ef598acaf4f5b2","492f32f6ccd046f5be6cdc0e88575d50"]},"executionInfo":{"status":"ok","timestamp":1660996902566,"user_tz":-540,"elapsed":2629,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"ed281d65-edaa-46f6-8891-09125e0e0e45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3639d156f0464ea6ab57d09ae725d7ce"},"application/json":{"n":0,"total":9912422,"elapsed":0.0654447078704834,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./.data/MNIST/raw/train-images-idx3-ubyte.gz to ./.data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f64d1baae1f4336b4f15c171d3a896d"},"application/json":{"n":0,"total":28881,"elapsed":0.09169864654541016,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./.data/MNIST/raw/train-labels-idx1-ubyte.gz to ./.data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c0c97c77104155832b71c814f112b4"},"application/json":{"n":0,"total":1648877,"elapsed":0.08716320991516113,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./.data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./.data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a96f19f1f5a4276b9292d6f0bccf950"},"application/json":{"n":0,"total":4542,"elapsed":0.07119178771972656,"ncols":null,"nrows":null,"prefix":"","ascii":false,"unit":"it","unit_scale":false,"rate":null,"bar_format":null,"postfix":null,"unit_divisor":1000,"initial":0,"colour":null}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./.data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./.data/MNIST/raw\n","\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 512\n","# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n","# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n","train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"r-L0KlKDP1Xt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"0Bq1HKFOP4Js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"id":"m6TQ_Z6LP6OG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660996902569,"user_tz":-540,"elapsed":19,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"de535174-067c-479c-d260-1a641a7bf767"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n","                         # require_true 얘는 학습되는 애구나 알 수 있음"],"metadata":{"id":"Usu7zwKcP8h1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660996903731,"user_tz":-540,"elapsed":9,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"3e862132-69d4-4bca-8d8d-21818dd55c48"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[-0.0354,  0.0022, -0.0296,  ...,  0.0228,  0.0289, -0.0352],\n","         [-0.0044, -0.0308, -0.0116,  ...,  0.0316,  0.0183,  0.0253],\n","         [-0.0095, -0.0296, -0.0299,  ..., -0.0045, -0.0122, -0.0325],\n","         ...,\n","         [-0.0030,  0.0301, -0.0337,  ...,  0.0073, -0.0335, -0.0143],\n","         [-0.0311, -0.0218, -0.0331,  ..., -0.0018, -0.0137,  0.0016],\n","         [ 0.0138, -0.0274,  0.0111,  ..., -0.0340,  0.0242,  0.0053]],\n","        requires_grad=True), Parameter containing:\n"," tensor([ 0.0150, -0.0297,  0.0307, -0.0355,  0.0235,  0.0296, -0.0064, -0.0091,\n","         -0.0252,  0.0179,  0.0195, -0.0248,  0.0078,  0.0186,  0.0029,  0.0137,\n","          0.0149,  0.0276,  0.0241, -0.0091, -0.0046, -0.0202,  0.0196,  0.0334,\n","         -0.0125, -0.0176, -0.0035,  0.0132, -0.0124,  0.0223, -0.0066, -0.0051,\n","          0.0249, -0.0308, -0.0110,  0.0236,  0.0155, -0.0255, -0.0011, -0.0256,\n","          0.0333,  0.0284, -0.0107, -0.0250,  0.0036,  0.0170,  0.0296,  0.0341,\n","         -0.0282, -0.0267,  0.0134,  0.0192,  0.0308, -0.0274, -0.0119,  0.0284,\n","         -0.0071,  0.0163, -0.0330,  0.0096,  0.0081, -0.0146, -0.0034, -0.0068,\n","         -0.0042, -0.0162,  0.0062, -0.0164, -0.0217,  0.0143, -0.0282,  0.0250,\n","         -0.0313, -0.0230, -0.0252, -0.0334,  0.0145,  0.0219, -0.0260, -0.0287,\n","         -0.0020,  0.0354, -0.0186, -0.0311, -0.0141, -0.0309,  0.0326,  0.0046,\n","         -0.0007, -0.0173,  0.0140, -0.0156, -0.0102,  0.0277, -0.0286,  0.0329,\n","          0.0331, -0.0096, -0.0277,  0.0279,  0.0181,  0.0153, -0.0282,  0.0310,\n","          0.0215, -0.0283,  0.0111, -0.0063, -0.0342, -0.0325,  0.0145, -0.0196,\n","         -0.0279, -0.0048,  0.0283,  0.0058, -0.0184,  0.0210, -0.0282,  0.0280,\n","         -0.0183,  0.0035, -0.0287, -0.0041, -0.0111, -0.0310, -0.0139,  0.0003],\n","        requires_grad=True), Parameter containing:\n"," tensor([[-0.0624,  0.0877, -0.0296,  ...,  0.0271, -0.0003,  0.0319],\n","         [ 0.0826, -0.0547, -0.0343,  ..., -0.0606, -0.0358, -0.0499],\n","         [-0.0376,  0.0095,  0.0143,  ...,  0.0744, -0.0084,  0.0638],\n","         ...,\n","         [-0.0536,  0.0143, -0.0285,  ...,  0.0108,  0.0229,  0.0318],\n","         [-0.0109,  0.0616, -0.0088,  ...,  0.0045,  0.0575, -0.0228],\n","         [-0.0101,  0.0599, -0.0793,  ...,  0.0138, -0.0331,  0.0193]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-3.8656e-02, -5.5721e-02,  3.1203e-02,  4.0161e-02, -3.3490e-02,\n","          7.7996e-02, -1.8945e-02, -6.4567e-02, -5.1942e-02, -5.5533e-02,\n","         -5.6529e-02, -4.4813e-02,  3.3085e-02,  6.4413e-02,  3.4983e-02,\n","          3.2283e-02, -3.3594e-03, -3.1447e-02,  4.7778e-02, -6.1763e-02,\n","         -7.6439e-03,  2.6215e-02, -6.4189e-02,  3.1781e-02, -1.2252e-02,\n","          8.3581e-02, -2.6145e-02, -8.2295e-02, -4.0702e-02,  3.7337e-02,\n","          7.4795e-02, -7.2409e-02, -2.3783e-02, -8.6407e-03, -8.2087e-02,\n","         -1.1802e-02, -2.6905e-02, -8.2229e-02, -1.5516e-02,  6.6509e-02,\n","         -5.8780e-02,  2.3090e-02,  4.0377e-05,  7.3440e-02,  5.2463e-02,\n","         -4.6136e-03,  1.7120e-02, -7.4430e-02, -2.7345e-02,  1.5490e-02,\n","         -8.4717e-03, -5.2428e-02, -2.4664e-02, -2.3087e-02, -7.3475e-02,\n","         -4.2150e-02, -7.2434e-02,  6.8116e-03, -6.1342e-02,  7.4666e-02,\n","          8.7434e-02, -3.8062e-02,  5.7768e-02,  3.4049e-02],\n","        requires_grad=True), Parameter containing:\n"," tensor([[ 0.1132, -0.0396, -0.0092,  0.1216, -0.0013,  0.1152, -0.0098,  0.1135,\n","           0.0322,  0.0019,  0.0983,  0.1250,  0.0505,  0.1201,  0.0819, -0.1055,\n","           0.0010, -0.0608, -0.0815,  0.0349, -0.0464,  0.0373, -0.0318, -0.0289,\n","           0.1033,  0.0419, -0.0917,  0.1243,  0.0202,  0.0922,  0.0615, -0.1051,\n","           0.0440, -0.0503,  0.1081, -0.0288, -0.0296, -0.0364,  0.0278, -0.0413,\n","          -0.0320,  0.0672, -0.0122, -0.0447,  0.1210, -0.0336,  0.0981,  0.0414,\n","          -0.0637,  0.1202,  0.0459,  0.1043,  0.0695,  0.0133,  0.0894, -0.0478,\n","           0.0673,  0.0970, -0.0194,  0.0710, -0.0772,  0.0819, -0.0384, -0.0867],\n","         [-0.0853,  0.0603, -0.0653,  0.0002,  0.0926, -0.1222,  0.0399, -0.0119,\n","           0.0585,  0.0528, -0.1225,  0.0590, -0.0859,  0.1122,  0.0401, -0.0924,\n","           0.0655, -0.0718,  0.1188,  0.0758, -0.1095, -0.0486,  0.0680,  0.0417,\n","          -0.0969,  0.1025, -0.1230,  0.0742,  0.0385, -0.1153, -0.0961, -0.0354,\n","           0.0689, -0.1171, -0.0976, -0.1224,  0.1034,  0.0010,  0.0843,  0.0299,\n","           0.0436,  0.0042, -0.0661, -0.0798, -0.0097,  0.0435,  0.0621,  0.0035,\n","          -0.0458,  0.0395,  0.1232, -0.0087, -0.1011,  0.1160,  0.1188,  0.0238,\n","          -0.0646, -0.0407,  0.0455,  0.0692,  0.0491, -0.0782, -0.0391,  0.0158],\n","         [-0.0431,  0.0960, -0.1163,  0.0683, -0.0076, -0.0046,  0.0143,  0.0833,\n","           0.1235,  0.0635, -0.0761, -0.1122, -0.0975,  0.1204,  0.0250, -0.1118,\n","           0.1184,  0.0054, -0.0515, -0.0447, -0.0822,  0.0571,  0.0341, -0.0259,\n","           0.0810,  0.1050,  0.1247,  0.0782, -0.1177,  0.0359, -0.0261,  0.1102,\n","          -0.0396, -0.0742,  0.0322, -0.0944,  0.0655, -0.0109, -0.0526, -0.0998,\n","          -0.0133, -0.0591, -0.0871,  0.1142, -0.0878,  0.1110,  0.0860,  0.0029,\n","           0.1170, -0.0500, -0.0096,  0.0714,  0.0123,  0.0154,  0.1023, -0.0701,\n","          -0.0504, -0.0001, -0.0528, -0.0745,  0.0906,  0.1245,  0.0393,  0.0641],\n","         [-0.0130,  0.0673, -0.1040,  0.1237,  0.0098,  0.0404,  0.0678, -0.0881,\n","          -0.0048,  0.0753,  0.0294, -0.0969, -0.1032,  0.1163, -0.0939, -0.0166,\n","          -0.0635,  0.0821,  0.0178, -0.0367,  0.0859,  0.0040,  0.0384,  0.0874,\n","           0.0664,  0.0233,  0.0593,  0.0861,  0.0248, -0.0352, -0.0654,  0.0494,\n","          -0.0143,  0.0869,  0.0492, -0.0885,  0.1248, -0.0119, -0.1051,  0.0748,\n","          -0.0442, -0.0708, -0.0238,  0.0654, -0.0173, -0.0817, -0.0655, -0.0125,\n","           0.0573,  0.0082,  0.0934, -0.0511,  0.0147, -0.0658, -0.1237, -0.0282,\n","          -0.1083,  0.0179,  0.0294,  0.0419, -0.0119,  0.0758,  0.0604, -0.1070],\n","         [-0.0520,  0.1238,  0.0204,  0.0783, -0.0801, -0.0862,  0.0598,  0.0411,\n","           0.0172,  0.0941,  0.0670,  0.1175, -0.0728,  0.0449,  0.0342, -0.0873,\n","          -0.0388,  0.1192, -0.1185,  0.0300, -0.0343,  0.0177, -0.1150,  0.0602,\n","          -0.0986, -0.0370, -0.0132,  0.0015,  0.0691,  0.0134,  0.0902,  0.1145,\n","           0.0908,  0.0687, -0.0355,  0.0218,  0.0475,  0.1109, -0.0174, -0.0882,\n","          -0.1123,  0.0901, -0.1080,  0.0043,  0.0243, -0.0613, -0.0113, -0.0130,\n","           0.0648,  0.0173, -0.0347, -0.0643,  0.0126, -0.0156, -0.1063, -0.0280,\n","           0.1178,  0.0706,  0.0968,  0.0021,  0.0371, -0.0212, -0.0523, -0.0104],\n","         [ 0.1148, -0.0614,  0.0121, -0.1206, -0.0460,  0.0331, -0.0846, -0.0471,\n","           0.0762, -0.1061, -0.1228,  0.0260, -0.0319, -0.0617, -0.0356,  0.0359,\n","           0.0272,  0.0501,  0.1168, -0.0381, -0.0110, -0.0219, -0.0780, -0.0249,\n","          -0.0740,  0.0838,  0.0037, -0.0770, -0.1127, -0.0075, -0.0555,  0.0125,\n","           0.0976,  0.1150, -0.0520, -0.1065, -0.0931, -0.0225, -0.0652,  0.0821,\n","          -0.0432, -0.0667, -0.0171, -0.0824,  0.0803, -0.0489, -0.1188,  0.0110,\n","          -0.0294, -0.0962, -0.0684,  0.0824,  0.0567,  0.0030, -0.0403,  0.0670,\n","           0.0646,  0.1110,  0.0446, -0.1235,  0.1013,  0.0583, -0.0265,  0.0453],\n","         [-0.0986, -0.1206, -0.1061, -0.0363,  0.0086, -0.1129, -0.0798, -0.0688,\n","          -0.0248,  0.0365,  0.0927, -0.0212, -0.0686,  0.0667,  0.0526,  0.0477,\n","          -0.0498,  0.0608,  0.0680,  0.0938, -0.0289,  0.0933,  0.0436, -0.0638,\n","           0.0344,  0.0833,  0.0491, -0.1001,  0.0707, -0.0278, -0.0253, -0.0108,\n","           0.0970,  0.0879,  0.0928,  0.0009,  0.1144, -0.0904, -0.1188,  0.0869,\n","           0.0832, -0.0518, -0.0932, -0.0320, -0.0667, -0.1011,  0.0736,  0.0131,\n","          -0.0935, -0.0132, -0.1210,  0.1044,  0.1106, -0.1127,  0.0770, -0.0417,\n","           0.0743,  0.1012, -0.0346, -0.0319,  0.0062, -0.0867,  0.0082,  0.1040],\n","         [ 0.0550, -0.1148,  0.0593,  0.0484,  0.0105,  0.0291,  0.0285, -0.1164,\n","           0.0376, -0.0345,  0.1193,  0.0114,  0.0369,  0.0408,  0.0730,  0.0495,\n","          -0.0810, -0.0172, -0.0900,  0.1114,  0.0577,  0.0810, -0.1144,  0.0716,\n","          -0.0016, -0.0239,  0.1021,  0.0972, -0.0881, -0.0002,  0.0749, -0.0885,\n","          -0.0966, -0.1216,  0.0551,  0.1168, -0.0712,  0.0420, -0.0698,  0.0758,\n","          -0.0762, -0.0681,  0.0722,  0.0455,  0.0776, -0.0296, -0.0642,  0.0422,\n","          -0.0820,  0.0694, -0.1166, -0.0197,  0.0695, -0.0520,  0.0673,  0.0306,\n","           0.0724,  0.0048,  0.1013, -0.1223,  0.0769, -0.0068, -0.0443, -0.0483],\n","         [ 0.0084, -0.0224, -0.0601, -0.0336,  0.0515,  0.0871, -0.1106, -0.0738,\n","          -0.0796, -0.0965,  0.0956, -0.0453, -0.0185,  0.0013, -0.0280,  0.0345,\n","          -0.0931, -0.1105, -0.0890,  0.0384, -0.0808, -0.0621,  0.0924, -0.1184,\n","          -0.1052, -0.0943,  0.0520, -0.0561,  0.0811,  0.0486, -0.0708,  0.0936,\n","          -0.0771,  0.0238, -0.0866,  0.0886,  0.0326, -0.1029,  0.0198, -0.0369,\n","          -0.0782, -0.0804,  0.0775,  0.0181,  0.1168,  0.0690, -0.0042, -0.0307,\n","          -0.0282,  0.0976, -0.0406,  0.0929,  0.0020,  0.1165,  0.0664, -0.1194,\n","          -0.0899, -0.0841,  0.1211, -0.1193, -0.0971,  0.0276,  0.0082,  0.0256],\n","         [-0.1011, -0.0873, -0.0389, -0.0945,  0.0347,  0.0907, -0.0517, -0.0444,\n","           0.0982, -0.1046,  0.0265, -0.1167,  0.0722, -0.0165,  0.0420, -0.0264,\n","           0.0683, -0.0209, -0.0155,  0.0299, -0.0291, -0.0364, -0.1131, -0.1236,\n","           0.0931, -0.1179,  0.1153, -0.0682, -0.1021,  0.1165, -0.1134,  0.1083,\n","           0.0782,  0.0351,  0.0574,  0.0698, -0.0292, -0.0409, -0.0965, -0.0626,\n","           0.0433, -0.0571, -0.0676,  0.0792,  0.0754, -0.1166, -0.0511, -0.1033,\n","          -0.0795,  0.0824,  0.0520, -0.0668, -0.1090, -0.0856,  0.0553, -0.0351,\n","           0.0376,  0.0527,  0.0202, -0.1006, -0.0824,  0.0937, -0.0994,  0.0109]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-0.0616, -0.0839, -0.1146, -0.1061,  0.0564, -0.0598, -0.0957,  0.1215,\n","          0.0161,  0.0568], requires_grad=True)]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"3L-Aqf7en2_R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"id":"ydaHoi-HP9t9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"gOnDlc1PP_kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"id":"fVfEwSzWQEUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660997648091,"user_tz":-540,"elapsed":714431,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"41eac8fc-35be-4aaa-d0fb-2f6c829c4824"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.8473\tTest Loss: 0.3494\tAccuracy: 88.84%\n","[2] Train Loss: 0.2382\tTest Loss: 0.3438\tAccuracy: 88.89%\n","[3] Train Loss: 0.1841\tTest Loss: 0.3928\tAccuracy: 88.14%\n","[4] Train Loss: 0.1373\tTest Loss: 0.1258\tAccuracy: 96.02%\n","[5] Train Loss: 0.1026\tTest Loss: 0.1321\tAccuracy: 95.73%\n","[6] Train Loss: 0.0873\tTest Loss: 0.1590\tAccuracy: 95.15%\n","[7] Train Loss: 0.0751\tTest Loss: 0.0892\tAccuracy: 97.14%\n","[8] Train Loss: 0.0649\tTest Loss: 0.1052\tAccuracy: 96.76%\n","[9] Train Loss: 0.0585\tTest Loss: 0.1203\tAccuracy: 96.15%\n","[10] Train Loss: 0.0514\tTest Loss: 0.1603\tAccuracy: 95.16%\n","[11] Train Loss: 0.0463\tTest Loss: 0.0944\tAccuracy: 97.01%\n","[12] Train Loss: 0.0391\tTest Loss: 0.0943\tAccuracy: 97.16%\n","[13] Train Loss: 0.0349\tTest Loss: 0.0790\tAccuracy: 97.56%\n","[14] Train Loss: 0.0309\tTest Loss: 0.0764\tAccuracy: 97.76%\n","[15] Train Loss: 0.0277\tTest Loss: 0.1327\tAccuracy: 96.12%\n","[16] Train Loss: 0.0264\tTest Loss: 0.0746\tAccuracy: 97.70%\n","[17] Train Loss: 0.0221\tTest Loss: 0.0952\tAccuracy: 96.94%\n","[18] Train Loss: 0.0198\tTest Loss: 0.1125\tAccuracy: 96.79%\n","[19] Train Loss: 0.0176\tTest Loss: 0.0797\tAccuracy: 97.64%\n","[20] Train Loss: 0.0152\tTest Loss: 0.0687\tAccuracy: 97.98%\n","[21] Train Loss: 0.0135\tTest Loss: 0.0699\tAccuracy: 97.94%\n","[22] Train Loss: 1.9757\tTest Loss: 0.7399\tAccuracy: 75.24%\n","[23] Train Loss: 0.3183\tTest Loss: 0.3074\tAccuracy: 91.11%\n","[24] Train Loss: 0.1620\tTest Loss: 0.1964\tAccuracy: 94.00%\n","[25] Train Loss: 0.1430\tTest Loss: 0.1555\tAccuracy: 95.07%\n","[26] Train Loss: 0.1181\tTest Loss: 0.2166\tAccuracy: 93.14%\n","[27] Train Loss: 0.1058\tTest Loss: 0.2044\tAccuracy: 93.64%\n","[28] Train Loss: 0.1201\tTest Loss: 0.1269\tAccuracy: 96.20%\n","[29] Train Loss: 0.0871\tTest Loss: 0.1539\tAccuracy: 95.28%\n","[30] Train Loss: 0.0821\tTest Loss: 0.1215\tAccuracy: 96.35%\n","[31] Train Loss: 0.0762\tTest Loss: 0.1108\tAccuracy: 96.78%\n","[32] Train Loss: 0.0754\tTest Loss: 0.1282\tAccuracy: 96.17%\n","[33] Train Loss: 0.0665\tTest Loss: 0.1526\tAccuracy: 95.54%\n","[34] Train Loss: 0.0656\tTest Loss: 1.2535\tAccuracy: 77.50%\n","[35] Train Loss: 0.5430\tTest Loss: 0.2808\tAccuracy: 91.67%\n","[36] Train Loss: 0.1545\tTest Loss: 0.1774\tAccuracy: 94.66%\n","[37] Train Loss: 0.1248\tTest Loss: 0.1794\tAccuracy: 94.58%\n","[38] Train Loss: 0.1127\tTest Loss: 0.4061\tAccuracy: 89.00%\n","[39] Train Loss: 0.1087\tTest Loss: 0.1694\tAccuracy: 94.63%\n","[40] Train Loss: 0.0951\tTest Loss: 0.1461\tAccuracy: 95.58%\n","[41] Train Loss: 0.0899\tTest Loss: 0.1326\tAccuracy: 95.95%\n","[42] Train Loss: 0.0824\tTest Loss: 0.1495\tAccuracy: 95.64%\n","[43] Train Loss: 0.0794\tTest Loss: 0.2753\tAccuracy: 92.63%\n","[44] Train Loss: 0.0764\tTest Loss: 0.2240\tAccuracy: 93.34%\n","[45] Train Loss: 0.0742\tTest Loss: 0.1122\tAccuracy: 96.61%\n","[46] Train Loss: 0.0680\tTest Loss: 0.1394\tAccuracy: 96.16%\n","[47] Train Loss: 0.0643\tTest Loss: 0.1186\tAccuracy: 96.45%\n","[48] Train Loss: 0.0624\tTest Loss: 0.3286\tAccuracy: 91.01%\n","[49] Train Loss: 0.2990\tTest Loss: 0.1901\tAccuracy: 94.53%\n","[50] Train Loss: 0.1163\tTest Loss: 0.2042\tAccuracy: 94.08%\n","[51] Train Loss: 0.1025\tTest Loss: 0.1618\tAccuracy: 95.36%\n","[52] Train Loss: 0.0930\tTest Loss: 0.1505\tAccuracy: 95.37%\n","[53] Train Loss: 0.0862\tTest Loss: 0.1338\tAccuracy: 95.92%\n","[54] Train Loss: 0.0822\tTest Loss: 0.2196\tAccuracy: 93.93%\n","[55] Train Loss: 0.0816\tTest Loss: 0.1767\tAccuracy: 95.20%\n","[56] Train Loss: 0.0734\tTest Loss: 0.1287\tAccuracy: 96.41%\n","[57] Train Loss: 0.0691\tTest Loss: 0.1509\tAccuracy: 95.92%\n","[58] Train Loss: 0.0669\tTest Loss: 0.1338\tAccuracy: 96.27%\n","[59] Train Loss: 0.0633\tTest Loss: 0.1500\tAccuracy: 95.60%\n","[60] Train Loss: 0.0600\tTest Loss: 0.1495\tAccuracy: 95.87%\n","[61] Train Loss: 0.0568\tTest Loss: 0.1265\tAccuracy: 96.48%\n","[62] Train Loss: 0.0547\tTest Loss: 0.1454\tAccuracy: 96.23%\n","[63] Train Loss: 0.0538\tTest Loss: 0.1830\tAccuracy: 95.20%\n","[64] Train Loss: 0.0517\tTest Loss: 0.1298\tAccuracy: 96.64%\n","[65] Train Loss: 0.0491\tTest Loss: 0.1599\tAccuracy: 95.84%\n","[66] Train Loss: 0.0477\tTest Loss: 0.1620\tAccuracy: 95.68%\n","[67] Train Loss: 0.0457\tTest Loss: 0.1311\tAccuracy: 96.52%\n","[68] Train Loss: 0.0415\tTest Loss: 0.1284\tAccuracy: 96.62%\n","[69] Train Loss: 0.0402\tTest Loss: 0.1413\tAccuracy: 96.28%\n","[70] Train Loss: 0.0403\tTest Loss: 0.1563\tAccuracy: 96.16%\n","[71] Train Loss: 0.0413\tTest Loss: 0.1803\tAccuracy: 95.51%\n","[72] Train Loss: 0.0371\tTest Loss: 0.1337\tAccuracy: 96.57%\n","[73] Train Loss: 0.0342\tTest Loss: 0.1705\tAccuracy: 95.81%\n","[74] Train Loss: 0.0335\tTest Loss: 0.1360\tAccuracy: 96.64%\n","[75] Train Loss: 0.0317\tTest Loss: 0.1423\tAccuracy: 96.65%\n","[76] Train Loss: 0.0298\tTest Loss: 0.1462\tAccuracy: 96.42%\n","[77] Train Loss: 0.0295\tTest Loss: 0.1404\tAccuracy: 96.54%\n","[78] Train Loss: 0.0284\tTest Loss: 0.1398\tAccuracy: 96.66%\n","[79] Train Loss: 0.0275\tTest Loss: 0.1697\tAccuracy: 95.86%\n","[80] Train Loss: 0.0254\tTest Loss: 0.1365\tAccuracy: 96.90%\n","[81] Train Loss: 0.0239\tTest Loss: 0.1985\tAccuracy: 95.21%\n","[82] Train Loss: 0.0243\tTest Loss: 0.1937\tAccuracy: 95.43%\n","[83] Train Loss: 0.0243\tTest Loss: 0.2783\tAccuracy: 93.74%\n","[84] Train Loss: 0.1747\tTest Loss: 0.1982\tAccuracy: 95.16%\n","[85] Train Loss: 0.0516\tTest Loss: 0.1531\tAccuracy: 96.05%\n","[86] Train Loss: 0.0408\tTest Loss: 0.2169\tAccuracy: 94.85%\n","[87] Train Loss: 0.0350\tTest Loss: 0.1838\tAccuracy: 95.78%\n","[88] Train Loss: 0.0284\tTest Loss: 0.1539\tAccuracy: 96.35%\n","[89] Train Loss: 0.0262\tTest Loss: 0.1261\tAccuracy: 97.01%\n","[90] Train Loss: 0.0232\tTest Loss: 0.1363\tAccuracy: 96.81%\n","[91] Train Loss: 0.0213\tTest Loss: 0.1619\tAccuracy: 96.12%\n","[92] Train Loss: 0.0211\tTest Loss: 0.1436\tAccuracy: 96.77%\n","[93] Train Loss: 0.0190\tTest Loss: 0.1529\tAccuracy: 96.35%\n","[94] Train Loss: 0.0172\tTest Loss: 0.1342\tAccuracy: 96.80%\n","[95] Train Loss: 0.0156\tTest Loss: 0.1387\tAccuracy: 96.88%\n","[96] Train Loss: 0.0156\tTest Loss: 0.1367\tAccuracy: 96.96%\n","[97] Train Loss: 0.0137\tTest Loss: 0.1428\tAccuracy: 96.80%\n","[98] Train Loss: 0.0131\tTest Loss: 0.1417\tAccuracy: 96.87%\n","[99] Train Loss: 0.0121\tTest Loss: 0.1547\tAccuracy: 96.69%\n","[100] Train Loss: 0.0123\tTest Loss: 0.1471\tAccuracy: 96.86%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.leakyrelu(out)\n","        out = self.layer2(out)\n","        out = self.leakyrelu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"03dcaJosMaTN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661000022488,"user_tz":-540,"elapsed":432,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"046d25af-3f8a-401f-c3e8-ff7570a280bb","id":"APZv7InGMaTO"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (leakyrelu): LeakyReLU(negative_slope=0.1)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.3, momentum = 0.9)"],"metadata":{"id":"niYTk8FvMaTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661000768019,"user_tz":-540,"elapsed":729646,"user":{"displayName":"도승범(일반대학원 경제학과)","userId":"10297938005611756404"}},"outputId":"487504fb-9857-4531-d477-0c9e30db6952","id":"cRfUrlqwMaTR"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.6055\tTest Loss: 0.1884\tAccuracy: 94.26%\n","[2] Train Loss: 0.1535\tTest Loss: 0.1251\tAccuracy: 96.05%\n","[3] Train Loss: 0.1146\tTest Loss: 0.1164\tAccuracy: 96.44%\n","[4] Train Loss: 0.0890\tTest Loss: 0.1108\tAccuracy: 96.80%\n","[5] Train Loss: 0.0759\tTest Loss: 0.1036\tAccuracy: 97.07%\n","[6] Train Loss: 0.0629\tTest Loss: 0.1030\tAccuracy: 97.14%\n","[7] Train Loss: 0.0564\tTest Loss: 0.0988\tAccuracy: 97.10%\n","[8] Train Loss: 0.0474\tTest Loss: 0.1025\tAccuracy: 97.36%\n","[9] Train Loss: 0.0477\tTest Loss: 0.1018\tAccuracy: 97.30%\n","[10] Train Loss: 0.0392\tTest Loss: 0.1024\tAccuracy: 97.32%\n","[11] Train Loss: 0.0395\tTest Loss: 0.1098\tAccuracy: 97.18%\n","[12] Train Loss: 0.0330\tTest Loss: 0.0958\tAccuracy: 97.62%\n","[13] Train Loss: 0.0279\tTest Loss: 0.1112\tAccuracy: 97.32%\n","[14] Train Loss: 0.0295\tTest Loss: 0.1040\tAccuracy: 97.57%\n","[15] Train Loss: 0.0186\tTest Loss: 0.1001\tAccuracy: 97.78%\n","[16] Train Loss: 0.0163\tTest Loss: 0.1056\tAccuracy: 97.68%\n","[17] Train Loss: 0.0137\tTest Loss: 0.1075\tAccuracy: 97.62%\n","[18] Train Loss: 0.0146\tTest Loss: 0.1055\tAccuracy: 97.77%\n","[19] Train Loss: 0.0106\tTest Loss: 0.1071\tAccuracy: 97.84%\n","[20] Train Loss: 0.0081\tTest Loss: 0.1056\tAccuracy: 97.85%\n","[21] Train Loss: 0.0092\tTest Loss: 0.1213\tAccuracy: 97.83%\n","[22] Train Loss: 0.0084\tTest Loss: 0.1171\tAccuracy: 97.69%\n","[23] Train Loss: 0.0141\tTest Loss: 0.1267\tAccuracy: 97.58%\n","[24] Train Loss: 0.0215\tTest Loss: 0.1164\tAccuracy: 97.71%\n","[25] Train Loss: 0.0110\tTest Loss: 0.1223\tAccuracy: 97.63%\n","[26] Train Loss: 0.0064\tTest Loss: 0.1160\tAccuracy: 97.89%\n","[27] Train Loss: 0.0047\tTest Loss: 0.1170\tAccuracy: 97.95%\n","[28] Train Loss: 0.0043\tTest Loss: 0.1219\tAccuracy: 97.60%\n","[29] Train Loss: 0.0041\tTest Loss: 0.1172\tAccuracy: 97.92%\n","[30] Train Loss: 0.0019\tTest Loss: 0.1314\tAccuracy: 97.74%\n","[31] Train Loss: 0.0008\tTest Loss: 0.1264\tAccuracy: 97.94%\n","[32] Train Loss: 0.0004\tTest Loss: 0.1267\tAccuracy: 97.95%\n","[33] Train Loss: 0.0003\tTest Loss: 0.1223\tAccuracy: 98.00%\n","[34] Train Loss: 0.0002\tTest Loss: 0.1253\tAccuracy: 97.96%\n","[35] Train Loss: 0.0002\tTest Loss: 0.1243\tAccuracy: 98.01%\n","[36] Train Loss: 0.0002\tTest Loss: 0.1260\tAccuracy: 98.01%\n","[37] Train Loss: 0.0002\tTest Loss: 0.1245\tAccuracy: 98.00%\n","[38] Train Loss: 0.0002\tTest Loss: 0.1301\tAccuracy: 98.00%\n","[39] Train Loss: 0.0002\tTest Loss: 0.1272\tAccuracy: 98.00%\n","[40] Train Loss: 0.0001\tTest Loss: 0.1305\tAccuracy: 98.00%\n","[41] Train Loss: 0.0001\tTest Loss: 0.1299\tAccuracy: 98.04%\n","[42] Train Loss: 0.0001\tTest Loss: 0.1310\tAccuracy: 98.01%\n","[43] Train Loss: 0.0001\tTest Loss: 0.1288\tAccuracy: 98.04%\n","[44] Train Loss: 0.0001\tTest Loss: 0.1322\tAccuracy: 98.01%\n","[45] Train Loss: 0.0001\tTest Loss: 0.1308\tAccuracy: 98.04%\n","[46] Train Loss: 0.0001\tTest Loss: 0.1311\tAccuracy: 98.04%\n","[47] Train Loss: 0.0001\tTest Loss: 0.1300\tAccuracy: 98.04%\n","[48] Train Loss: 0.0001\tTest Loss: 0.1324\tAccuracy: 98.02%\n","[49] Train Loss: 0.0001\tTest Loss: 0.1317\tAccuracy: 98.06%\n","[50] Train Loss: 0.0001\tTest Loss: 0.1328\tAccuracy: 98.05%\n","[51] Train Loss: 0.0001\tTest Loss: 0.1345\tAccuracy: 98.05%\n","[52] Train Loss: 0.0001\tTest Loss: 0.1334\tAccuracy: 98.04%\n","[53] Train Loss: 0.0001\tTest Loss: 0.1329\tAccuracy: 98.06%\n","[54] Train Loss: 0.0001\tTest Loss: 0.1329\tAccuracy: 98.03%\n","[55] Train Loss: 0.0001\tTest Loss: 0.1335\tAccuracy: 98.03%\n","[56] Train Loss: 0.0001\tTest Loss: 0.1347\tAccuracy: 98.03%\n","[57] Train Loss: 0.0001\tTest Loss: 0.1335\tAccuracy: 98.04%\n","[58] Train Loss: 0.0001\tTest Loss: 0.1366\tAccuracy: 98.04%\n","[59] Train Loss: 0.0001\tTest Loss: 0.1345\tAccuracy: 98.06%\n","[60] Train Loss: 0.0001\tTest Loss: 0.1394\tAccuracy: 98.06%\n","[61] Train Loss: 0.0001\tTest Loss: 0.1329\tAccuracy: 98.05%\n","[62] Train Loss: 0.0001\tTest Loss: 0.1353\tAccuracy: 98.07%\n","[63] Train Loss: 0.0001\tTest Loss: 0.1353\tAccuracy: 98.06%\n","[64] Train Loss: 0.0001\tTest Loss: 0.1406\tAccuracy: 98.08%\n","[65] Train Loss: 0.0001\tTest Loss: 0.1365\tAccuracy: 98.03%\n","[66] Train Loss: 0.0001\tTest Loss: 0.1400\tAccuracy: 98.06%\n","[67] Train Loss: 0.0001\tTest Loss: 0.1379\tAccuracy: 98.05%\n","[68] Train Loss: 0.0001\tTest Loss: 0.1373\tAccuracy: 98.07%\n","[69] Train Loss: 0.0001\tTest Loss: 0.1368\tAccuracy: 98.08%\n","[70] Train Loss: 0.0001\tTest Loss: 0.1390\tAccuracy: 98.06%\n","[71] Train Loss: 0.0001\tTest Loss: 0.1395\tAccuracy: 98.07%\n","[72] Train Loss: 0.0001\tTest Loss: 0.1386\tAccuracy: 98.07%\n","[73] Train Loss: 0.0001\tTest Loss: 0.1382\tAccuracy: 98.06%\n","[74] Train Loss: 0.0001\tTest Loss: 0.1377\tAccuracy: 98.08%\n","[75] Train Loss: 0.0001\tTest Loss: 0.1401\tAccuracy: 98.05%\n","[76] Train Loss: 0.0000\tTest Loss: 0.1429\tAccuracy: 98.10%\n","[77] Train Loss: 0.0000\tTest Loss: 0.1385\tAccuracy: 98.08%\n","[78] Train Loss: 0.0000\tTest Loss: 0.1433\tAccuracy: 98.08%\n","[79] Train Loss: 0.0000\tTest Loss: 0.1394\tAccuracy: 98.06%\n","[80] Train Loss: 0.0000\tTest Loss: 0.1398\tAccuracy: 98.09%\n","[81] Train Loss: 0.0000\tTest Loss: 0.1397\tAccuracy: 98.07%\n","[82] Train Loss: 0.0000\tTest Loss: 0.1392\tAccuracy: 98.09%\n","[83] Train Loss: 0.0000\tTest Loss: 0.1400\tAccuracy: 98.08%\n","[84] Train Loss: 0.0000\tTest Loss: 0.1453\tAccuracy: 98.08%\n","[85] Train Loss: 0.0000\tTest Loss: 0.1403\tAccuracy: 98.07%\n","[86] Train Loss: 0.0000\tTest Loss: 0.1437\tAccuracy: 98.04%\n","[87] Train Loss: 0.0000\tTest Loss: 0.1388\tAccuracy: 98.09%\n","[88] Train Loss: 0.0000\tTest Loss: 0.1414\tAccuracy: 98.09%\n","[89] Train Loss: 0.0000\tTest Loss: 0.1413\tAccuracy: 98.07%\n","[90] Train Loss: 0.0000\tTest Loss: 0.1400\tAccuracy: 98.05%\n","[91] Train Loss: 0.0000\tTest Loss: 0.1395\tAccuracy: 98.04%\n","[92] Train Loss: 0.0000\tTest Loss: 0.1449\tAccuracy: 98.07%\n","[93] Train Loss: 0.0000\tTest Loss: 0.1446\tAccuracy: 98.07%\n","[94] Train Loss: 0.0000\tTest Loss: 0.1434\tAccuracy: 98.08%\n","[95] Train Loss: 0.0000\tTest Loss: 0.1416\tAccuracy: 98.09%\n","[96] Train Loss: 0.0000\tTest Loss: 0.1486\tAccuracy: 98.06%\n","[97] Train Loss: 0.0000\tTest Loss: 0.1433\tAccuracy: 98.08%\n","[98] Train Loss: 0.0000\tTest Loss: 0.1428\tAccuracy: 98.10%\n","[99] Train Loss: 0.0000\tTest Loss: 0.1457\tAccuracy: 98.06%\n","[100] Train Loss: 0.0000\tTest Loss: 0.1418\tAccuracy: 98.09%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","source":["1) 기존 모형의 accuracy가 0.95~0.97에서 증감을 반복<br>\n"," -> local minima에 갇히는 문제가 있을 수도 있어서 <br>\n","    momentum 파라미터를 추가하였습니다.\n","<br>\n","\n","2) Dyling Relu 문제 <br>\n"," -> Dying Relu 문제가 있을 수도 있으므로 <br> \n","    leaky Relu를 대신 사용해보았습니다.\n","<br>\n","\n","결과적으로 Accuracy가 98% 이상으로 높아졌고, <br>\n","train 과정에서 성능이 감소하는 경우가 <br>\n","줄어들었습니다. 하지만 큰 차이는 없었습니다."],"metadata":{"id":"KJpZSWGCSlYp"}}]}