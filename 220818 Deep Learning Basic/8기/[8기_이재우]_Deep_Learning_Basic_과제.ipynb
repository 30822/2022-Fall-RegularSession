{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[8기 이재우]_Deep_Learning_Basic_과제.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "1yjLIvojoLZj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_relu(x):\n",
        "  if x > 0:\n",
        "    result = 1\n",
        "  else: \n",
        "    result = 0\n",
        "  return result"
      ],
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relu(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VKnMbBMp-ru",
        "outputId": "4db98ea3-fd7b-45f3-aaf2-34d8fe32373e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu(-3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2EUelkNqA8I",
        "outputId": "1fbfb45c-c573-4fe0-c589-bd657a2e21d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_relu(13)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLUGiMVBqC3f",
        "outputId": "9c13e03b-0277-4ffd-ecf4-7305ed7c2ead"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_relu(-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p04Tu1y0qF1D",
        "outputId": "398722a6-61bd-450e-f3e1-8eee1e6d7931"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "import numpy as np\n",
        "import sklearn.datasets"
      ],
      "metadata": {
        "id": "JF1yMJBJqoex"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
      ],
      "metadata": {
        "id": "JZftT6nQqs77"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "num_train = 60000\n",
        "num_class = 10\n",
        "\n",
        "x_train = np.float32(mnist.data[:num_train]).T\n",
        "y_train_index = np.int32(mnist.target[:num_train]).T\n",
        "x_test = np.float32(mnist.data[num_train:]).T\n",
        "y_test_index = np.int32(mnist.target[num_train:]).T\n",
        "\n",
        "# Normalization\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "x_size = x_train.shape[0]\n",
        "\n",
        "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
        "for idx in range(y_train_index.shape[0]):\n",
        "  y_train[y_train_index[idx], idx] = 1\n",
        "\n",
        "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
        "for idx in range(y_test_index.shape[0]):\n",
        "  y_test[y_test_index[idx], idx] = 1    "
      ],
      "metadata": {
        "id": "-Vq7WSzrqvnf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_ZCtboqw6R",
        "outputId": "2598cb6c-1fca-4551-d4d0-176af40c95b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter initialization\n",
        "\n",
        "hidden_size1 = 128 # hidden unit size1\n",
        "hidden_size2 = 64\n",
        "\n",
        "# two-layer neural network\n",
        "\n",
        "params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n",
        "          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/ x_size),\n",
        "          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/ hidden_size1),\n",
        "          \"b2\": np.zeros((hidden_size2, 1)) * np.sqrt(1/ hidden_size1),\n",
        "          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n",
        "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n",
        "          }\n",
        "# Xavier initialization: https://reniew.github.io/13/"
      ],
      "metadata": {
        "id": "tch8u0f1qxYY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  # derivative of sigmoid\n",
        "  exp = np.exp(-x)\n",
        "  return (exp)/((1+exp)**2)\n",
        "\n",
        "def softmax(x):\n",
        "  exp = np.exp(x)\n",
        "  return exp/np.sum(exp, axis=0)"
      ],
      "metadata": {
        "id": "Bf7akc9pqz6m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y_true, y_pred):\n",
        "  # loss calculation\n",
        "\n",
        "  num_sample = y_true.shape[1]\n",
        "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
        "  \n",
        "  return Li/num_sample"
      ],
      "metadata": {
        "id": "WMGyf5Rdq2l9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = softmax(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "wWlonfzWrBHh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass_test(x, params):\n",
        "\n",
        "  params_test = {}\n",
        "  \n",
        "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
        "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
        "  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
        "  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
        "  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n",
        "\n",
        "  return params_test"
      ],
      "metadata": {
        "id": "wuZujQ6TrCZv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_true, y_pred):\n",
        "  y_true_idx = np.argmax(y_true, axis = 0)\n",
        "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
        "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
        "\n",
        "  accuracy = num_correct / y_true.shape[1] * 100\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "fubMgqirrDy3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] = (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "BN4oC80XrFQy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.5\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  if i == 0:\n",
        "    params = foward_pass(x_train, params)\n",
        "    \n",
        "  grads = backward_pass(x_train, y_train, params)\n",
        "\n",
        "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "\n",
        "  params = foward_pass(x_train, params)\n",
        "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
        "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
        "\n",
        "  params_test = foward_pass_test(x_test, params)\n",
        "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
        "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
        "\n",
        "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, testing acuracy = {}%\"\n",
        "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfhYoRZ5rHAt",
        "outputId": "e3070b5c-749c-4fef-93fc-c4b3e40e2fd9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: training loss = 2.302905, training acuracy = 10.93%, test loss = 2.302975, testing acuracy = 10.93%\n",
            "Epoch 2: training loss = 2.302724, training acuracy = 10.95%, test loss = 2.302788, testing acuracy = 10.91%\n",
            "Epoch 3: training loss = 2.302542, training acuracy = 11.45%, test loss = 2.302601, testing acuracy = 11.63%\n",
            "Epoch 4: training loss = 2.30236, training acuracy = 12.5%, test loss = 2.302414, testing acuracy = 12.67%\n",
            "Epoch 5: training loss = 2.302178, training acuracy = 13.65%, test loss = 2.302227, testing acuracy = 14.02%\n",
            "Epoch 6: training loss = 2.301995, training acuracy = 14.55%, test loss = 2.302039, testing acuracy = 14.94%\n",
            "Epoch 7: training loss = 2.301812, training acuracy = 15.2%, test loss = 2.301851, testing acuracy = 15.68%\n",
            "Epoch 8: training loss = 2.301629, training acuracy = 15.52%, test loss = 2.301662, testing acuracy = 16.02%\n",
            "Epoch 9: training loss = 2.301444, training acuracy = 15.66%, test loss = 2.301473, testing acuracy = 16.07%\n",
            "Epoch 10: training loss = 2.301259, training acuracy = 15.72%, test loss = 2.301282, testing acuracy = 15.98%\n",
            "Epoch 11: training loss = 2.301073, training acuracy = 15.67%, test loss = 2.301091, testing acuracy = 15.93%\n",
            "Epoch 12: training loss = 2.300886, training acuracy = 15.6%, test loss = 2.300898, testing acuracy = 15.79%\n",
            "Epoch 13: training loss = 2.300698, training acuracy = 15.54%, test loss = 2.300705, testing acuracy = 15.74%\n",
            "Epoch 14: training loss = 2.300508, training acuracy = 15.48%, test loss = 2.300509, testing acuracy = 15.79%\n",
            "Epoch 15: training loss = 2.300317, training acuracy = 15.47%, test loss = 2.300313, testing acuracy = 15.69%\n",
            "Epoch 16: training loss = 2.300124, training acuracy = 15.44%, test loss = 2.300115, testing acuracy = 15.74%\n",
            "Epoch 17: training loss = 2.29993, training acuracy = 15.44%, test loss = 2.299915, testing acuracy = 15.77%\n",
            "Epoch 18: training loss = 2.299733, training acuracy = 15.47%, test loss = 2.299713, testing acuracy = 15.8%\n",
            "Epoch 19: training loss = 2.299535, training acuracy = 15.52%, test loss = 2.299509, testing acuracy = 15.86%\n",
            "Epoch 20: training loss = 2.299334, training acuracy = 15.58%, test loss = 2.299303, testing acuracy = 15.93%\n",
            "Epoch 21: training loss = 2.299132, training acuracy = 15.65%, test loss = 2.299094, testing acuracy = 16.0%\n",
            "Epoch 22: training loss = 2.298926, training acuracy = 15.75%, test loss = 2.298883, testing acuracy = 16.1%\n",
            "Epoch 23: training loss = 2.298719, training acuracy = 15.86%, test loss = 2.29867, testing acuracy = 16.16%\n",
            "Epoch 24: training loss = 2.298508, training acuracy = 15.98%, test loss = 2.298453, testing acuracy = 16.28%\n",
            "Epoch 25: training loss = 2.298295, training acuracy = 16.1%, test loss = 2.298234, testing acuracy = 16.34%\n",
            "Epoch 26: training loss = 2.298078, training acuracy = 16.26%, test loss = 2.298012, testing acuracy = 16.49%\n",
            "Epoch 27: training loss = 2.297859, training acuracy = 16.43%, test loss = 2.297786, testing acuracy = 16.61%\n",
            "Epoch 28: training loss = 2.297635, training acuracy = 16.59%, test loss = 2.297557, testing acuracy = 16.76%\n",
            "Epoch 29: training loss = 2.297409, training acuracy = 16.75%, test loss = 2.297324, testing acuracy = 16.85%\n",
            "Epoch 30: training loss = 2.297178, training acuracy = 16.9%, test loss = 2.297087, testing acuracy = 17.04%\n",
            "Epoch 31: training loss = 2.296944, training acuracy = 17.08%, test loss = 2.296847, testing acuracy = 17.2%\n",
            "Epoch 32: training loss = 2.296706, training acuracy = 17.26%, test loss = 2.296602, testing acuracy = 17.32%\n",
            "Epoch 33: training loss = 2.296463, training acuracy = 17.41%, test loss = 2.296353, testing acuracy = 17.49%\n",
            "Epoch 34: training loss = 2.296216, training acuracy = 17.55%, test loss = 2.2961, testing acuracy = 17.64%\n",
            "Epoch 35: training loss = 2.295964, training acuracy = 17.7%, test loss = 2.295841, testing acuracy = 17.8%\n",
            "Epoch 36: training loss = 2.295707, training acuracy = 17.84%, test loss = 2.295578, testing acuracy = 17.9%\n",
            "Epoch 37: training loss = 2.295445, training acuracy = 18.02%, test loss = 2.295309, testing acuracy = 18.01%\n",
            "Epoch 38: training loss = 2.295178, training acuracy = 18.14%, test loss = 2.295036, testing acuracy = 18.13%\n",
            "Epoch 39: training loss = 2.294906, training acuracy = 18.27%, test loss = 2.294756, testing acuracy = 18.22%\n",
            "Epoch 40: training loss = 2.294628, training acuracy = 18.4%, test loss = 2.294471, testing acuracy = 18.37%\n",
            "Epoch 41: training loss = 2.294343, training acuracy = 18.51%, test loss = 2.29418, testing acuracy = 18.51%\n",
            "Epoch 42: training loss = 2.294053, training acuracy = 18.7%, test loss = 2.293883, testing acuracy = 18.81%\n",
            "Epoch 43: training loss = 2.293757, training acuracy = 19.02%, test loss = 2.29358, testing acuracy = 19.19%\n",
            "Epoch 44: training loss = 2.293454, training acuracy = 19.5%, test loss = 2.29327, testing acuracy = 19.7%\n",
            "Epoch 45: training loss = 2.293144, training acuracy = 20.07%, test loss = 2.292953, testing acuracy = 20.33%\n",
            "Epoch 46: training loss = 2.292827, training acuracy = 20.7%, test loss = 2.292629, testing acuracy = 21.01%\n",
            "Epoch 47: training loss = 2.292504, training acuracy = 21.41%, test loss = 2.292298, testing acuracy = 21.81%\n",
            "Epoch 48: training loss = 2.292172, training acuracy = 22.24%, test loss = 2.291959, testing acuracy = 22.71%\n",
            "Epoch 49: training loss = 2.291834, training acuracy = 23.34%, test loss = 2.291613, testing acuracy = 23.9%\n",
            "Epoch 50: training loss = 2.291487, training acuracy = 25.08%, test loss = 2.291259, testing acuracy = 25.78%\n",
            "Epoch 51: training loss = 2.291133, training acuracy = 27.25%, test loss = 2.290897, testing acuracy = 27.97%\n",
            "Epoch 52: training loss = 2.29077, training acuracy = 29.33%, test loss = 2.290526, testing acuracy = 29.91%\n",
            "Epoch 53: training loss = 2.290399, training acuracy = 31.28%, test loss = 2.290148, testing acuracy = 31.86%\n",
            "Epoch 54: training loss = 2.29002, training acuracy = 33.07%, test loss = 2.28976, testing acuracy = 33.7%\n",
            "Epoch 55: training loss = 2.289631, training acuracy = 34.67%, test loss = 2.289364, testing acuracy = 35.33%\n",
            "Epoch 56: training loss = 2.289234, training acuracy = 36.05%, test loss = 2.288959, testing acuracy = 36.61%\n",
            "Epoch 57: training loss = 2.288828, training acuracy = 37.27%, test loss = 2.288544, testing acuracy = 37.95%\n",
            "Epoch 58: training loss = 2.288412, training acuracy = 38.43%, test loss = 2.28812, testing acuracy = 39.24%\n",
            "Epoch 59: training loss = 2.287986, training acuracy = 39.49%, test loss = 2.287686, testing acuracy = 40.34%\n",
            "Epoch 60: training loss = 2.287551, training acuracy = 40.58%, test loss = 2.287243, testing acuracy = 41.34%\n",
            "Epoch 61: training loss = 2.287106, training acuracy = 41.5%, test loss = 2.286789, testing acuracy = 42.38%\n",
            "Epoch 62: training loss = 2.286651, training acuracy = 42.46%, test loss = 2.286325, testing acuracy = 43.4%\n",
            "Epoch 63: training loss = 2.286185, training acuracy = 43.37%, test loss = 2.285851, testing acuracy = 44.27%\n",
            "Epoch 64: training loss = 2.285709, training acuracy = 44.22%, test loss = 2.285366, testing acuracy = 45.2%\n",
            "Epoch 65: training loss = 2.285222, training acuracy = 44.99%, test loss = 2.284871, testing acuracy = 45.99%\n",
            "Epoch 66: training loss = 2.284725, training acuracy = 45.83%, test loss = 2.284364, testing acuracy = 46.98%\n",
            "Epoch 67: training loss = 2.284216, training acuracy = 46.66%, test loss = 2.283847, testing acuracy = 47.78%\n",
            "Epoch 68: training loss = 2.283696, training acuracy = 47.53%, test loss = 2.283318, testing acuracy = 48.7%\n",
            "Epoch 69: training loss = 2.283165, training acuracy = 48.29%, test loss = 2.282778, testing acuracy = 49.49%\n",
            "Epoch 70: training loss = 2.282622, training acuracy = 49.03%, test loss = 2.282226, testing acuracy = 50.22%\n",
            "Epoch 71: training loss = 2.282068, training acuracy = 49.73%, test loss = 2.281662, testing acuracy = 50.82%\n",
            "Epoch 72: training loss = 2.281502, training acuracy = 50.4%, test loss = 2.281087, testing acuracy = 51.45%\n",
            "Epoch 73: training loss = 2.280923, training acuracy = 51.04%, test loss = 2.280499, testing acuracy = 51.82%\n",
            "Epoch 74: training loss = 2.280333, training acuracy = 51.65%, test loss = 2.2799, testing acuracy = 52.21%\n",
            "Epoch 75: training loss = 2.279731, training acuracy = 52.21%, test loss = 2.279288, testing acuracy = 52.63%\n",
            "Epoch 76: training loss = 2.279116, training acuracy = 52.72%, test loss = 2.278664, testing acuracy = 53.14%\n",
            "Epoch 77: training loss = 2.278489, training acuracy = 53.2%, test loss = 2.278028, testing acuracy = 53.51%\n",
            "Epoch 78: training loss = 2.27785, training acuracy = 53.6%, test loss = 2.277379, testing acuracy = 53.9%\n",
            "Epoch 79: training loss = 2.277198, training acuracy = 53.95%, test loss = 2.276718, testing acuracy = 54.23%\n",
            "Epoch 80: training loss = 2.276533, training acuracy = 54.2%, test loss = 2.276044, testing acuracy = 54.45%\n",
            "Epoch 81: training loss = 2.275856, training acuracy = 54.45%, test loss = 2.275358, testing acuracy = 54.79%\n",
            "Epoch 82: training loss = 2.275167, training acuracy = 54.72%, test loss = 2.274658, testing acuracy = 55.1%\n",
            "Epoch 83: training loss = 2.274465, training acuracy = 54.89%, test loss = 2.273947, testing acuracy = 55.4%\n",
            "Epoch 84: training loss = 2.27375, training acuracy = 55.11%, test loss = 2.273223, testing acuracy = 55.46%\n",
            "Epoch 85: training loss = 2.273022, training acuracy = 55.25%, test loss = 2.272486, testing acuracy = 55.53%\n",
            "Epoch 86: training loss = 2.272283, training acuracy = 55.35%, test loss = 2.271737, testing acuracy = 55.71%\n",
            "Epoch 87: training loss = 2.27153, training acuracy = 55.45%, test loss = 2.270975, testing acuracy = 55.8%\n",
            "Epoch 88: training loss = 2.270765, training acuracy = 55.51%, test loss = 2.270201, testing acuracy = 55.97%\n",
            "Epoch 89: training loss = 2.269988, training acuracy = 55.56%, test loss = 2.269414, testing acuracy = 56.06%\n",
            "Epoch 90: training loss = 2.269198, training acuracy = 55.59%, test loss = 2.268615, testing acuracy = 56.09%\n",
            "Epoch 91: training loss = 2.268397, training acuracy = 55.65%, test loss = 2.267804, testing acuracy = 56.18%\n",
            "Epoch 92: training loss = 2.267583, training acuracy = 55.64%, test loss = 2.266981, testing acuracy = 56.18%\n",
            "Epoch 93: training loss = 2.266757, training acuracy = 55.68%, test loss = 2.266147, testing acuracy = 56.18%\n",
            "Epoch 94: training loss = 2.265919, training acuracy = 55.64%, test loss = 2.2653, testing acuracy = 56.2%\n",
            "Epoch 95: training loss = 2.26507, training acuracy = 55.67%, test loss = 2.264442, testing acuracy = 56.16%\n",
            "Epoch 96: training loss = 2.264209, training acuracy = 55.63%, test loss = 2.263572, testing acuracy = 56.08%\n",
            "Epoch 97: training loss = 2.263336, training acuracy = 55.59%, test loss = 2.262691, testing acuracy = 56.04%\n",
            "Epoch 98: training loss = 2.262453, training acuracy = 55.55%, test loss = 2.261799, testing acuracy = 55.94%\n",
            "Epoch 99: training loss = 2.261558, training acuracy = 55.5%, test loss = 2.260896, testing acuracy = 56.0%\n",
            "Epoch 100: training loss = 2.260652, training acuracy = 55.46%, test loss = 2.259982, testing acuracy = 55.9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "rJwJey7uGcCC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 텐서로 변경\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "-_opt74xHugU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "id": "YiPibepNH0AL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "JHhjmXz5H3Y5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJO249A3jhk"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQTdNy4PGYVR",
        "outputId": "64d8ba20-295d-431e-f528-e655318505a9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n",
        "                         # require_true 얘는 학습되는 애구나 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAZPXiuIGe0W",
        "outputId": "66402d26-15a9-497a-f9d9-b86e05f60858"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0203,  0.0310, -0.0247,  ..., -0.0077, -0.0173, -0.0153],\n",
              "         [ 0.0252, -0.0022,  0.0350,  ..., -0.0265,  0.0296, -0.0198],\n",
              "         [ 0.0190, -0.0280,  0.0329,  ...,  0.0156, -0.0292,  0.0091],\n",
              "         ...,\n",
              "         [-0.0185, -0.0191, -0.0026,  ...,  0.0234,  0.0222,  0.0092],\n",
              "         [-0.0269, -0.0042,  0.0065,  ...,  0.0338,  0.0353, -0.0144],\n",
              "         [ 0.0027, -0.0005,  0.0280,  ..., -0.0162,  0.0003, -0.0122]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 1.0070e-02, -1.5995e-02,  7.2597e-03, -2.4965e-02,  2.0189e-02,\n",
              "         -2.2571e-02, -2.3948e-02, -2.9904e-02,  1.8628e-02,  2.5505e-03,\n",
              "          1.0518e-02, -2.7607e-02, -2.1285e-02,  2.4328e-02,  5.6603e-03,\n",
              "          3.4405e-02,  2.2507e-03,  2.1228e-02,  2.5734e-02,  2.2452e-02,\n",
              "          2.6311e-02,  2.8190e-03, -4.6057e-03,  2.1130e-02,  1.0302e-02,\n",
              "         -4.3101e-04, -1.7172e-04,  2.6341e-03,  7.3069e-03, -1.7542e-02,\n",
              "         -1.5784e-02, -9.7718e-03,  5.6371e-03, -2.5953e-02,  1.5572e-02,\n",
              "          3.2387e-02,  1.4152e-02, -1.8361e-02, -2.2066e-02,  3.1467e-02,\n",
              "         -2.2209e-02,  2.6908e-02, -2.3590e-02, -2.1480e-02, -2.0190e-02,\n",
              "         -1.4815e-02,  1.6934e-02, -1.6178e-06,  1.4310e-02,  2.5648e-02,\n",
              "         -2.6173e-03, -2.8650e-02, -5.4118e-04, -3.1160e-02,  8.5975e-03,\n",
              "          3.3075e-02,  3.3211e-03, -3.5605e-02,  3.3536e-02, -1.3059e-02,\n",
              "          1.4963e-03, -2.2539e-02, -3.0699e-02, -3.0669e-02, -1.5660e-02,\n",
              "         -1.9299e-02,  2.9610e-02, -2.3531e-02,  1.0031e-02,  1.5343e-02,\n",
              "          2.0906e-02,  2.6282e-02, -1.2125e-02, -8.0760e-03, -3.4466e-02,\n",
              "          1.5432e-02, -3.2635e-02, -2.2779e-02,  9.4048e-03, -9.2173e-03,\n",
              "          2.1598e-02,  8.3711e-03, -3.0311e-02, -7.9089e-03,  3.3680e-02,\n",
              "         -3.3788e-02,  8.7963e-04, -3.5049e-02,  1.0814e-04,  6.5088e-03,\n",
              "         -1.0329e-02, -2.4769e-02,  2.7978e-02, -7.8494e-04,  9.2687e-04,\n",
              "          3.5054e-02, -2.1580e-02, -2.4022e-02,  8.2366e-03,  3.6664e-03,\n",
              "         -1.1377e-02,  1.8018e-03, -1.9287e-02,  4.5639e-03, -1.6151e-03,\n",
              "         -1.0928e-02, -3.5650e-02, -5.2971e-03,  1.7812e-02, -2.5435e-02,\n",
              "         -3.2947e-02, -3.0890e-02,  1.1508e-02, -3.5686e-02,  6.9624e-03,\n",
              "         -2.0948e-02,  1.9033e-02, -2.1966e-02,  2.0184e-02, -1.8650e-02,\n",
              "          3.4988e-02,  5.1656e-03, -3.3398e-03,  8.4539e-03,  1.3614e-02,\n",
              "         -1.8588e-02, -6.3261e-03, -3.2888e-02], requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0414,  0.0449,  0.0082,  ..., -0.0854, -0.0159,  0.0517],\n",
              "         [-0.0610, -0.0398, -0.0626,  ..., -0.0320, -0.0682,  0.0695],\n",
              "         [ 0.0263, -0.0486,  0.0306,  ..., -0.0070, -0.0718, -0.0550],\n",
              "         ...,\n",
              "         [ 0.0562, -0.0072, -0.0394,  ..., -0.0882, -0.0089,  0.0224],\n",
              "         [ 0.0336, -0.0032,  0.0559,  ..., -0.0156,  0.0707,  0.0404],\n",
              "         [-0.0706,  0.0690,  0.0003,  ..., -0.0238, -0.0789, -0.0100]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0221,  0.0559,  0.0210, -0.0218, -0.0174,  0.0042, -0.0810, -0.0671,\n",
              "          0.0369, -0.0052,  0.0424,  0.0421,  0.0564, -0.0574,  0.0639,  0.0343,\n",
              "         -0.0568, -0.0004, -0.0727, -0.0073,  0.0467,  0.0223,  0.0820,  0.0651,\n",
              "         -0.0383,  0.0857, -0.0306,  0.0355, -0.0516,  0.0235, -0.0114, -0.0599,\n",
              "         -0.0651, -0.0224,  0.0544,  0.0495,  0.0699,  0.0412, -0.0603,  0.0295,\n",
              "          0.0497, -0.0253,  0.0268,  0.0003, -0.0264, -0.0213, -0.0712,  0.0273,\n",
              "         -0.0002, -0.0446, -0.0150,  0.0398,  0.0756,  0.0737, -0.0614, -0.0701,\n",
              "         -0.0847,  0.0379,  0.0022,  0.0345,  0.0051,  0.0223, -0.0087,  0.0412],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[-0.0106,  0.1160,  0.0373,  0.1170, -0.0350, -0.1097,  0.0726, -0.0907,\n",
              "          -0.0819, -0.1095, -0.0315,  0.0800,  0.0247, -0.0811, -0.0674, -0.0193,\n",
              "           0.1183, -0.1002, -0.0131, -0.0789, -0.0733, -0.1184, -0.0828, -0.1236,\n",
              "          -0.0340,  0.0132,  0.0986,  0.0606,  0.1194,  0.0536, -0.0013,  0.0780,\n",
              "          -0.0102,  0.0391,  0.0759,  0.0687, -0.0702,  0.1065, -0.0376, -0.0875,\n",
              "           0.0961, -0.0944, -0.0586,  0.0656, -0.1102,  0.1131, -0.0639,  0.0592,\n",
              "          -0.0245,  0.0519,  0.1089,  0.0064, -0.0470, -0.0548, -0.0408,  0.1084,\n",
              "           0.0592, -0.0365, -0.1042, -0.0578, -0.0745,  0.0225, -0.0865, -0.0228],\n",
              "         [ 0.1114,  0.0472,  0.0967,  0.1200, -0.0971,  0.0950, -0.0509,  0.0019,\n",
              "           0.1049, -0.0098, -0.0725, -0.0224,  0.1101, -0.0314,  0.0298,  0.0856,\n",
              "           0.0877,  0.0229, -0.0532,  0.0027,  0.0165, -0.0264, -0.0990, -0.1068,\n",
              "           0.0844, -0.1005,  0.0333, -0.0839,  0.0492,  0.0153, -0.0261, -0.0687,\n",
              "           0.0286,  0.0775,  0.0407,  0.0042, -0.0813,  0.0578,  0.0429,  0.0761,\n",
              "           0.0604,  0.1168,  0.1070, -0.0172,  0.0717, -0.0354, -0.0261, -0.0349,\n",
              "           0.1083, -0.0085, -0.0891,  0.0619, -0.0584,  0.0802, -0.0952,  0.1100,\n",
              "          -0.0388,  0.1219,  0.0532, -0.0714, -0.1092, -0.0266,  0.0456,  0.0007],\n",
              "         [-0.0138, -0.0346, -0.0782, -0.0872, -0.1133, -0.1225, -0.0926, -0.0462,\n",
              "          -0.0039, -0.0989,  0.0170, -0.0034, -0.0258,  0.0772,  0.0100,  0.0855,\n",
              "          -0.0074,  0.0203, -0.1176, -0.0347, -0.0891, -0.0102, -0.1241,  0.1154,\n",
              "           0.0542,  0.1011, -0.0440,  0.0305,  0.0290,  0.0743,  0.0261,  0.0961,\n",
              "           0.0887, -0.0520,  0.1151, -0.0417, -0.0272, -0.0136, -0.1124,  0.0225,\n",
              "           0.0227,  0.0960,  0.1027, -0.0227,  0.1035,  0.0733, -0.0408,  0.0810,\n",
              "           0.0874, -0.0638, -0.1190, -0.0148, -0.0609, -0.0167,  0.0616,  0.1145,\n",
              "           0.0138, -0.0023,  0.0954, -0.0605,  0.0845,  0.0144, -0.1004,  0.0071],\n",
              "         [-0.0417, -0.0800, -0.1171, -0.0683, -0.0393,  0.0893,  0.0380,  0.1009,\n",
              "          -0.0208,  0.0088,  0.0940, -0.0558, -0.0868,  0.0659, -0.1063,  0.0425,\n",
              "           0.0236, -0.0451, -0.0727,  0.0554,  0.0453,  0.0426,  0.0831, -0.0458,\n",
              "          -0.0737,  0.0198, -0.1223,  0.0995, -0.1181, -0.1212, -0.1032,  0.1219,\n",
              "          -0.0364, -0.0500, -0.1082,  0.0719,  0.0078,  0.1166, -0.1121, -0.0183,\n",
              "           0.1177,  0.1214, -0.0222, -0.0482, -0.0350,  0.0344, -0.0704, -0.0585,\n",
              "           0.0051, -0.0367,  0.0212, -0.0554,  0.0390, -0.0894,  0.0266, -0.0444,\n",
              "           0.0877,  0.0974,  0.0013, -0.0545,  0.0735,  0.0911,  0.0762,  0.0244],\n",
              "         [-0.0569,  0.0466,  0.0660,  0.0645, -0.1047, -0.1233,  0.0920, -0.0280,\n",
              "          -0.0885,  0.0677, -0.0266, -0.0225,  0.0872,  0.1050, -0.0076, -0.1063,\n",
              "          -0.1064,  0.1240, -0.0918,  0.0821, -0.0868,  0.0439, -0.0586,  0.0578,\n",
              "           0.1070,  0.1047,  0.0093,  0.0630,  0.1220, -0.0971, -0.0462, -0.0454,\n",
              "           0.0467,  0.0582,  0.0976,  0.0649, -0.1171,  0.0017, -0.0060, -0.1210,\n",
              "          -0.0673, -0.1030,  0.0586, -0.0588,  0.0632,  0.0451, -0.1184,  0.0959,\n",
              "           0.0411, -0.0803, -0.0084, -0.0604,  0.0095, -0.0413, -0.0755,  0.0386,\n",
              "           0.0456, -0.0208,  0.0543, -0.1030, -0.0783,  0.0908,  0.1208,  0.0643],\n",
              "         [-0.0597,  0.0058, -0.0949, -0.0553,  0.0980, -0.0804, -0.0767,  0.0511,\n",
              "           0.1062, -0.0095, -0.1030,  0.0889, -0.0714,  0.0378,  0.0916, -0.0680,\n",
              "          -0.1065,  0.0720, -0.0795, -0.0963, -0.0086, -0.1009, -0.0389, -0.0598,\n",
              "          -0.1189, -0.0914,  0.0489,  0.0152,  0.0803,  0.1162, -0.0602, -0.0532,\n",
              "           0.0436, -0.0941,  0.0184,  0.1141, -0.0111,  0.0202, -0.1219, -0.0704,\n",
              "           0.0440,  0.0476, -0.0973, -0.1212,  0.1202,  0.0092,  0.0264,  0.0819,\n",
              "          -0.0379,  0.0316, -0.1095, -0.1215,  0.0404, -0.0960,  0.0900,  0.0455,\n",
              "           0.0956, -0.1024,  0.0724,  0.0180, -0.0876, -0.0612, -0.0771, -0.0791],\n",
              "         [ 0.0612, -0.1067, -0.1112,  0.0202, -0.0933, -0.0919, -0.1173, -0.0464,\n",
              "           0.0617,  0.0856, -0.0864,  0.0320,  0.0505, -0.0755,  0.0508, -0.0428,\n",
              "          -0.0204,  0.0459,  0.0960,  0.0227,  0.0977,  0.0837, -0.0453, -0.0819,\n",
              "           0.1124,  0.1149, -0.0876,  0.0258,  0.0107, -0.0795,  0.0875, -0.0558,\n",
              "           0.0090, -0.1178,  0.0328,  0.0990,  0.1133, -0.0868,  0.0870,  0.1134,\n",
              "           0.1139,  0.0544,  0.1085,  0.0198,  0.0348,  0.0664, -0.0445,  0.1248,\n",
              "           0.0360,  0.1122,  0.0570, -0.0104, -0.0712, -0.0427,  0.1055, -0.0151,\n",
              "           0.0672,  0.0745, -0.1250, -0.0236, -0.1057, -0.0967, -0.0063,  0.0089],\n",
              "         [-0.0205,  0.0325, -0.1072, -0.0779,  0.0595, -0.0334, -0.0294,  0.0359,\n",
              "           0.0139,  0.0720,  0.0501,  0.0417,  0.0080, -0.0836,  0.0961, -0.0137,\n",
              "          -0.1203, -0.1117, -0.1185, -0.0968, -0.0468,  0.1030,  0.1217,  0.0944,\n",
              "          -0.0256,  0.0612,  0.0038,  0.1014, -0.0397, -0.1152,  0.0757,  0.0007,\n",
              "           0.1030, -0.0358, -0.1244, -0.0246, -0.0827, -0.1186,  0.0814,  0.0164,\n",
              "           0.0669, -0.1142, -0.0812,  0.0086, -0.0182, -0.0891, -0.0378, -0.0943,\n",
              "          -0.1106,  0.0913, -0.0281,  0.1080, -0.1109,  0.0229,  0.0609, -0.0531,\n",
              "          -0.0122,  0.0376,  0.0720, -0.1093,  0.1150, -0.0151,  0.1111,  0.1198],\n",
              "         [-0.1047,  0.0294, -0.0444,  0.0910,  0.0582, -0.0471,  0.0986, -0.0799,\n",
              "          -0.1151, -0.0100,  0.0595,  0.0289, -0.0346, -0.0306, -0.0849,  0.0645,\n",
              "          -0.1179,  0.0408,  0.0906, -0.0274,  0.0770,  0.0743, -0.0659,  0.0823,\n",
              "          -0.0849, -0.0276, -0.0967,  0.0391, -0.0638, -0.1073,  0.0296, -0.0962,\n",
              "           0.0472,  0.1158,  0.0517,  0.0492,  0.0168, -0.0514,  0.0483,  0.0134,\n",
              "           0.0733,  0.0382, -0.1216,  0.1229, -0.1109,  0.0634,  0.0581, -0.0798,\n",
              "          -0.0017,  0.0010, -0.0355, -0.1199, -0.0936,  0.0881,  0.1025, -0.1061,\n",
              "           0.0970,  0.1008,  0.1104, -0.0921,  0.1059, -0.0903, -0.0054, -0.0924],\n",
              "         [-0.0408,  0.0034,  0.1206, -0.0053,  0.0286, -0.1198,  0.1227, -0.0181,\n",
              "          -0.1100, -0.0964,  0.0153,  0.0703, -0.0026,  0.0096,  0.0937,  0.0578,\n",
              "          -0.1076, -0.0217, -0.0794, -0.0190, -0.0544,  0.0320, -0.0353,  0.0461,\n",
              "          -0.0770, -0.0368, -0.0602,  0.0327,  0.0206,  0.0251, -0.1059, -0.0629,\n",
              "           0.0769, -0.0711, -0.0381, -0.0482, -0.0039, -0.0167,  0.0635, -0.0241,\n",
              "           0.1196,  0.0558,  0.0220, -0.0513,  0.0752, -0.0460,  0.0321, -0.0379,\n",
              "          -0.0770, -0.0217,  0.0106, -0.0977, -0.0516,  0.0575, -0.1021,  0.0264,\n",
              "          -0.0467, -0.0806,  0.0360,  0.0162,  0.0937, -0.0840,  0.0507,  0.0574]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0799, -0.0697, -0.0699, -0.0555, -0.0418,  0.0486, -0.0435, -0.0185,\n",
              "          0.0892, -0.0202], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "23hDKQaSGghZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    # 배치 당 loss 값을 담을 리스트 생성\n",
        "    batch_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # 옵티마이저의 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # y pred 값 산출\n",
        "        output = model(data)\n",
        "        # loss 계산\n",
        "        # 정답 데이터와의 cross entropy loss 계산\n",
        "        # 이 loss를 배치 당 loss로 보관\n",
        "        loss = criterion(output, target)\n",
        "        batch_losses.append(loss)\n",
        "\n",
        "        # 기울기 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트!\n",
        "        optimizer.step()\n",
        "        \n",
        "    # 배치당 평균 loss 계산\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    \n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "dpe2jaBtIsaL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    batch_losses = []\n",
        "    correct = 0 \n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            # 예측값 생성\n",
        "            output = model(data)\n",
        "\n",
        "            # loss 계산 (이전과 동일)\n",
        "            loss = criterion(output, target)\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "           # Accuracy 계산\n",
        "           # y pred와 y가 일치하면 correct에 1을 더해주기\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # 배치 당 평균 loss 계산 \n",
        "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    #정확도 계산\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "Fr-RDZ8YJAHM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJjeMfCZJRbf",
        "outputId": "516767ab-66f3-4c67-85ff-583e49c1d7fb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.8582\tTest Loss: 0.2881\tAccuracy: 91.10%\n",
            "[2] Train Loss: 0.2406\tTest Loss: 0.2590\tAccuracy: 91.69%\n",
            "[3] Train Loss: 0.1702\tTest Loss: 0.2368\tAccuracy: 92.34%\n",
            "[4] Train Loss: 0.1351\tTest Loss: 0.2077\tAccuracy: 93.20%\n",
            "[5] Train Loss: 0.1083\tTest Loss: 0.1441\tAccuracy: 95.52%\n",
            "[6] Train Loss: 0.0892\tTest Loss: 0.1084\tAccuracy: 96.66%\n",
            "[7] Train Loss: 0.0773\tTest Loss: 0.1318\tAccuracy: 95.81%\n",
            "[8] Train Loss: 0.0670\tTest Loss: 0.1827\tAccuracy: 94.62%\n",
            "[9] Train Loss: 0.0645\tTest Loss: 0.0873\tAccuracy: 97.24%\n",
            "[10] Train Loss: 0.0530\tTest Loss: 0.2386\tAccuracy: 93.01%\n",
            "[11] Train Loss: 0.1013\tTest Loss: 0.0763\tAccuracy: 97.64%\n",
            "[12] Train Loss: 0.0456\tTest Loss: 0.1002\tAccuracy: 96.93%\n",
            "[13] Train Loss: 0.0407\tTest Loss: 0.0780\tAccuracy: 97.64%\n",
            "[14] Train Loss: 0.0354\tTest Loss: 0.1070\tAccuracy: 96.93%\n",
            "[15] Train Loss: 0.0317\tTest Loss: 0.0821\tAccuracy: 97.59%\n",
            "[16] Train Loss: 0.0284\tTest Loss: 0.1294\tAccuracy: 95.74%\n",
            "[17] Train Loss: 0.0964\tTest Loss: 0.0758\tAccuracy: 97.73%\n",
            "[18] Train Loss: 0.0308\tTest Loss: 0.2740\tAccuracy: 92.48%\n",
            "[19] Train Loss: 0.2594\tTest Loss: 0.1280\tAccuracy: 95.86%\n",
            "[20] Train Loss: 0.0588\tTest Loss: 0.1164\tAccuracy: 96.26%\n",
            "[21] Train Loss: 0.0465\tTest Loss: 0.1157\tAccuracy: 96.37%\n",
            "[22] Train Loss: 0.0402\tTest Loss: 0.1712\tAccuracy: 94.70%\n",
            "[23] Train Loss: 0.0364\tTest Loss: 0.2347\tAccuracy: 93.31%\n",
            "[24] Train Loss: 0.1244\tTest Loss: 0.0796\tAccuracy: 97.47%\n",
            "[25] Train Loss: 0.0413\tTest Loss: 0.1455\tAccuracy: 95.55%\n",
            "[26] Train Loss: 0.0634\tTest Loss: 0.0733\tAccuracy: 97.69%\n",
            "[27] Train Loss: 0.0303\tTest Loss: 0.0851\tAccuracy: 97.53%\n",
            "[28] Train Loss: 0.0255\tTest Loss: 0.0776\tAccuracy: 97.70%\n",
            "[29] Train Loss: 0.0224\tTest Loss: 0.0773\tAccuracy: 97.52%\n",
            "[30] Train Loss: 0.0189\tTest Loss: 0.0728\tAccuracy: 97.84%\n",
            "[31] Train Loss: 0.0171\tTest Loss: 0.0725\tAccuracy: 97.87%\n",
            "[32] Train Loss: 0.0148\tTest Loss: 0.0802\tAccuracy: 97.77%\n",
            "[33] Train Loss: 0.0131\tTest Loss: 0.0844\tAccuracy: 97.60%\n",
            "[34] Train Loss: 0.0117\tTest Loss: 0.0773\tAccuracy: 97.84%\n",
            "[35] Train Loss: 0.0104\tTest Loss: 0.0724\tAccuracy: 97.81%\n",
            "[36] Train Loss: 0.0093\tTest Loss: 0.0757\tAccuracy: 97.86%\n",
            "[37] Train Loss: 0.0080\tTest Loss: 0.0738\tAccuracy: 98.01%\n",
            "[38] Train Loss: 0.0072\tTest Loss: 0.0783\tAccuracy: 97.98%\n",
            "[39] Train Loss: 0.0066\tTest Loss: 0.0779\tAccuracy: 97.84%\n",
            "[40] Train Loss: 0.0060\tTest Loss: 0.0797\tAccuracy: 97.87%\n",
            "[41] Train Loss: 0.0053\tTest Loss: 0.0770\tAccuracy: 97.91%\n",
            "[42] Train Loss: 0.0048\tTest Loss: 0.0805\tAccuracy: 97.87%\n",
            "[43] Train Loss: 0.0045\tTest Loss: 0.0863\tAccuracy: 97.82%\n",
            "[44] Train Loss: 0.0041\tTest Loss: 0.0789\tAccuracy: 97.91%\n",
            "[45] Train Loss: 0.0037\tTest Loss: 0.0806\tAccuracy: 97.90%\n",
            "[46] Train Loss: 0.0034\tTest Loss: 0.0858\tAccuracy: 97.87%\n",
            "[47] Train Loss: 0.0032\tTest Loss: 0.0801\tAccuracy: 97.95%\n",
            "[48] Train Loss: 0.0030\tTest Loss: 0.0847\tAccuracy: 97.79%\n",
            "[49] Train Loss: 0.0028\tTest Loss: 0.0848\tAccuracy: 97.79%\n",
            "[50] Train Loss: 0.0026\tTest Loss: 0.0816\tAccuracy: 97.96%\n",
            "[51] Train Loss: 0.0023\tTest Loss: 0.0837\tAccuracy: 97.98%\n",
            "[52] Train Loss: 0.0022\tTest Loss: 0.0836\tAccuracy: 97.86%\n",
            "[53] Train Loss: 0.0021\tTest Loss: 0.0832\tAccuracy: 97.91%\n",
            "[54] Train Loss: 0.0019\tTest Loss: 0.0833\tAccuracy: 97.97%\n",
            "[55] Train Loss: 0.0018\tTest Loss: 0.0840\tAccuracy: 97.92%\n",
            "[56] Train Loss: 0.0017\tTest Loss: 0.0863\tAccuracy: 97.94%\n",
            "[57] Train Loss: 0.0016\tTest Loss: 0.0867\tAccuracy: 97.97%\n",
            "[58] Train Loss: 0.0016\tTest Loss: 0.0851\tAccuracy: 97.92%\n",
            "[59] Train Loss: 0.0015\tTest Loss: 0.0859\tAccuracy: 97.98%\n",
            "[60] Train Loss: 0.0014\tTest Loss: 0.0860\tAccuracy: 97.92%\n",
            "[61] Train Loss: 0.0014\tTest Loss: 0.0883\tAccuracy: 97.98%\n",
            "[62] Train Loss: 0.0013\tTest Loss: 0.0884\tAccuracy: 97.94%\n",
            "[63] Train Loss: 0.0013\tTest Loss: 0.0878\tAccuracy: 97.96%\n",
            "[64] Train Loss: 0.0012\tTest Loss: 0.0884\tAccuracy: 97.93%\n",
            "[65] Train Loss: 0.0011\tTest Loss: 0.0886\tAccuracy: 97.93%\n",
            "[66] Train Loss: 0.0011\tTest Loss: 0.0883\tAccuracy: 97.96%\n",
            "[67] Train Loss: 0.0011\tTest Loss: 0.0892\tAccuracy: 97.92%\n",
            "[68] Train Loss: 0.0010\tTest Loss: 0.0905\tAccuracy: 97.93%\n",
            "[69] Train Loss: 0.0010\tTest Loss: 0.0904\tAccuracy: 97.93%\n",
            "[70] Train Loss: 0.0009\tTest Loss: 0.0916\tAccuracy: 97.95%\n",
            "[71] Train Loss: 0.0009\tTest Loss: 0.0910\tAccuracy: 97.95%\n",
            "[72] Train Loss: 0.0009\tTest Loss: 0.0927\tAccuracy: 97.94%\n",
            "[73] Train Loss: 0.0009\tTest Loss: 0.0948\tAccuracy: 97.94%\n",
            "[74] Train Loss: 0.0008\tTest Loss: 0.0921\tAccuracy: 97.97%\n",
            "[75] Train Loss: 0.0008\tTest Loss: 0.0914\tAccuracy: 97.97%\n",
            "[76] Train Loss: 0.0008\tTest Loss: 0.0924\tAccuracy: 97.95%\n",
            "[77] Train Loss: 0.0008\tTest Loss: 0.0946\tAccuracy: 97.96%\n",
            "[78] Train Loss: 0.0007\tTest Loss: 0.0955\tAccuracy: 97.96%\n",
            "[79] Train Loss: 0.0007\tTest Loss: 0.0930\tAccuracy: 97.94%\n",
            "[80] Train Loss: 0.0007\tTest Loss: 0.0957\tAccuracy: 97.94%\n",
            "[81] Train Loss: 0.0007\tTest Loss: 0.0934\tAccuracy: 97.95%\n",
            "[82] Train Loss: 0.0007\tTest Loss: 0.0959\tAccuracy: 97.95%\n",
            "[83] Train Loss: 0.0006\tTest Loss: 0.0940\tAccuracy: 97.92%\n",
            "[84] Train Loss: 0.0006\tTest Loss: 0.0952\tAccuracy: 97.93%\n",
            "[85] Train Loss: 0.0006\tTest Loss: 0.0950\tAccuracy: 97.92%\n",
            "[86] Train Loss: 0.0006\tTest Loss: 0.0960\tAccuracy: 97.92%\n",
            "[87] Train Loss: 0.0006\tTest Loss: 0.0972\tAccuracy: 97.93%\n",
            "[88] Train Loss: 0.0006\tTest Loss: 0.0955\tAccuracy: 97.94%\n",
            "[89] Train Loss: 0.0006\tTest Loss: 0.0958\tAccuracy: 97.94%\n",
            "[90] Train Loss: 0.0005\tTest Loss: 0.0975\tAccuracy: 97.95%\n",
            "[91] Train Loss: 0.0005\tTest Loss: 0.0965\tAccuracy: 97.93%\n",
            "[92] Train Loss: 0.0005\tTest Loss: 0.0971\tAccuracy: 97.94%\n",
            "[93] Train Loss: 0.0005\tTest Loss: 0.0994\tAccuracy: 97.93%\n",
            "[94] Train Loss: 0.0005\tTest Loss: 0.0985\tAccuracy: 97.97%\n",
            "[95] Train Loss: 0.0005\tTest Loss: 0.0978\tAccuracy: 97.96%\n",
            "[96] Train Loss: 0.0005\tTest Loss: 0.1010\tAccuracy: 97.95%\n",
            "[97] Train Loss: 0.0005\tTest Loss: 0.0996\tAccuracy: 97.96%\n",
            "[98] Train Loss: 0.0005\tTest Loss: 0.0993\tAccuracy: 97.97%\n",
            "[99] Train Loss: 0.0005\tTest Loss: 0.0970\tAccuracy: 97.96%\n",
            "[100] Train Loss: 0.0004\tTest Loss: 0.1002\tAccuracy: 97.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,256)\n",
        "        self.layer2 = nn.Linear(256,128)\n",
        "        self.layer3 = nn.Linear(128,10)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.01)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.leakyrelu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.leakyrelu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "v9pVw_Ie45Lo"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "PeUfYzR9z1JZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40\n",
        "\n",
        "for epoch in(range(1, EPOCHS + 1)):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRVUg9JXz111",
        "outputId": "a411d723-5675-4146-f568-a9fba3b35bbb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.0004\tTest Loss: 0.1015\tAccuracy: 97.92%\n",
            "[2] Train Loss: 0.0004\tTest Loss: 0.1011\tAccuracy: 97.93%\n",
            "[3] Train Loss: 0.0004\tTest Loss: 0.1023\tAccuracy: 97.93%\n",
            "[4] Train Loss: 0.0004\tTest Loss: 0.1004\tAccuracy: 97.94%\n",
            "[5] Train Loss: 0.0004\tTest Loss: 0.1014\tAccuracy: 97.94%\n",
            "[6] Train Loss: 0.0004\tTest Loss: 0.1003\tAccuracy: 97.94%\n",
            "[7] Train Loss: 0.0004\tTest Loss: 0.0988\tAccuracy: 97.94%\n",
            "[8] Train Loss: 0.0004\tTest Loss: 0.0997\tAccuracy: 97.94%\n",
            "[9] Train Loss: 0.0004\tTest Loss: 0.1005\tAccuracy: 97.95%\n",
            "[10] Train Loss: 0.0004\tTest Loss: 0.0996\tAccuracy: 97.95%\n",
            "[11] Train Loss: 0.0004\tTest Loss: 0.0999\tAccuracy: 97.95%\n",
            "[12] Train Loss: 0.0004\tTest Loss: 0.1014\tAccuracy: 97.95%\n",
            "[13] Train Loss: 0.0004\tTest Loss: 0.1000\tAccuracy: 97.95%\n",
            "[14] Train Loss: 0.0004\tTest Loss: 0.0999\tAccuracy: 97.95%\n",
            "[15] Train Loss: 0.0004\tTest Loss: 0.1003\tAccuracy: 97.95%\n",
            "[16] Train Loss: 0.0004\tTest Loss: 0.1010\tAccuracy: 97.95%\n",
            "[17] Train Loss: 0.0004\tTest Loss: 0.1007\tAccuracy: 97.95%\n",
            "[18] Train Loss: 0.0004\tTest Loss: 0.1012\tAccuracy: 97.95%\n",
            "[19] Train Loss: 0.0004\tTest Loss: 0.0993\tAccuracy: 97.95%\n",
            "[20] Train Loss: 0.0004\tTest Loss: 0.0992\tAccuracy: 97.95%\n",
            "[21] Train Loss: 0.0004\tTest Loss: 0.1040\tAccuracy: 97.95%\n",
            "[22] Train Loss: 0.0004\tTest Loss: 0.1008\tAccuracy: 97.95%\n",
            "[23] Train Loss: 0.0004\tTest Loss: 0.0989\tAccuracy: 97.95%\n",
            "[24] Train Loss: 0.0004\tTest Loss: 0.0996\tAccuracy: 97.95%\n",
            "[25] Train Loss: 0.0004\tTest Loss: 0.1020\tAccuracy: 97.95%\n",
            "[26] Train Loss: 0.0004\tTest Loss: 0.1012\tAccuracy: 97.95%\n",
            "[27] Train Loss: 0.0004\tTest Loss: 0.0999\tAccuracy: 97.95%\n",
            "[28] Train Loss: 0.0004\tTest Loss: 0.1029\tAccuracy: 97.95%\n",
            "[29] Train Loss: 0.0004\tTest Loss: 0.1016\tAccuracy: 97.95%\n",
            "[30] Train Loss: 0.0004\tTest Loss: 0.1022\tAccuracy: 97.95%\n",
            "[31] Train Loss: 0.0004\tTest Loss: 0.1029\tAccuracy: 97.95%\n",
            "[32] Train Loss: 0.0004\tTest Loss: 0.0999\tAccuracy: 97.95%\n",
            "[33] Train Loss: 0.0004\tTest Loss: 0.1011\tAccuracy: 97.95%\n",
            "[34] Train Loss: 0.0004\tTest Loss: 0.1020\tAccuracy: 97.95%\n",
            "[35] Train Loss: 0.0004\tTest Loss: 0.1016\tAccuracy: 97.95%\n",
            "[36] Train Loss: 0.0004\tTest Loss: 0.1005\tAccuracy: 97.95%\n",
            "[37] Train Loss: 0.0004\tTest Loss: 0.1014\tAccuracy: 97.95%\n",
            "[38] Train Loss: 0.0004\tTest Loss: 0.1007\tAccuracy: 97.95%\n",
            "[39] Train Loss: 0.0004\tTest Loss: 0.1005\tAccuracy: 97.95%\n",
            "[40] Train Loss: 0.0004\tTest Loss: 0.1008\tAccuracy: 97.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Activation function으로 leakyrelu를 적용해보았다.\n",
        "2. 128, 64였던 hidden size를 두배씩 늘려 256, 128로 변경하였다.\n",
        "3. Learning rate가 크다고 생각되어 줄이고자 하였다. 따라서 기존의 0.5였던 learning rate를 0.3으로 줄여보았다.\n",
        "4. Epochs가 클수록 오히려 중간에 더 높은 accuracy가 나타나다가 다시 줄어드는 경향성이 보여서 기존의 100이던 epochs를 40으로 줄여보았다. 40으로 설정한 이유는 과제 3에서 37번째에서 98.01%라는 가장 높은 accuracy를 보였기 때문이다.\n",
        " \n",
        "\n",
        "결과적으로 위의 변화들을 주었더니 accuracy가 97.93%에서 97.95%로 0.02%만큼만 높아진 것을 확인할 수 있다. 과제 3과의 약간의 차이점은 accuracy가 높아졌다 줄어들었다를 반복하지 않고 약간씩 높아지기다가 97.95%에서 멈추는 것을 확인할 수 있다.\n"
      ],
      "metadata": {
        "id": "ljpkPNl_LKxi"
      }
    }
  ]
}