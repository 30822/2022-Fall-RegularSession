{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rjuQY9f2mdS"
   },
   "source": [
    "## 과제 1\n",
    "ReLu activation function과 derivative function을 구현해보세요\n",
    "- Hint : np.maximum 함수 사용하면 편리합니다\n",
    "- 다른 방법 사용하셔도 무방합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "puH0YVGI2uLz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "print(relu(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Esm4jmTVijro"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def d_relu(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "print(d_relu(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz8Hi0Rc2-yJ"
   },
   "source": [
    "## 과제 2\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
    "- Hint : 코드 파일의 예시는 Two layer MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fusEy49j3uhs"
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "\n",
    "    dS3 = params[\"A3\"] - y_true\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
    "    grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "    grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "    grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "    dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "    dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
    "\n",
    "    grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "    grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twf-R8s-34zT"
   },
   "source": [
    "## 과제 3\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
    "\n",
    "hyperparameter는 다음과 같이 설정\n",
    "\n",
    "- epochs : 100\n",
    "- hiddensize : 128, 64 (two layer)\n",
    "- learning_rate : 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bxJO249A3jhk"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11e7671ae6f4d2e8198c96a9501f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe0b3a3e679468d83ceeeff0ec9b0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b703f5f6964dbc918cb701216ef0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649311c9f96248358a6804327420b3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./.data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./.data/MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = True,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")\n",
    "testset = datasets.MNIST(\n",
    "    root      = './.data/', \n",
    "    train     = False,\n",
    "    download  = True,\n",
    "    transform = transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(784,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0169, -0.0306, -0.0293,  ...,  0.0024,  0.0168, -0.0325],\n",
       "         [ 0.0265,  0.0076, -0.0232,  ...,  0.0342, -0.0088,  0.0207],\n",
       "         [ 0.0172,  0.0176, -0.0089,  ..., -0.0352, -0.0152,  0.0040],\n",
       "         ...,\n",
       "         [-0.0060, -0.0324,  0.0348,  ...,  0.0028, -0.0220,  0.0205],\n",
       "         [-0.0238, -0.0005, -0.0208,  ..., -0.0102, -0.0267, -0.0141],\n",
       "         [ 0.0273,  0.0270, -0.0103,  ..., -0.0306, -0.0317,  0.0123]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 2.3096e-02, -3.1422e-02, -6.8047e-03,  1.3254e-02, -2.6889e-03,\n",
       "         -3.4172e-02,  2.7767e-02,  2.6929e-02, -2.4585e-02, -2.7267e-02,\n",
       "          2.1142e-02, -9.7406e-03, -4.7191e-03, -3.4670e-02,  1.8651e-02,\n",
       "          2.0922e-02, -8.9046e-03,  1.3520e-02, -1.9148e-02,  2.5238e-02,\n",
       "         -1.2193e-02, -2.8478e-02, -5.2660e-03,  2.8169e-02, -5.1021e-03,\n",
       "         -1.2769e-02, -3.4826e-02,  5.2888e-05, -1.4744e-02,  3.4501e-02,\n",
       "          3.1735e-02, -9.5470e-03,  1.3981e-02, -1.4071e-03,  1.5881e-03,\n",
       "          1.9838e-02, -9.1095e-03, -1.3310e-02,  3.2051e-02,  5.7465e-03,\n",
       "          1.2582e-02, -2.1718e-02, -9.8020e-03, -1.4656e-02, -2.2455e-02,\n",
       "          1.3136e-02,  2.1361e-02, -2.3368e-02,  2.3027e-02,  1.5619e-02,\n",
       "          2.8038e-02,  2.3045e-02, -2.0967e-02,  2.7601e-02,  1.4304e-02,\n",
       "         -5.7562e-03,  1.6055e-03,  3.4863e-02,  5.2483e-03,  1.0615e-02,\n",
       "         -2.1639e-02, -5.7765e-03,  1.7620e-02, -6.1967e-03,  1.7026e-02,\n",
       "          3.0548e-02, -1.4341e-02,  3.3128e-02, -1.9428e-02, -7.1063e-03,\n",
       "          3.5339e-03,  2.2544e-02,  7.9524e-03,  1.6352e-02, -1.1020e-02,\n",
       "         -1.4672e-02,  1.7399e-02, -3.0012e-02, -1.5715e-02,  7.8758e-03,\n",
       "         -2.2586e-03,  4.8654e-03,  2.4672e-02, -1.8193e-02, -1.6292e-03,\n",
       "          3.4205e-02,  2.0059e-02, -1.2785e-02, -1.7513e-02,  1.8045e-02,\n",
       "         -2.5117e-02, -2.1995e-03,  1.2330e-02,  2.1500e-02,  2.8444e-02,\n",
       "          1.3705e-03,  3.3686e-02, -1.6944e-02,  4.1648e-04, -2.1335e-02,\n",
       "         -1.2091e-02,  1.0457e-02, -2.7464e-02,  3.3040e-02,  4.7702e-03,\n",
       "          2.6681e-03, -3.1913e-02,  7.0713e-03, -4.9628e-03, -2.2790e-03,\n",
       "         -1.5393e-02, -3.2009e-02,  1.4663e-02,  3.2819e-02, -2.6840e-02,\n",
       "          2.2977e-02,  3.4035e-02,  1.9094e-02,  1.0667e-02,  8.2627e-03,\n",
       "         -1.0027e-02,  1.1156e-03,  1.7733e-02,  3.4603e-03, -2.1399e-02,\n",
       "          1.1717e-02, -9.6619e-03, -2.0719e-02], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0330, -0.0815, -0.0731,  ...,  0.0270,  0.0121, -0.0619],\n",
       "         [-0.0355, -0.0691, -0.0516,  ..., -0.0794, -0.0834,  0.0583],\n",
       "         [-0.0035, -0.0667,  0.0875,  ...,  0.0583,  0.0440, -0.0473],\n",
       "         ...,\n",
       "         [-0.0569,  0.0478, -0.0506,  ...,  0.0659,  0.0747, -0.0638],\n",
       "         [ 0.0028, -0.0474,  0.0625,  ..., -0.0495,  0.0729, -0.0494],\n",
       "         [-0.0467,  0.0506, -0.0865,  ...,  0.0599, -0.0312,  0.0105]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.0789,  0.0754, -0.0074, -0.0836,  0.0599,  0.0865,  0.0217, -0.0057,\n",
       "          0.0200, -0.0812,  0.0270, -0.0523,  0.0085,  0.0765,  0.0316, -0.0697,\n",
       "          0.0222,  0.0765,  0.0859,  0.0004,  0.0708, -0.0688, -0.0552,  0.0231,\n",
       "         -0.0082,  0.0279, -0.0718,  0.0375,  0.0260,  0.0446,  0.0125,  0.0196,\n",
       "         -0.0006, -0.0688,  0.0836, -0.0035, -0.0325,  0.0378,  0.0309,  0.0641,\n",
       "         -0.0411,  0.0128,  0.0706, -0.0070, -0.0221,  0.0862,  0.0078, -0.0062,\n",
       "          0.0042, -0.0862,  0.0595, -0.0298,  0.0240,  0.0404, -0.0641,  0.0077,\n",
       "         -0.0653,  0.0406,  0.0137,  0.0258, -0.0713,  0.0513,  0.0756, -0.0364],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 9.6563e-02, -1.0737e-01,  4.2806e-03, -1.7603e-02,  3.6317e-02,\n",
       "          -5.5970e-02,  6.6664e-02,  1.3036e-02,  7.1517e-02,  7.6893e-02,\n",
       "           4.5239e-02, -9.3456e-02, -1.1195e-01,  1.0623e-01,  1.1528e-01,\n",
       "           9.5950e-02, -6.7550e-02, -6.4926e-02, -7.7757e-03,  1.1495e-01,\n",
       "           9.4149e-02, -2.3633e-02, -1.2163e-01, -1.2336e-01,  3.3108e-02,\n",
       "          -1.1871e-01,  8.2898e-02,  8.5383e-02, -2.4239e-02, -6.3475e-02,\n",
       "          -1.1566e-02,  8.4548e-02, -5.8125e-02, -4.4513e-02, -1.1631e-01,\n",
       "           1.1604e-01,  6.0989e-02,  5.9020e-02,  1.1341e-01,  7.6590e-02,\n",
       "           2.4107e-03, -7.2023e-02, -5.4784e-02, -4.3780e-02, -5.7536e-02,\n",
       "           6.8348e-03, -7.5407e-02,  1.3653e-02,  6.7448e-02, -6.6428e-02,\n",
       "           4.3695e-02,  8.1294e-02, -2.9870e-03,  7.1306e-02, -1.5580e-02,\n",
       "           6.6839e-02, -4.9988e-02,  5.6134e-02,  4.1263e-02,  1.5714e-02,\n",
       "           6.0289e-02, -2.5122e-03, -8.4141e-02, -8.5266e-02],\n",
       "         [-1.0748e-01, -5.6645e-02, -3.2334e-02, -2.7108e-02, -8.8592e-02,\n",
       "          -5.0980e-02, -4.4061e-02,  3.0899e-03,  8.9900e-02,  1.0884e-01,\n",
       "          -4.1622e-02, -1.0281e-01,  2.6498e-02, -5.7849e-02,  5.4912e-02,\n",
       "           2.7852e-02, -1.0737e-01,  4.8418e-02, -4.3966e-02, -9.5856e-02,\n",
       "          -3.4583e-03,  7.8868e-02,  7.3344e-02,  4.2885e-02,  5.9553e-02,\n",
       "           2.4620e-02, -1.4716e-02,  4.2032e-02,  9.5928e-02, -5.3441e-02,\n",
       "           1.2204e-01,  7.1898e-02, -2.8873e-02,  8.2104e-04,  1.1107e-01,\n",
       "           6.6367e-03,  2.1571e-02, -5.9618e-03,  7.3020e-02, -9.7309e-02,\n",
       "           6.3471e-03,  5.5316e-02, -7.9691e-02, -3.8547e-02, -1.0750e-01,\n",
       "          -1.1858e-01, -1.0666e-01, -1.2666e-03,  1.0303e-01,  1.1346e-01,\n",
       "           7.8390e-03,  6.2834e-04, -1.1446e-01, -1.2340e-01,  7.0115e-02,\n",
       "          -4.4869e-02, -2.2566e-02,  9.9097e-02, -4.8591e-02,  2.8460e-02,\n",
       "          -1.1735e-01,  3.4157e-02, -1.1369e-01,  6.6992e-03],\n",
       "         [ 1.1764e-01, -6.5966e-02, -5.5567e-02, -3.7642e-02,  9.6769e-02,\n",
       "           8.5417e-02,  8.7295e-02,  1.0790e-01, -4.4225e-03,  6.4034e-02,\n",
       "          -1.1305e-01,  3.7838e-02,  6.3421e-02,  5.8378e-02,  3.3847e-02,\n",
       "          -4.6155e-02, -3.9808e-02, -4.4859e-02,  4.0545e-02,  5.7556e-02,\n",
       "          -3.3569e-02,  1.2266e-01,  8.3695e-02, -1.3000e-02, -6.3656e-02,\n",
       "           6.2968e-02, -1.8068e-02, -2.8671e-02, -1.0030e-02, -9.3286e-02,\n",
       "           4.1497e-02, -3.5650e-02,  4.6081e-02, -1.0675e-01, -8.5017e-02,\n",
       "          -8.3428e-02, -7.8417e-02, -4.5570e-02,  2.7974e-02, -7.4034e-02,\n",
       "          -9.2031e-02, -7.5289e-02, -1.2021e-01, -1.1165e-01, -9.0286e-02,\n",
       "           1.1068e-01,  7.3184e-02,  7.6544e-02,  9.0347e-02, -1.0850e-01,\n",
       "           1.1329e-01, -4.7618e-03,  3.3554e-02,  2.8478e-02,  3.6647e-02,\n",
       "           1.0246e-02,  5.1780e-02,  4.7557e-02, -7.1983e-02, -7.1860e-02,\n",
       "           5.8072e-02,  6.8901e-02, -1.4677e-02, -7.4743e-02],\n",
       "         [-6.8614e-02,  4.7681e-02,  5.7572e-02,  2.0083e-02, -1.2200e-01,\n",
       "          -3.2182e-02,  6.6930e-02,  4.0415e-02, -9.2176e-02, -6.7337e-02,\n",
       "           4.8150e-03,  1.0336e-01,  3.6622e-02,  5.9783e-02, -4.2968e-02,\n",
       "          -5.8406e-02,  8.1754e-02, -6.8211e-03, -3.5189e-02, -2.1711e-02,\n",
       "           4.3959e-02,  6.8246e-02,  1.2403e-01,  1.3944e-02,  7.6870e-03,\n",
       "          -7.9810e-02, -3.7773e-02, -1.1467e-01,  4.9830e-02, -2.1079e-02,\n",
       "           7.0373e-02, -1.1158e-01,  8.1731e-02,  1.3913e-02,  1.1652e-01,\n",
       "          -7.1022e-02, -7.6698e-02,  6.1848e-02, -2.6362e-03,  8.7855e-02,\n",
       "           5.4389e-02,  3.4891e-02, -2.9000e-02, -9.2299e-03, -1.6309e-02,\n",
       "           5.7451e-02,  8.7893e-02, -5.5459e-02, -2.7762e-02, -7.8854e-02,\n",
       "          -3.7797e-02, -5.5429e-02, -2.0427e-02, -1.2026e-01,  4.6492e-02,\n",
       "           5.9374e-02,  2.0960e-03,  4.4562e-03, -1.0623e-02,  5.8156e-02,\n",
       "          -1.1053e-01, -3.5028e-02,  6.0055e-02, -5.1421e-02],\n",
       "         [ 8.4890e-02,  9.7841e-02, -6.4513e-02,  1.1311e-01, -5.9951e-02,\n",
       "           9.2526e-02,  1.0721e-01, -1.0279e-01,  1.1382e-01,  8.1442e-02,\n",
       "          -3.7774e-02, -6.4003e-02, -1.1674e-01,  2.0188e-02,  3.6220e-02,\n",
       "          -3.2540e-02,  3.4178e-02,  5.1959e-02, -5.3990e-02,  7.3851e-02,\n",
       "          -7.6324e-02,  8.2948e-02,  6.7385e-02, -7.4543e-03, -5.5678e-02,\n",
       "          -1.0308e-01, -7.5244e-02, -6.6519e-02, -3.4642e-02,  9.9446e-02,\n",
       "          -7.7326e-03, -5.4135e-02,  5.1526e-02, -7.3779e-02, -5.5611e-05,\n",
       "           1.0061e-01,  8.6174e-03, -6.9537e-02,  1.1971e-01,  2.4033e-02,\n",
       "          -4.3833e-02,  6.3403e-02, -1.1438e-01, -1.2165e-01, -6.0341e-02,\n",
       "          -2.8438e-02, -6.0081e-02,  4.1210e-02, -2.8658e-02,  1.1214e-01,\n",
       "           3.6600e-02,  5.1290e-02,  3.8547e-02, -1.1215e-01, -1.2321e-01,\n",
       "           1.0677e-01, -5.3083e-02,  3.4138e-02,  9.0409e-02, -8.6268e-02,\n",
       "           8.0584e-02,  7.4657e-02, -9.0846e-02, -7.4520e-02],\n",
       "         [-3.4067e-02,  6.3681e-02,  4.4441e-02,  6.3634e-02, -1.0556e-01,\n",
       "          -6.6505e-02,  5.6046e-02, -8.0759e-02, -6.7465e-04,  1.1254e-01,\n",
       "           7.9396e-02,  8.9730e-02, -2.1804e-02,  5.3505e-02,  8.6916e-02,\n",
       "           7.0362e-02, -8.7187e-02, -1.0034e-01,  9.1671e-03,  2.5697e-04,\n",
       "          -5.3790e-02, -1.2349e-01, -7.6415e-02,  5.3962e-02, -1.7511e-02,\n",
       "          -8.4132e-02, -3.1767e-02, -9.4426e-02, -2.9810e-02, -2.7096e-02,\n",
       "          -4.7919e-03,  5.9596e-02,  1.9603e-02, -1.1738e-01,  7.5438e-02,\n",
       "          -1.1554e-01,  7.1409e-02, -7.8562e-02,  6.8243e-02, -8.8518e-02,\n",
       "           5.5974e-02,  1.0643e-01, -1.2346e-01, -9.1290e-02, -1.1750e-01,\n",
       "           2.1604e-02, -6.5463e-03,  6.1996e-02, -6.1692e-02,  9.9202e-02,\n",
       "           1.1762e-02,  5.2137e-03,  4.7737e-02, -5.4422e-03,  5.1725e-02,\n",
       "           8.7492e-02, -1.6536e-02,  7.1262e-02,  6.6900e-02,  9.5457e-03,\n",
       "           7.9669e-02,  4.5290e-02,  8.2182e-02,  7.7528e-04],\n",
       "         [ 9.6970e-02, -1.1399e-01, -1.0974e-01, -4.0137e-02, -5.8700e-02,\n",
       "          -4.3935e-02, -8.4361e-02,  3.7659e-02, -1.2388e-01,  7.9877e-02,\n",
       "          -8.4723e-02, -7.3966e-03, -7.9867e-02,  8.8500e-02,  7.9574e-02,\n",
       "          -7.5474e-02, -6.3832e-02, -5.6457e-02,  9.3484e-02, -6.9031e-02,\n",
       "          -3.6814e-02,  6.7703e-02,  2.6985e-02, -2.8627e-02, -4.2561e-02,\n",
       "           5.8862e-02,  6.7792e-02, -7.0475e-02, -1.1579e-01,  1.6733e-02,\n",
       "          -3.9781e-02,  2.4551e-02,  9.4942e-02, -1.0935e-02,  2.1422e-02,\n",
       "          -7.8607e-02, -4.8860e-03,  2.4464e-02,  1.1984e-01,  3.2253e-02,\n",
       "           1.2027e-01, -5.4153e-02, -1.2109e-01,  5.3994e-02,  1.1248e-01,\n",
       "           6.3749e-02, -8.9815e-02, -7.7282e-02, -1.1220e-01,  1.2254e-01,\n",
       "           7.4473e-02, -8.8094e-02, -6.5828e-02, -1.0985e-01,  1.0950e-01,\n",
       "           8.5297e-02, -4.2380e-02,  4.2046e-02, -4.9627e-02, -5.4770e-03,\n",
       "           4.5726e-02,  1.0871e-01,  2.3449e-02, -9.6047e-02],\n",
       "         [ 8.4026e-02, -7.0357e-02, -6.0242e-02, -7.7382e-02, -8.1813e-02,\n",
       "           7.0985e-02, -1.0216e-01, -7.5896e-02,  7.6295e-02, -6.2453e-03,\n",
       "          -3.4395e-02, -7.0797e-02, -7.5411e-02,  2.4344e-02,  8.9121e-03,\n",
       "           1.7406e-02,  3.6446e-02,  1.1743e-01, -7.7192e-02, -4.1304e-03,\n",
       "           1.2434e-01,  2.0702e-02,  1.2025e-01,  3.2579e-02,  1.1363e-02,\n",
       "           3.8471e-02,  3.6484e-02, -3.4567e-02, -8.9363e-02,  1.1113e-01,\n",
       "          -2.3596e-02,  3.1775e-02, -4.7967e-02, -1.1036e-01, -5.6587e-02,\n",
       "          -1.1626e-01,  4.2708e-02, -6.8331e-02,  1.2186e-01, -4.2592e-02,\n",
       "           7.6500e-02, -2.9522e-02,  3.6271e-02, -8.0686e-02,  1.0374e-01,\n",
       "           9.8671e-02,  6.0646e-02,  1.0212e-01,  2.5534e-02,  9.1584e-03,\n",
       "           4.7883e-02,  5.7924e-02,  2.4061e-02,  5.2323e-02,  9.4900e-04,\n",
       "          -7.3082e-02,  4.8986e-02, -1.0005e-01,  1.9935e-02, -1.9855e-02,\n",
       "           5.5689e-02, -6.0385e-02, -9.1594e-02, -6.8968e-02],\n",
       "         [-9.4020e-02, -4.0723e-02,  8.1763e-02,  4.8583e-02,  1.1553e-01,\n",
       "          -2.8682e-02, -1.9395e-02,  7.4767e-02,  2.8819e-02,  2.7880e-02,\n",
       "          -1.0859e-01, -6.6644e-02, -9.1812e-02, -4.4409e-02,  7.0646e-02,\n",
       "          -5.3860e-02,  2.7895e-02, -1.0441e-01, -1.1254e-01,  4.6217e-02,\n",
       "          -2.1196e-02,  2.1207e-02,  2.0334e-02, -2.1089e-02, -1.1081e-01,\n",
       "          -1.0278e-01,  3.6402e-02, -6.0609e-03, -3.1175e-02, -2.2176e-02,\n",
       "           1.0864e-01,  7.1187e-02,  1.1348e-01,  6.4335e-03, -1.0096e-01,\n",
       "          -8.5887e-02,  8.1107e-02, -5.5296e-03,  1.3845e-02, -6.5645e-02,\n",
       "           3.3763e-02,  8.0661e-02, -4.4193e-02, -3.7159e-02,  1.1920e-01,\n",
       "           1.0632e-01,  6.8428e-02,  6.1747e-02, -5.3910e-02, -9.0291e-02,\n",
       "           6.0563e-02,  5.9438e-02, -3.1692e-02, -5.9413e-02,  4.5624e-02,\n",
       "           4.9711e-02, -1.0366e-01,  1.8317e-02,  1.1758e-01, -7.0445e-02,\n",
       "          -7.0631e-02,  4.3002e-02,  5.9945e-02,  5.2059e-02],\n",
       "         [ 1.0017e-01, -4.7041e-02,  6.4017e-02,  3.3204e-02,  8.0222e-03,\n",
       "           6.3059e-02,  2.5375e-03,  8.5376e-03, -8.3466e-02,  7.8056e-02,\n",
       "          -4.3365e-02, -4.4817e-02, -9.5344e-02, -5.1539e-02,  4.5641e-02,\n",
       "          -1.3628e-02, -3.2809e-03, -6.4075e-02,  3.5716e-02, -1.0427e-01,\n",
       "           1.1010e-01, -2.0377e-02, -1.7449e-02,  5.9980e-02, -9.5228e-02,\n",
       "           9.2333e-02, -7.3687e-03,  1.1044e-01,  8.0889e-02,  7.0492e-02,\n",
       "           1.1039e-01,  1.2087e-01,  1.1563e-01, -6.0698e-02, -7.1237e-03,\n",
       "          -7.1476e-02,  1.2302e-01,  1.2294e-01, -5.9863e-03,  6.7081e-02,\n",
       "           4.6690e-02,  1.0396e-01,  1.1490e-01,  7.2395e-02,  4.3254e-02,\n",
       "           1.8441e-02,  7.2417e-02,  1.1256e-01,  1.2090e-01,  4.7212e-02,\n",
       "          -4.4332e-03,  7.9028e-02,  4.0979e-02,  1.1572e-01, -4.5096e-02,\n",
       "           8.4188e-02,  3.8350e-03, -1.1297e-01, -8.4361e-03,  1.2299e-01,\n",
       "          -2.1899e-02,  7.3406e-02, -3.3796e-03, -6.5557e-02]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0536,  0.0356,  0.0649, -0.1155,  0.1129,  0.0442,  0.0727,  0.0529,\n",
       "          0.0806, -0.1025], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    correct = 0 \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            batch_losses.append(loss)     \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train Loss: 0.8271\tTest Loss: 0.3808\tAccuracy: 87.68%\n",
      "[2] Train Loss: 0.2517\tTest Loss: 0.1967\tAccuracy: 93.95%\n",
      "[3] Train Loss: 0.1715\tTest Loss: 0.1615\tAccuracy: 94.79%\n",
      "[4] Train Loss: 0.1350\tTest Loss: 0.1588\tAccuracy: 95.30%\n",
      "[5] Train Loss: 0.1107\tTest Loss: 0.1328\tAccuracy: 95.64%\n",
      "[6] Train Loss: 0.0938\tTest Loss: 0.1250\tAccuracy: 95.87%\n",
      "[7] Train Loss: 0.0804\tTest Loss: 0.2637\tAccuracy: 91.95%\n",
      "[8] Train Loss: 0.3573\tTest Loss: 0.1660\tAccuracy: 94.61%\n",
      "[9] Train Loss: 0.0994\tTest Loss: 0.1119\tAccuracy: 96.40%\n",
      "[10] Train Loss: 0.0794\tTest Loss: 0.1208\tAccuracy: 96.46%\n",
      "[11] Train Loss: 0.0688\tTest Loss: 0.2296\tAccuracy: 93.30%\n",
      "[12] Train Loss: 0.0633\tTest Loss: 0.0807\tAccuracy: 97.51%\n",
      "[13] Train Loss: 0.0531\tTest Loss: 0.1673\tAccuracy: 95.00%\n",
      "[14] Train Loss: 0.5788\tTest Loss: 0.1832\tAccuracy: 94.39%\n",
      "[15] Train Loss: 0.1096\tTest Loss: 0.1640\tAccuracy: 94.82%\n",
      "[16] Train Loss: 0.0864\tTest Loss: 0.1143\tAccuracy: 96.55%\n",
      "[17] Train Loss: 0.0751\tTest Loss: 0.1590\tAccuracy: 95.22%\n",
      "[18] Train Loss: 0.0668\tTest Loss: 0.0961\tAccuracy: 97.11%\n",
      "[19] Train Loss: 0.0601\tTest Loss: 0.1081\tAccuracy: 96.49%\n",
      "[20] Train Loss: 0.0542\tTest Loss: 0.1334\tAccuracy: 95.69%\n",
      "[21] Train Loss: 0.0504\tTest Loss: 0.1287\tAccuracy: 96.31%\n",
      "[22] Train Loss: 0.0483\tTest Loss: 0.1179\tAccuracy: 96.57%\n",
      "[23] Train Loss: 0.0439\tTest Loss: 0.0927\tAccuracy: 97.14%\n",
      "[24] Train Loss: 0.0417\tTest Loss: 0.1041\tAccuracy: 96.84%\n",
      "[25] Train Loss: 0.0376\tTest Loss: 0.0985\tAccuracy: 97.22%\n",
      "[26] Train Loss: 0.0356\tTest Loss: 0.0897\tAccuracy: 97.45%\n",
      "[27] Train Loss: 0.0315\tTest Loss: 0.0921\tAccuracy: 97.39%\n",
      "[28] Train Loss: 0.0309\tTest Loss: 0.2259\tAccuracy: 93.74%\n",
      "[29] Train Loss: 1.3906\tTest Loss: 0.2488\tAccuracy: 93.06%\n",
      "[30] Train Loss: 0.2252\tTest Loss: 0.2306\tAccuracy: 92.77%\n",
      "[31] Train Loss: 0.1566\tTest Loss: 0.2772\tAccuracy: 90.98%\n",
      "[32] Train Loss: 0.1236\tTest Loss: 0.1431\tAccuracy: 95.74%\n",
      "[33] Train Loss: 0.1069\tTest Loss: 0.2360\tAccuracy: 92.77%\n",
      "[34] Train Loss: 0.1198\tTest Loss: 0.1589\tAccuracy: 95.06%\n",
      "[35] Train Loss: 0.0889\tTest Loss: 0.1340\tAccuracy: 95.99%\n",
      "[36] Train Loss: 0.0799\tTest Loss: 0.1068\tAccuracy: 96.77%\n",
      "[37] Train Loss: 0.0756\tTest Loss: 0.1065\tAccuracy: 97.03%\n",
      "[38] Train Loss: 0.0704\tTest Loss: 0.1924\tAccuracy: 93.97%\n",
      "[39] Train Loss: 0.0658\tTest Loss: 0.1099\tAccuracy: 96.80%\n",
      "[40] Train Loss: 0.0613\tTest Loss: 0.1888\tAccuracy: 94.63%\n",
      "[41] Train Loss: 0.0629\tTest Loss: 0.1143\tAccuracy: 96.86%\n",
      "[42] Train Loss: 0.0554\tTest Loss: 0.1436\tAccuracy: 96.03%\n",
      "[43] Train Loss: 0.0536\tTest Loss: 0.1147\tAccuracy: 96.66%\n",
      "[44] Train Loss: 0.0492\tTest Loss: 0.1077\tAccuracy: 96.89%\n",
      "[45] Train Loss: 0.0484\tTest Loss: 0.0982\tAccuracy: 97.22%\n",
      "[46] Train Loss: 0.0452\tTest Loss: 0.1188\tAccuracy: 96.59%\n",
      "[47] Train Loss: 0.0434\tTest Loss: 0.2959\tAccuracy: 92.56%\n",
      "[48] Train Loss: 0.0456\tTest Loss: 0.1004\tAccuracy: 97.24%\n",
      "[49] Train Loss: 0.0391\tTest Loss: 0.1023\tAccuracy: 97.18%\n",
      "[50] Train Loss: 0.0371\tTest Loss: 0.0971\tAccuracy: 97.29%\n",
      "[51] Train Loss: 0.0372\tTest Loss: 0.1075\tAccuracy: 97.13%\n",
      "[52] Train Loss: 0.0331\tTest Loss: 0.0971\tAccuracy: 97.43%\n",
      "[53] Train Loss: 0.0335\tTest Loss: 0.1270\tAccuracy: 96.67%\n",
      "[54] Train Loss: 0.0328\tTest Loss: 0.1526\tAccuracy: 96.19%\n",
      "[55] Train Loss: 0.0311\tTest Loss: 0.1132\tAccuracy: 96.98%\n",
      "[56] Train Loss: 0.0286\tTest Loss: 0.1096\tAccuracy: 97.14%\n",
      "[57] Train Loss: 0.0271\tTest Loss: 0.1535\tAccuracy: 96.10%\n",
      "[58] Train Loss: 0.0281\tTest Loss: 0.1079\tAccuracy: 97.12%\n",
      "[59] Train Loss: 0.0237\tTest Loss: 0.1074\tAccuracy: 97.29%\n",
      "[60] Train Loss: 0.0229\tTest Loss: 0.1096\tAccuracy: 97.21%\n",
      "[61] Train Loss: 0.0223\tTest Loss: 0.1101\tAccuracy: 97.35%\n",
      "[62] Train Loss: 0.1424\tTest Loss: 0.2168\tAccuracy: 93.54%\n",
      "[63] Train Loss: 0.0774\tTest Loss: 0.1394\tAccuracy: 96.14%\n",
      "[64] Train Loss: 0.0421\tTest Loss: 0.1209\tAccuracy: 97.01%\n",
      "[65] Train Loss: 0.0309\tTest Loss: 0.1121\tAccuracy: 97.37%\n",
      "[66] Train Loss: 0.0262\tTest Loss: 0.1175\tAccuracy: 97.25%\n",
      "[67] Train Loss: 0.0239\tTest Loss: 0.1115\tAccuracy: 97.45%\n",
      "[68] Train Loss: 0.0219\tTest Loss: 0.1189\tAccuracy: 97.28%\n",
      "[69] Train Loss: 0.0211\tTest Loss: 0.1152\tAccuracy: 97.31%\n",
      "[70] Train Loss: 0.0193\tTest Loss: 0.1156\tAccuracy: 97.22%\n",
      "[71] Train Loss: 0.0178\tTest Loss: 0.1314\tAccuracy: 96.95%\n",
      "[72] Train Loss: 0.0176\tTest Loss: 0.1272\tAccuracy: 97.03%\n",
      "[73] Train Loss: 0.0161\tTest Loss: 0.1230\tAccuracy: 97.03%\n",
      "[74] Train Loss: 0.0152\tTest Loss: 0.1275\tAccuracy: 97.22%\n",
      "[75] Train Loss: 0.0140\tTest Loss: 0.1240\tAccuracy: 97.29%\n",
      "[76] Train Loss: 0.0141\tTest Loss: 0.1209\tAccuracy: 97.22%\n",
      "[77] Train Loss: 0.0126\tTest Loss: 0.1201\tAccuracy: 97.39%\n",
      "[78] Train Loss: 0.0122\tTest Loss: 0.1261\tAccuracy: 97.31%\n",
      "[79] Train Loss: 0.0114\tTest Loss: 0.1355\tAccuracy: 97.04%\n",
      "[80] Train Loss: 0.0105\tTest Loss: 0.1313\tAccuracy: 97.38%\n",
      "[81] Train Loss: 0.0104\tTest Loss: 0.2488\tAccuracy: 95.07%\n",
      "[82] Train Loss: 1.1678\tTest Loss: 0.5542\tAccuracy: 82.73%\n",
      "[83] Train Loss: 0.2570\tTest Loss: 0.3314\tAccuracy: 89.98%\n",
      "[84] Train Loss: 0.2032\tTest Loss: 0.2150\tAccuracy: 93.53%\n",
      "[85] Train Loss: 0.1698\tTest Loss: 0.2078\tAccuracy: 93.70%\n",
      "[86] Train Loss: 0.1497\tTest Loss: 0.2112\tAccuracy: 93.80%\n",
      "[87] Train Loss: 0.1331\tTest Loss: 0.2085\tAccuracy: 93.59%\n",
      "[88] Train Loss: 0.1197\tTest Loss: 0.1506\tAccuracy: 95.58%\n",
      "[89] Train Loss: 0.1104\tTest Loss: 0.1451\tAccuracy: 95.88%\n",
      "[90] Train Loss: 0.1022\tTest Loss: 0.1566\tAccuracy: 95.55%\n",
      "[91] Train Loss: 0.0970\tTest Loss: 0.1758\tAccuracy: 95.00%\n",
      "[92] Train Loss: 0.0904\tTest Loss: 0.2237\tAccuracy: 93.16%\n",
      "[93] Train Loss: 0.0848\tTest Loss: 0.1230\tAccuracy: 96.42%\n",
      "[94] Train Loss: 0.0793\tTest Loss: 0.1538\tAccuracy: 95.73%\n",
      "[95] Train Loss: 0.0764\tTest Loss: 0.1239\tAccuracy: 96.51%\n",
      "[96] Train Loss: 0.0717\tTest Loss: 0.1747\tAccuracy: 94.90%\n",
      "[97] Train Loss: 0.0722\tTest Loss: 0.1718\tAccuracy: 95.03%\n",
      "[98] Train Loss: 0.0674\tTest Loss: 0.1274\tAccuracy: 96.39%\n",
      "[99] Train Loss: 0.0647\tTest Loss: 0.1925\tAccuracy: 94.65%\n",
      "[100] Train Loss: 0.0617\tTest Loss: 0.1208\tAccuracy: 96.59%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
    "          epoch, train_loss, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaqqRzF73oBu"
   },
   "source": [
    "## 과제 4\n",
    "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
    "\n",
    "- Hint : Activation function, hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "k6b82DZG6W3j"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Linear(784,128)\n",
    "        self.layer2 = nn.Linear(128,64)\n",
    "        self.layer3 = nn.Linear(64,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0017, -0.0350, -0.0138,  ...,  0.0004, -0.0261, -0.0273],\n",
       "         [-0.0267, -0.0274,  0.0013,  ..., -0.0279,  0.0163, -0.0325],\n",
       "         [-0.0050,  0.0159,  0.0221,  ...,  0.0110, -0.0108,  0.0058],\n",
       "         ...,\n",
       "         [ 0.0312,  0.0227, -0.0290,  ...,  0.0111, -0.0259, -0.0119],\n",
       "         [ 0.0171,  0.0321,  0.0337,  ..., -0.0256,  0.0100, -0.0199],\n",
       "         [-0.0297, -0.0092,  0.0235,  ...,  0.0024, -0.0073,  0.0195]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.6172e-02, -1.7031e-02,  1.0837e-02,  3.0965e-02, -5.4007e-03,\n",
       "          1.8564e-02,  3.5060e-02, -2.1299e-02,  3.1358e-02,  2.3605e-02,\n",
       "          1.9931e-02, -1.3445e-02, -3.7950e-03, -2.7798e-02, -3.5247e-02,\n",
       "          8.0492e-03,  3.0886e-02, -2.0439e-02,  1.1902e-02,  1.4418e-02,\n",
       "         -3.4872e-02,  2.9560e-02,  2.5756e-02, -2.5807e-02, -2.5183e-02,\n",
       "         -1.6014e-02, -1.0088e-02,  7.4029e-03, -1.2149e-02, -2.6508e-02,\n",
       "          1.3365e-02, -3.0130e-03,  2.7232e-02,  3.1895e-02, -1.0810e-02,\n",
       "         -2.2215e-02,  3.3847e-02, -1.6035e-02, -1.8888e-02, -3.1013e-02,\n",
       "          7.7726e-03,  1.9314e-04, -9.8611e-03, -1.7022e-02,  6.9609e-03,\n",
       "         -3.0051e-02, -2.2985e-02,  3.1530e-02,  6.8771e-03, -2.6510e-02,\n",
       "         -1.0432e-02,  2.5045e-02, -1.8529e-02,  1.1993e-02, -2.4244e-02,\n",
       "         -1.6157e-02,  1.2408e-03,  9.7086e-03,  7.8793e-03,  6.5300e-03,\n",
       "          2.8607e-02, -2.5786e-02,  2.8582e-02, -2.7554e-02, -1.3398e-02,\n",
       "          9.1258e-03,  2.0611e-02, -3.9330e-03,  2.0360e-02,  3.3982e-02,\n",
       "         -2.7275e-02,  2.5327e-02, -3.4120e-02,  1.1785e-02, -2.7458e-02,\n",
       "         -2.2990e-04, -2.9888e-02,  5.3185e-03, -2.1913e-02,  1.4817e-02,\n",
       "         -3.4160e-02,  2.6889e-02,  2.7349e-02, -3.5315e-02,  3.4923e-02,\n",
       "         -1.2859e-02, -7.2382e-03,  1.3334e-02, -1.6328e-02,  9.3676e-05,\n",
       "         -2.7053e-02, -1.7703e-02, -2.1214e-02, -5.7767e-03, -1.7225e-02,\n",
       "          6.0126e-03,  3.4324e-02, -1.3133e-02, -1.6433e-02, -3.3611e-02,\n",
       "          3.1435e-02, -3.2424e-02, -1.2887e-02, -6.8405e-03,  9.1622e-03,\n",
       "         -2.2992e-02, -3.4701e-02, -4.3394e-03,  1.4571e-02,  2.8736e-02,\n",
       "         -7.3237e-03, -8.1834e-03,  1.8898e-02, -2.7287e-02, -3.0390e-02,\n",
       "         -2.7758e-02, -2.6413e-02,  1.0284e-02, -1.9517e-03, -1.4516e-02,\n",
       "         -1.9581e-02, -7.8155e-03, -1.4850e-02,  3.3515e-02,  2.7600e-02,\n",
       "         -6.2819e-03,  1.0140e-02, -4.8822e-03], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0729,  0.0724, -0.0825,  ...,  0.0216,  0.0514, -0.0246],\n",
       "         [-0.0422,  0.0349,  0.0300,  ..., -0.0539,  0.0789, -0.0316],\n",
       "         [ 0.0630,  0.0493,  0.0088,  ..., -0.0639, -0.0198, -0.0074],\n",
       "         ...,\n",
       "         [ 0.0150, -0.0369, -0.0097,  ...,  0.0032, -0.0786, -0.0612],\n",
       "         [-0.0463,  0.0265, -0.0644,  ...,  0.0787, -0.0448, -0.0259],\n",
       "         [-0.0476,  0.0788, -0.0375,  ...,  0.0061, -0.0187, -0.0395]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0849,  0.0248,  0.0800,  0.0392,  0.0307, -0.0703, -0.0275, -0.0416,\n",
       "          0.0770,  0.0852,  0.0876,  0.0246, -0.0073, -0.0843,  0.0175, -0.0548,\n",
       "         -0.0672, -0.0841,  0.0789,  0.0138,  0.0402, -0.0655,  0.0740,  0.0150,\n",
       "         -0.0653,  0.0727,  0.0608,  0.0881, -0.0610,  0.0617, -0.0251,  0.0107,\n",
       "          0.0463, -0.0319,  0.0595,  0.0328, -0.0865, -0.0280, -0.0676,  0.0149,\n",
       "          0.0343, -0.0785,  0.0305,  0.0225,  0.0587,  0.0674,  0.0392,  0.0389,\n",
       "          0.0676,  0.0173, -0.0174,  0.0689,  0.0449, -0.0007,  0.0007,  0.0586,\n",
       "         -0.0850,  0.0418, -0.0180, -0.0044,  0.0208, -0.0081,  0.0796, -0.0093],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 1.1173e-01, -1.1122e-01, -2.8712e-02,  1.1841e-01,  7.3293e-02,\n",
       "          -9.9052e-02,  5.7481e-02,  4.6052e-02, -9.4018e-02, -8.6014e-02,\n",
       "          -4.9996e-02,  4.1352e-02,  6.0922e-03,  1.1414e-01,  6.6995e-02,\n",
       "          -5.2419e-02,  6.5256e-02,  9.0589e-02, -5.3742e-02, -7.0782e-02,\n",
       "           3.4688e-02, -1.0411e-01,  4.0785e-02, -7.9899e-03, -9.9705e-02,\n",
       "          -9.6351e-02, -5.9971e-03,  8.2243e-02,  2.4887e-02,  7.0396e-02,\n",
       "          -1.7288e-02,  2.3052e-02, -4.3748e-02,  1.3494e-02, -9.1377e-02,\n",
       "           4.7619e-02,  9.6350e-02,  8.8995e-02, -1.1697e-01, -1.1120e-01,\n",
       "           1.6971e-02,  2.4605e-03, -4.8527e-02,  9.3855e-02,  1.6270e-02,\n",
       "          -1.2367e-01, -1.2153e-01,  4.9238e-02,  2.6303e-02,  4.8682e-02,\n",
       "          -1.1381e-01,  1.0490e-01, -1.2684e-02, -3.7250e-02,  1.0145e-01,\n",
       "           4.3979e-02,  8.2381e-02,  8.0539e-02, -8.6008e-02, -7.1841e-02,\n",
       "          -1.0991e-01,  1.8514e-02,  7.4216e-02,  4.6998e-02],\n",
       "         [-4.0513e-02,  8.7768e-02, -7.3948e-02,  3.8421e-02,  8.6533e-02,\n",
       "           1.0000e-01, -3.6944e-02,  9.4712e-02,  6.8468e-02,  1.0824e-02,\n",
       "          -1.1625e-01, -4.8079e-02,  2.1388e-02, -1.0770e-01,  9.5537e-02,\n",
       "          -6.9040e-02,  2.7212e-02,  4.3069e-02, -5.5020e-02, -5.8292e-02,\n",
       "           4.5269e-03,  1.2123e-01, -8.1423e-02, -1.1484e-01,  1.1691e-01,\n",
       "          -2.2294e-02, -7.0809e-02,  8.8011e-02, -7.0463e-02,  1.1723e-01,\n",
       "          -1.0294e-02, -1.0423e-01,  4.0835e-02,  6.4949e-02,  7.8171e-02,\n",
       "          -4.0218e-02, -9.0066e-02, -1.2357e-01,  7.8259e-02,  4.2026e-02,\n",
       "          -2.2626e-03, -6.5254e-02,  4.9315e-02,  1.7564e-02,  1.0595e-01,\n",
       "          -1.0990e-01, -8.2692e-03, -7.8963e-02,  6.5603e-02,  1.1036e-01,\n",
       "           2.4012e-02, -5.8845e-02,  1.1953e-01,  5.6711e-02, -4.6780e-02,\n",
       "           1.1162e-01, -1.6710e-02,  1.9332e-02,  1.0331e-01, -5.9097e-02,\n",
       "          -1.1972e-01, -8.8711e-02, -6.3577e-02,  1.6699e-02],\n",
       "         [-4.5202e-02, -1.1520e-01, -1.1692e-01,  1.4664e-02,  1.0767e-01,\n",
       "          -2.7998e-02,  1.0059e-01,  2.6225e-02, -9.5306e-02, -5.3454e-02,\n",
       "           8.9594e-02, -4.7357e-02,  5.3138e-02, -1.1043e-02, -1.1048e-01,\n",
       "          -8.6804e-02,  1.0508e-01,  1.1733e-01, -3.5972e-02,  4.0220e-02,\n",
       "          -1.1441e-02, -9.6796e-02, -1.0174e-01,  7.7835e-03, -7.6062e-02,\n",
       "           7.5191e-02,  1.0168e-01,  6.3588e-02,  6.7424e-02,  9.7368e-02,\n",
       "           2.0833e-02,  2.2453e-02, -4.9191e-02,  9.9522e-02, -2.7668e-02,\n",
       "          -8.6170e-02, -1.0798e-01,  1.8785e-02,  5.4396e-02,  1.4625e-02,\n",
       "           2.0529e-02,  4.7450e-02,  2.8192e-02,  6.0829e-02, -4.2540e-02,\n",
       "           1.0507e-01,  1.1284e-01,  1.0765e-01,  1.2171e-01, -6.3665e-02,\n",
       "           2.0070e-02, -8.7601e-02,  1.2450e-01, -5.4165e-02, -5.0205e-02,\n",
       "           4.9003e-02, -6.7039e-02,  4.2285e-02, -8.9984e-02,  5.9180e-02,\n",
       "           6.7889e-02,  8.0612e-02, -1.3102e-02, -7.7452e-03],\n",
       "         [-6.9635e-02, -7.7159e-02, -6.9491e-02,  5.0050e-02,  5.1643e-02,\n",
       "           8.6824e-02,  1.1915e-01, -5.3074e-02,  5.5305e-02,  3.9799e-02,\n",
       "          -8.5809e-02,  5.9054e-02,  2.6350e-02, -2.0690e-02, -1.5809e-03,\n",
       "          -2.7199e-02,  2.7686e-02, -1.1226e-01,  8.7333e-02, -2.7640e-02,\n",
       "          -6.3797e-02, -9.9708e-02,  1.0729e-01, -1.0148e-01, -2.0727e-02,\n",
       "          -1.1041e-01,  1.2714e-02, -1.1173e-01,  6.3130e-02, -4.6745e-02,\n",
       "           9.7401e-02,  7.3108e-02,  8.7445e-03, -4.9095e-02,  4.9716e-02,\n",
       "           6.5172e-02,  8.1299e-02, -7.3593e-02,  2.5063e-03,  9.4461e-02,\n",
       "           1.1545e-01, -6.9176e-02,  5.7000e-02, -4.7067e-02,  9.1296e-02,\n",
       "          -9.2329e-02, -9.0742e-02,  6.9118e-02, -5.8830e-02,  6.1244e-02,\n",
       "          -5.9206e-02, -2.3656e-02,  6.2897e-02,  9.7834e-02, -3.8109e-02,\n",
       "          -1.0002e-01, -8.6094e-02, -7.9708e-02,  4.7994e-02, -5.2303e-02,\n",
       "          -5.6167e-02,  5.6267e-02, -5.2466e-02,  1.9758e-03],\n",
       "         [ 4.8135e-02, -1.1664e-01,  5.5620e-02, -8.3304e-02, -5.3608e-02,\n",
       "           7.4497e-02, -2.7418e-02,  2.1975e-02,  9.8519e-02,  1.9581e-02,\n",
       "           6.6975e-03,  1.1348e-01, -8.4984e-02, -1.0660e-01,  9.7296e-02,\n",
       "           6.6125e-02, -4.9377e-02, -1.1516e-01, -6.6514e-02, -1.8463e-03,\n",
       "           7.5769e-02,  1.0897e-01, -1.1328e-01,  4.2126e-02,  5.3614e-02,\n",
       "          -7.7749e-02,  5.4391e-02, -2.2323e-02, -1.0471e-01,  9.8561e-02,\n",
       "           3.3526e-02,  3.8378e-02,  5.9162e-02, -3.8878e-02,  4.4335e-03,\n",
       "           4.3991e-02,  5.8106e-02,  1.9979e-03, -3.2223e-02, -1.9663e-02,\n",
       "          -9.5342e-02, -8.4838e-02, -8.8846e-02, -7.2221e-02, -1.2145e-01,\n",
       "           5.0140e-03,  7.4581e-02, -1.1777e-01,  5.0879e-02,  9.0464e-02,\n",
       "          -1.1811e-01, -1.0707e-01,  6.5120e-02, -7.8125e-03, -6.8927e-02,\n",
       "          -1.0862e-01,  1.5216e-02,  1.6093e-02, -7.8680e-02, -8.8044e-02,\n",
       "          -1.1324e-01,  8.4563e-03,  2.3728e-02, -3.0857e-02],\n",
       "         [-9.7941e-02,  3.1633e-02, -1.0623e-04,  1.2373e-01,  4.5577e-02,\n",
       "           7.0127e-02, -7.5596e-02, -1.1053e-01,  8.2509e-03, -1.0017e-01,\n",
       "           1.2306e-01,  5.1015e-02, -8.6224e-02,  1.1573e-01,  9.1546e-02,\n",
       "          -9.8508e-02,  1.0354e-01, -6.6240e-02, -8.8713e-02,  2.3072e-02,\n",
       "           4.7462e-02,  9.4645e-02,  1.1286e-01, -1.2168e-01, -1.2962e-02,\n",
       "           4.9626e-02,  7.7070e-02, -1.8475e-02,  3.4920e-02,  1.2281e-02,\n",
       "          -8.3786e-02,  2.2977e-03, -5.4876e-02, -1.1478e-01, -4.6512e-02,\n",
       "           1.0294e-01,  4.2018e-02, -4.5143e-02, -8.7675e-02,  1.1295e-01,\n",
       "          -6.7738e-02,  1.1328e-01,  3.9802e-02,  5.0161e-02,  4.1735e-02,\n",
       "           9.8652e-02,  3.3512e-02,  7.8498e-02,  7.8638e-02, -4.9371e-02,\n",
       "          -9.8778e-02, -2.8387e-02, -5.2415e-02, -1.1201e-02, -1.8300e-02,\n",
       "          -4.4542e-02, -7.5187e-02, -1.1225e-01, -9.1028e-02, -1.2714e-02,\n",
       "           9.9190e-02,  3.8096e-02,  4.6796e-02,  3.2806e-02],\n",
       "         [-6.0505e-02, -5.2708e-02,  1.0641e-01, -3.9626e-02, -3.6523e-02,\n",
       "           8.9471e-04,  1.0745e-02,  1.9604e-02,  4.6168e-02,  7.2313e-02,\n",
       "           7.9704e-02,  1.1733e-01,  5.1080e-02, -1.0835e-01, -3.7905e-02,\n",
       "           1.0259e-01,  9.9978e-03, -1.5814e-02,  2.3437e-02,  3.7012e-02,\n",
       "           1.2239e-01,  9.8446e-02,  2.5392e-02, -5.7797e-02,  1.1081e-01,\n",
       "          -9.8283e-02,  1.4892e-02, -7.1583e-03,  2.9868e-02,  5.3780e-02,\n",
       "           5.6404e-02, -7.9236e-02, -7.0002e-02, -7.4316e-02,  5.0384e-02,\n",
       "           4.5970e-02, -2.3041e-02,  1.0466e-01,  1.1497e-01,  1.0563e-02,\n",
       "          -3.0149e-02,  3.6260e-02, -7.3647e-02, -2.4747e-02, -8.8180e-02,\n",
       "           5.8517e-02,  5.2184e-02,  2.2295e-02,  9.7656e-02,  4.3963e-02,\n",
       "           5.9402e-02, -1.0542e-01,  1.3104e-02, -2.4074e-02, -8.1310e-03,\n",
       "           5.4367e-02,  2.7386e-02,  1.8368e-02,  2.6550e-03,  6.2021e-02,\n",
       "           1.7665e-02, -8.9554e-02, -3.3932e-02,  7.8174e-02],\n",
       "         [ 6.8078e-02,  1.6247e-02,  1.5763e-02, -5.7401e-02, -6.7987e-02,\n",
       "          -1.1517e-01, -1.2409e-01,  8.3224e-02,  3.2510e-02,  1.0558e-01,\n",
       "           1.2406e-01,  5.0481e-02, -5.8419e-02, -3.6072e-02, -1.8678e-02,\n",
       "           7.0091e-02, -5.8328e-02, -5.2825e-02,  9.4566e-02, -1.4020e-02,\n",
       "           1.1007e-01,  9.6317e-02, -2.2526e-02,  6.4358e-02, -2.3786e-02,\n",
       "           1.0990e-01, -1.0171e-01, -1.2298e-01,  7.0628e-02,  7.3974e-02,\n",
       "          -4.0658e-03, -8.9350e-02, -1.6805e-02, -1.1065e-01,  6.7721e-02,\n",
       "           4.8564e-02, -4.6538e-02, -1.2179e-01,  8.0373e-02, -1.2231e-01,\n",
       "          -1.0117e-02,  1.7236e-02,  2.8872e-02, -1.2302e-01,  6.8897e-02,\n",
       "           5.7851e-02,  5.6234e-02, -5.0752e-02,  6.8390e-02, -6.4784e-02,\n",
       "           8.5503e-02,  6.6972e-02,  4.9408e-02,  2.0033e-02,  6.1899e-02,\n",
       "          -6.4964e-02,  1.5356e-02, -6.4805e-05,  3.4997e-02,  7.1069e-02,\n",
       "          -6.0080e-02,  3.1347e-02,  1.1419e-01,  5.1080e-02],\n",
       "         [-3.0403e-03, -3.1253e-02,  7.3020e-02, -9.0225e-02,  1.7397e-04,\n",
       "           1.0582e-01, -1.4728e-02,  9.7212e-02,  7.6625e-02, -4.7010e-03,\n",
       "           4.6490e-02, -6.0657e-02, -5.7562e-02,  2.4882e-02, -9.7982e-02,\n",
       "           5.3335e-02, -4.0565e-03,  2.4007e-02, -1.4017e-02, -6.3173e-02,\n",
       "          -9.0204e-02, -1.3975e-02, -5.1795e-02,  4.6125e-02, -6.1021e-02,\n",
       "          -1.0650e-01, -4.9304e-02, -1.0233e-01,  1.2888e-02,  6.7809e-02,\n",
       "           1.0567e-01, -1.8557e-02, -5.4957e-02, -5.3249e-02,  1.1542e-01,\n",
       "           1.0321e-01, -9.0859e-02,  9.5979e-02, -1.2330e-01,  7.3808e-02,\n",
       "           7.1369e-02,  6.4788e-02, -8.6945e-02, -4.0094e-02,  3.4173e-02,\n",
       "          -4.3585e-02,  3.4895e-02, -4.5228e-03,  1.1529e-01,  2.6892e-04,\n",
       "           9.8309e-02, -1.6991e-02,  5.9058e-02,  5.1886e-02,  1.0216e-01,\n",
       "          -3.9626e-03,  7.3526e-02,  2.6260e-02, -2.8178e-02, -5.4869e-02,\n",
       "           8.8921e-02, -4.0688e-02,  9.6868e-02, -1.1098e-01],\n",
       "         [ 9.8137e-02,  8.8526e-02, -1.6502e-02,  1.2183e-01, -6.1949e-02,\n",
       "          -3.6050e-02,  9.2440e-03,  3.2641e-02,  5.3735e-02,  1.1156e-01,\n",
       "           7.8259e-02,  2.8894e-02, -1.2218e-01, -3.8035e-02, -9.1739e-02,\n",
       "          -4.2086e-02,  5.5714e-02,  7.5995e-02, -7.5769e-02, -6.2647e-02,\n",
       "           1.1558e-01,  1.2171e-01,  2.9763e-02, -3.2382e-02, -1.5681e-02,\n",
       "          -8.4905e-02, -9.7763e-02,  2.4608e-02,  8.9968e-02,  6.9416e-02,\n",
       "          -1.0406e-01,  3.1096e-02,  4.2760e-02, -1.4849e-02,  6.8822e-03,\n",
       "           8.1773e-02,  6.7244e-02,  9.0532e-02, -2.2861e-02, -1.0429e-01,\n",
       "           5.8810e-02, -5.9736e-02, -5.1920e-02,  1.0409e-01,  8.2531e-02,\n",
       "          -7.2669e-02,  3.1429e-02, -1.1319e-01,  1.0168e-01, -6.9271e-02,\n",
       "          -1.8104e-02,  7.4747e-02,  4.3430e-02, -1.7257e-02, -4.4539e-02,\n",
       "           4.1138e-02, -8.1453e-02, -7.0653e-02, -6.0996e-02,  1.1403e-01,\n",
       "          -1.2475e-01, -1.0042e-01,  4.0308e-02, -1.1157e-01]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.1071, -0.0614,  0.0334,  0.1063, -0.0019,  0.0366,  0.0741, -0.0249,\n",
       "         -0.0551, -0.1007], requires_grad=True)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        batch_losses.append(loss)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    correct = 0 \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            batch_losses.append(loss)     \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train Loss: 1.1510\tTest Loss: 0.4518\tAccuracy: 87.07%\n",
      "[2] Train Loss: 0.3752\tTest Loss: 0.3264\tAccuracy: 90.37%\n",
      "[3] Train Loss: 0.3028\tTest Loss: 0.2731\tAccuracy: 92.27%\n",
      "[4] Train Loss: 0.2613\tTest Loss: 0.2373\tAccuracy: 93.31%\n",
      "[5] Train Loss: 0.2285\tTest Loss: 0.2057\tAccuracy: 94.17%\n",
      "[6] Train Loss: 0.2026\tTest Loss: 0.1852\tAccuracy: 94.66%\n",
      "[7] Train Loss: 0.1820\tTest Loss: 0.1701\tAccuracy: 94.98%\n",
      "[8] Train Loss: 0.1632\tTest Loss: 0.1552\tAccuracy: 95.52%\n",
      "[9] Train Loss: 0.1484\tTest Loss: 0.1415\tAccuracy: 95.83%\n",
      "[10] Train Loss: 0.1352\tTest Loss: 0.1377\tAccuracy: 95.90%\n",
      "[11] Train Loss: 0.1243\tTest Loss: 0.1252\tAccuracy: 96.33%\n",
      "[12] Train Loss: 0.1150\tTest Loss: 0.1159\tAccuracy: 96.61%\n",
      "[13] Train Loss: 0.1063\tTest Loss: 0.1135\tAccuracy: 96.59%\n",
      "[14] Train Loss: 0.0984\tTest Loss: 0.1070\tAccuracy: 96.91%\n",
      "[15] Train Loss: 0.0925\tTest Loss: 0.1024\tAccuracy: 96.93%\n",
      "[16] Train Loss: 0.0860\tTest Loss: 0.0996\tAccuracy: 97.02%\n",
      "[17] Train Loss: 0.0808\tTest Loss: 0.0953\tAccuracy: 97.14%\n",
      "[18] Train Loss: 0.0766\tTest Loss: 0.0929\tAccuracy: 97.27%\n",
      "[19] Train Loss: 0.0710\tTest Loss: 0.0917\tAccuracy: 97.24%\n",
      "[20] Train Loss: 0.0675\tTest Loss: 0.0886\tAccuracy: 97.34%\n",
      "[21] Train Loss: 0.0638\tTest Loss: 0.0855\tAccuracy: 97.40%\n",
      "[22] Train Loss: 0.0602\tTest Loss: 0.0826\tAccuracy: 97.46%\n",
      "[23] Train Loss: 0.0570\tTest Loss: 0.0820\tAccuracy: 97.53%\n",
      "[24] Train Loss: 0.0542\tTest Loss: 0.0781\tAccuracy: 97.55%\n",
      "[25] Train Loss: 0.0513\tTest Loss: 0.0780\tAccuracy: 97.57%\n",
      "[26] Train Loss: 0.0488\tTest Loss: 0.0777\tAccuracy: 97.57%\n",
      "[27] Train Loss: 0.0460\tTest Loss: 0.0768\tAccuracy: 97.65%\n",
      "[28] Train Loss: 0.0438\tTest Loss: 0.0803\tAccuracy: 97.53%\n",
      "[29] Train Loss: 0.0417\tTest Loss: 0.0736\tAccuracy: 97.67%\n",
      "[30] Train Loss: 0.0395\tTest Loss: 0.0721\tAccuracy: 97.81%\n",
      "[31] Train Loss: 0.0376\tTest Loss: 0.0748\tAccuracy: 97.68%\n",
      "[32] Train Loss: 0.0361\tTest Loss: 0.0701\tAccuracy: 97.81%\n",
      "[33] Train Loss: 0.0341\tTest Loss: 0.0692\tAccuracy: 97.89%\n",
      "[34] Train Loss: 0.0325\tTest Loss: 0.0722\tAccuracy: 97.77%\n",
      "[35] Train Loss: 0.0308\tTest Loss: 0.0707\tAccuracy: 97.87%\n",
      "[36] Train Loss: 0.0295\tTest Loss: 0.0709\tAccuracy: 97.83%\n",
      "[37] Train Loss: 0.0281\tTest Loss: 0.0712\tAccuracy: 97.76%\n",
      "[38] Train Loss: 0.0268\tTest Loss: 0.0691\tAccuracy: 97.78%\n",
      "[39] Train Loss: 0.0258\tTest Loss: 0.0693\tAccuracy: 97.76%\n",
      "[40] Train Loss: 0.0242\tTest Loss: 0.0705\tAccuracy: 97.88%\n",
      "[41] Train Loss: 0.0232\tTest Loss: 0.0701\tAccuracy: 97.88%\n",
      "[42] Train Loss: 0.0224\tTest Loss: 0.0703\tAccuracy: 97.81%\n",
      "[43] Train Loss: 0.0210\tTest Loss: 0.0736\tAccuracy: 97.78%\n",
      "[44] Train Loss: 0.0202\tTest Loss: 0.0713\tAccuracy: 97.89%\n",
      "[45] Train Loss: 0.0191\tTest Loss: 0.0687\tAccuracy: 97.89%\n",
      "[46] Train Loss: 0.0182\tTest Loss: 0.0712\tAccuracy: 97.77%\n",
      "[47] Train Loss: 0.0175\tTest Loss: 0.0695\tAccuracy: 97.85%\n",
      "[48] Train Loss: 0.0168\tTest Loss: 0.0720\tAccuracy: 97.79%\n",
      "[49] Train Loss: 0.0161\tTest Loss: 0.0705\tAccuracy: 97.84%\n",
      "[50] Train Loss: 0.0152\tTest Loss: 0.0702\tAccuracy: 97.88%\n",
      "[51] Train Loss: 0.0147\tTest Loss: 0.0697\tAccuracy: 97.84%\n",
      "[52] Train Loss: 0.0139\tTest Loss: 0.0710\tAccuracy: 97.81%\n",
      "[53] Train Loss: 0.0132\tTest Loss: 0.0739\tAccuracy: 97.84%\n",
      "[54] Train Loss: 0.0125\tTest Loss: 0.0707\tAccuracy: 97.88%\n",
      "[55] Train Loss: 0.0122\tTest Loss: 0.0710\tAccuracy: 97.89%\n",
      "[56] Train Loss: 0.0116\tTest Loss: 0.0714\tAccuracy: 97.89%\n",
      "[57] Train Loss: 0.0110\tTest Loss: 0.0708\tAccuracy: 97.79%\n",
      "[58] Train Loss: 0.0107\tTest Loss: 0.0720\tAccuracy: 97.90%\n",
      "[59] Train Loss: 0.0102\tTest Loss: 0.0711\tAccuracy: 97.86%\n",
      "[60] Train Loss: 0.0098\tTest Loss: 0.0726\tAccuracy: 97.87%\n",
      "[61] Train Loss: 0.0094\tTest Loss: 0.0727\tAccuracy: 97.85%\n",
      "[62] Train Loss: 0.0089\tTest Loss: 0.0726\tAccuracy: 97.82%\n",
      "[63] Train Loss: 0.0087\tTest Loss: 0.0718\tAccuracy: 97.90%\n",
      "[64] Train Loss: 0.0083\tTest Loss: 0.0730\tAccuracy: 97.82%\n",
      "[65] Train Loss: 0.0080\tTest Loss: 0.0728\tAccuracy: 97.89%\n",
      "[66] Train Loss: 0.0076\tTest Loss: 0.0716\tAccuracy: 97.89%\n",
      "[67] Train Loss: 0.0074\tTest Loss: 0.0751\tAccuracy: 97.93%\n",
      "[68] Train Loss: 0.0070\tTest Loss: 0.0722\tAccuracy: 97.96%\n",
      "[69] Train Loss: 0.0068\tTest Loss: 0.0756\tAccuracy: 97.89%\n",
      "[70] Train Loss: 0.0065\tTest Loss: 0.0745\tAccuracy: 97.95%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 70\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_loader, optimizer)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    \n",
    "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
    "          epoch, train_loss, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pboMIBQq7onH"
   },
   "source": [
    "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux7mPf6E78d4"
   },
   "outputs": [],
   "source": [
    "과적합의 문제를 해결하고자 Batch Size의 크기를 줄여보았습니다. #512->300 \n",
    "마찬가지로 과적합 문제를 해결하고자 epoch를 줄여보았습니다. #100->70\n",
    "오버슈팅의 문제로 인해서 생기는 train loss 이슈를 해결하기 위해 learning rate 를 줄여보았습니다. #0.5->0.1\n",
    "\n",
    "결과적으로 전체적인 accuracy는 약간 높아진 것으로 보입니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[0818] Deep Learning Basic_과제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
