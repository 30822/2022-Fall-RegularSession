{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[8기 장준혁]_Deep_Learning_Basic_과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "  x = np.maximum(0, x)\n",
        "  return x"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_relu(x):\n",
        "  x[x > 0] = 1\n",
        "  x[x <= 0] = 0\n",
        "  return x"
      ],
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS2 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "wDhjqpOUh1Om"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 텐서로 변경\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "pGCDAx_Ph-CA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "id": "oEDGV2uOh-2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "bCV6yDJgh2If"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "0R38_G99iEJq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "TZd3Jj3XiddT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    # 배치 당 loss 값을 담을 리스트 생성\n",
        "    batch_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # 옵티마이저의 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # y pred 값 산출\n",
        "        output = model(data)\n",
        "        # loss 계산\n",
        "        # 정답 데이터와의 cross entropy loss 계산\n",
        "        # 이 loss를 배치 당 loss로 보관\n",
        "        loss = criterion(output, target)\n",
        "        batch_losses.append(loss)\n",
        "\n",
        "        # 기울기 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트!\n",
        "        optimizer.step()\n",
        "        \n",
        "    # 배치당 평균 loss 계산\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    \n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "92z6I1_Wi7nR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    batch_losses = []\n",
        "    correct = 0 \n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            # 예측값 생성\n",
        "            output = model(data)\n",
        "\n",
        "            # loss 계산 (이전과 동일)\n",
        "            loss = criterion(output, target)\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "           # Accuracy 계산\n",
        "           # y pred와 y가 일치하면 correct에 1을 더해주기\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # 배치 당 평균 loss 계산 \n",
        "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    #정확도 계산\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "UYDJY9WYjAML"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJO249A3jhk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc23ef47-5aff-4d79-c014-999e510bddcf"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.8268\tTest Loss: 0.6319\tAccuracy: 79.96%\n",
            "[2] Train Loss: 0.2537\tTest Loss: 0.1977\tAccuracy: 93.90%\n",
            "[3] Train Loss: 0.1684\tTest Loss: 0.1492\tAccuracy: 95.43%\n",
            "[4] Train Loss: 0.1289\tTest Loss: 0.1700\tAccuracy: 94.88%\n",
            "[5] Train Loss: 0.1078\tTest Loss: 0.1242\tAccuracy: 95.97%\n",
            "[6] Train Loss: 0.0904\tTest Loss: 0.1686\tAccuracy: 94.74%\n",
            "[7] Train Loss: 0.0798\tTest Loss: 0.1687\tAccuracy: 94.47%\n",
            "[8] Train Loss: 0.0883\tTest Loss: 0.1761\tAccuracy: 94.90%\n",
            "[9] Train Loss: 0.0601\tTest Loss: 0.1106\tAccuracy: 96.67%\n",
            "[10] Train Loss: 0.0533\tTest Loss: 0.1074\tAccuracy: 96.51%\n",
            "[11] Train Loss: 0.0462\tTest Loss: 0.1437\tAccuracy: 95.52%\n",
            "[12] Train Loss: 0.1781\tTest Loss: 0.1184\tAccuracy: 96.46%\n",
            "[13] Train Loss: 0.0558\tTest Loss: 0.0949\tAccuracy: 96.94%\n",
            "[14] Train Loss: 0.0447\tTest Loss: 0.0857\tAccuracy: 97.34%\n",
            "[15] Train Loss: 0.0387\tTest Loss: 0.1292\tAccuracy: 96.17%\n",
            "[16] Train Loss: 0.0338\tTest Loss: 0.0767\tAccuracy: 97.71%\n",
            "[17] Train Loss: 0.0285\tTest Loss: 0.0944\tAccuracy: 97.06%\n",
            "[18] Train Loss: 0.0263\tTest Loss: 0.0883\tAccuracy: 97.36%\n",
            "[19] Train Loss: 0.0231\tTest Loss: 0.0834\tAccuracy: 97.67%\n",
            "[20] Train Loss: 0.0206\tTest Loss: 0.0794\tAccuracy: 97.85%\n",
            "[21] Train Loss: 0.0187\tTest Loss: 0.0748\tAccuracy: 97.90%\n",
            "[22] Train Loss: 0.0162\tTest Loss: 0.0861\tAccuracy: 97.55%\n",
            "[23] Train Loss: 0.0149\tTest Loss: 0.0906\tAccuracy: 97.55%\n",
            "[24] Train Loss: 0.0129\tTest Loss: 0.0877\tAccuracy: 97.64%\n",
            "[25] Train Loss: 0.0113\tTest Loss: 0.0775\tAccuracy: 97.82%\n",
            "[26] Train Loss: 0.0102\tTest Loss: 0.0923\tAccuracy: 97.40%\n",
            "[27] Train Loss: 0.0092\tTest Loss: 0.0752\tAccuracy: 98.00%\n",
            "[28] Train Loss: 0.0079\tTest Loss: 0.0738\tAccuracy: 98.01%\n",
            "[29] Train Loss: 0.0071\tTest Loss: 0.0847\tAccuracy: 97.90%\n",
            "[30] Train Loss: 0.0064\tTest Loss: 0.0772\tAccuracy: 98.03%\n",
            "[31] Train Loss: 0.0058\tTest Loss: 0.0772\tAccuracy: 97.95%\n",
            "[32] Train Loss: 0.0051\tTest Loss: 0.0781\tAccuracy: 98.11%\n",
            "[33] Train Loss: 0.0046\tTest Loss: 0.0811\tAccuracy: 97.91%\n",
            "[34] Train Loss: 0.0043\tTest Loss: 0.0810\tAccuracy: 97.94%\n",
            "[35] Train Loss: 0.0040\tTest Loss: 0.0823\tAccuracy: 97.98%\n",
            "[36] Train Loss: 0.0036\tTest Loss: 0.0831\tAccuracy: 98.00%\n",
            "[37] Train Loss: 0.0033\tTest Loss: 0.0808\tAccuracy: 97.96%\n",
            "[38] Train Loss: 0.0031\tTest Loss: 0.0822\tAccuracy: 97.97%\n",
            "[39] Train Loss: 0.0028\tTest Loss: 0.0813\tAccuracy: 98.03%\n",
            "[40] Train Loss: 0.0026\tTest Loss: 0.0830\tAccuracy: 97.97%\n",
            "[41] Train Loss: 0.0025\tTest Loss: 0.0896\tAccuracy: 97.90%\n",
            "[42] Train Loss: 0.0024\tTest Loss: 0.0840\tAccuracy: 98.05%\n",
            "[43] Train Loss: 0.0021\tTest Loss: 0.0870\tAccuracy: 97.96%\n",
            "[44] Train Loss: 0.0020\tTest Loss: 0.0859\tAccuracy: 97.91%\n",
            "[45] Train Loss: 0.0019\tTest Loss: 0.0857\tAccuracy: 97.96%\n",
            "[46] Train Loss: 0.0018\tTest Loss: 0.0848\tAccuracy: 98.03%\n",
            "[47] Train Loss: 0.0017\tTest Loss: 0.0856\tAccuracy: 97.98%\n",
            "[48] Train Loss: 0.0016\tTest Loss: 0.0849\tAccuracy: 98.02%\n",
            "[49] Train Loss: 0.0015\tTest Loss: 0.0854\tAccuracy: 97.99%\n",
            "[50] Train Loss: 0.0014\tTest Loss: 0.0872\tAccuracy: 97.97%\n",
            "[51] Train Loss: 0.0014\tTest Loss: 0.0902\tAccuracy: 98.01%\n",
            "[52] Train Loss: 0.0013\tTest Loss: 0.0874\tAccuracy: 98.01%\n",
            "[53] Train Loss: 0.0012\tTest Loss: 0.0876\tAccuracy: 97.97%\n",
            "[54] Train Loss: 0.0012\tTest Loss: 0.0871\tAccuracy: 97.99%\n",
            "[55] Train Loss: 0.0011\tTest Loss: 0.0891\tAccuracy: 98.01%\n",
            "[56] Train Loss: 0.0011\tTest Loss: 0.0891\tAccuracy: 98.00%\n",
            "[57] Train Loss: 0.0011\tTest Loss: 0.0905\tAccuracy: 97.93%\n",
            "[58] Train Loss: 0.0010\tTest Loss: 0.0896\tAccuracy: 98.01%\n",
            "[59] Train Loss: 0.0010\tTest Loss: 0.0932\tAccuracy: 97.98%\n",
            "[60] Train Loss: 0.0009\tTest Loss: 0.0903\tAccuracy: 98.00%\n",
            "[61] Train Loss: 0.0009\tTest Loss: 0.0904\tAccuracy: 98.02%\n",
            "[62] Train Loss: 0.0009\tTest Loss: 0.0968\tAccuracy: 98.02%\n",
            "[63] Train Loss: 0.0009\tTest Loss: 0.0919\tAccuracy: 98.04%\n",
            "[64] Train Loss: 0.0008\tTest Loss: 0.0908\tAccuracy: 97.97%\n",
            "[65] Train Loss: 0.0008\tTest Loss: 0.0911\tAccuracy: 98.02%\n",
            "[66] Train Loss: 0.0008\tTest Loss: 0.0925\tAccuracy: 98.02%\n",
            "[67] Train Loss: 0.0007\tTest Loss: 0.0942\tAccuracy: 97.97%\n",
            "[68] Train Loss: 0.0007\tTest Loss: 0.0935\tAccuracy: 98.04%\n",
            "[69] Train Loss: 0.0007\tTest Loss: 0.0939\tAccuracy: 98.03%\n",
            "[70] Train Loss: 0.0007\tTest Loss: 0.0943\tAccuracy: 98.02%\n",
            "[71] Train Loss: 0.0007\tTest Loss: 0.0946\tAccuracy: 98.00%\n",
            "[72] Train Loss: 0.0006\tTest Loss: 0.0948\tAccuracy: 98.04%\n",
            "[73] Train Loss: 0.0006\tTest Loss: 0.0937\tAccuracy: 98.01%\n",
            "[74] Train Loss: 0.0006\tTest Loss: 0.0971\tAccuracy: 98.03%\n",
            "[75] Train Loss: 0.0006\tTest Loss: 0.0951\tAccuracy: 98.06%\n",
            "[76] Train Loss: 0.0006\tTest Loss: 0.0949\tAccuracy: 98.02%\n",
            "[77] Train Loss: 0.0006\tTest Loss: 0.0974\tAccuracy: 98.01%\n",
            "[78] Train Loss: 0.0006\tTest Loss: 0.0979\tAccuracy: 98.03%\n",
            "[79] Train Loss: 0.0005\tTest Loss: 0.0980\tAccuracy: 98.05%\n",
            "[80] Train Loss: 0.0005\tTest Loss: 0.0968\tAccuracy: 98.01%\n",
            "[81] Train Loss: 0.0005\tTest Loss: 0.1004\tAccuracy: 98.03%\n",
            "[82] Train Loss: 0.0005\tTest Loss: 0.1003\tAccuracy: 98.02%\n",
            "[83] Train Loss: 0.0005\tTest Loss: 0.0962\tAccuracy: 98.00%\n",
            "[84] Train Loss: 0.0005\tTest Loss: 0.0973\tAccuracy: 98.06%\n",
            "[85] Train Loss: 0.0005\tTest Loss: 0.0979\tAccuracy: 98.01%\n",
            "[86] Train Loss: 0.0005\tTest Loss: 0.0974\tAccuracy: 97.98%\n",
            "[87] Train Loss: 0.0005\tTest Loss: 0.0993\tAccuracy: 98.03%\n",
            "[88] Train Loss: 0.0005\tTest Loss: 0.0993\tAccuracy: 98.04%\n",
            "[89] Train Loss: 0.0004\tTest Loss: 0.0977\tAccuracy: 98.04%\n",
            "[90] Train Loss: 0.0004\tTest Loss: 0.0998\tAccuracy: 98.02%\n",
            "[91] Train Loss: 0.0004\tTest Loss: 0.0992\tAccuracy: 97.98%\n",
            "[92] Train Loss: 0.0004\tTest Loss: 0.0999\tAccuracy: 98.02%\n",
            "[93] Train Loss: 0.0004\tTest Loss: 0.1000\tAccuracy: 98.01%\n",
            "[94] Train Loss: 0.0004\tTest Loss: 0.0992\tAccuracy: 98.01%\n",
            "[95] Train Loss: 0.0004\tTest Loss: 0.0989\tAccuracy: 98.04%\n",
            "[96] Train Loss: 0.0004\tTest Loss: 0.1002\tAccuracy: 98.02%\n",
            "[97] Train Loss: 0.0004\tTest Loss: 0.1014\tAccuracy: 97.99%\n",
            "[98] Train Loss: 0.0004\tTest Loss: 0.1014\tAccuracy: 98.06%\n",
            "[99] Train Loss: 0.0004\tTest Loss: 0.1043\tAccuracy: 98.05%\n",
            "[100] Train Loss: 0.0004\tTest Loss: 0.0996\tAccuracy: 98.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6b82DZG6W3j"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.relu = nn.LogSigmoid()\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.relu = nn.LogSigmoid()\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "TNx7bDp7kpB2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEFJBmCVko6s",
        "outputId": "b595d6d6-5a7d-47b5-a22f-1fcad5a5bd6c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 1.5529\tTest Loss: 0.8070\tAccuracy: 70.68%\n",
            "[2] Train Loss: 0.4489\tTest Loss: 0.4115\tAccuracy: 87.21%\n",
            "[3] Train Loss: 0.3084\tTest Loss: 0.2798\tAccuracy: 91.03%\n",
            "[4] Train Loss: 0.2628\tTest Loss: 0.2439\tAccuracy: 92.17%\n",
            "[5] Train Loss: 0.2125\tTest Loss: 0.2271\tAccuracy: 92.87%\n",
            "[6] Train Loss: 0.1878\tTest Loss: 0.2157\tAccuracy: 93.10%\n",
            "[7] Train Loss: 0.1709\tTest Loss: 0.2481\tAccuracy: 91.71%\n",
            "[8] Train Loss: 0.1540\tTest Loss: 0.1584\tAccuracy: 95.13%\n",
            "[9] Train Loss: 0.1416\tTest Loss: 0.1934\tAccuracy: 93.88%\n",
            "[10] Train Loss: 0.1287\tTest Loss: 0.1421\tAccuracy: 95.33%\n",
            "[11] Train Loss: 0.1196\tTest Loss: 0.1330\tAccuracy: 95.83%\n",
            "[12] Train Loss: 0.1106\tTest Loss: 0.1676\tAccuracy: 95.00%\n",
            "[13] Train Loss: 0.1030\tTest Loss: 0.1085\tAccuracy: 96.55%\n",
            "[14] Train Loss: 0.0965\tTest Loss: 0.1184\tAccuracy: 96.46%\n",
            "[15] Train Loss: 0.0910\tTest Loss: 0.1120\tAccuracy: 96.36%\n",
            "[16] Train Loss: 0.0853\tTest Loss: 0.1511\tAccuracy: 95.16%\n",
            "[17] Train Loss: 0.0810\tTest Loss: 0.1086\tAccuracy: 96.58%\n",
            "[18] Train Loss: 0.0762\tTest Loss: 0.1058\tAccuracy: 96.63%\n",
            "[19] Train Loss: 0.0740\tTest Loss: 0.2351\tAccuracy: 92.83%\n",
            "[20] Train Loss: 0.4264\tTest Loss: 0.1564\tAccuracy: 95.34%\n",
            "[21] Train Loss: 0.1081\tTest Loss: 0.1246\tAccuracy: 96.03%\n",
            "[22] Train Loss: 0.0905\tTest Loss: 0.1174\tAccuracy: 96.57%\n",
            "[23] Train Loss: 0.0802\tTest Loss: 0.1674\tAccuracy: 94.80%\n",
            "[24] Train Loss: 0.0739\tTest Loss: 0.1124\tAccuracy: 96.43%\n",
            "[25] Train Loss: 0.0685\tTest Loss: 0.1055\tAccuracy: 96.82%\n",
            "[26] Train Loss: 0.0638\tTest Loss: 0.1141\tAccuracy: 96.44%\n",
            "[27] Train Loss: 0.0599\tTest Loss: 0.1062\tAccuracy: 96.77%\n",
            "[28] Train Loss: 0.0562\tTest Loss: 0.1010\tAccuracy: 96.95%\n",
            "[29] Train Loss: 0.0531\tTest Loss: 0.0949\tAccuracy: 97.04%\n",
            "[30] Train Loss: 0.0506\tTest Loss: 0.0924\tAccuracy: 97.21%\n",
            "[31] Train Loss: 0.0497\tTest Loss: 0.1071\tAccuracy: 96.85%\n",
            "[32] Train Loss: 0.0453\tTest Loss: 0.1126\tAccuracy: 96.68%\n",
            "[33] Train Loss: 0.0434\tTest Loss: 0.1016\tAccuracy: 96.94%\n",
            "[34] Train Loss: 0.0420\tTest Loss: 0.1577\tAccuracy: 95.42%\n",
            "[35] Train Loss: 0.0409\tTest Loss: 0.0880\tAccuracy: 97.52%\n",
            "[36] Train Loss: 0.0370\tTest Loss: 0.0893\tAccuracy: 97.43%\n",
            "[37] Train Loss: 0.0360\tTest Loss: 0.1052\tAccuracy: 96.92%\n",
            "[38] Train Loss: 0.0347\tTest Loss: 0.0980\tAccuracy: 97.18%\n",
            "[39] Train Loss: 0.0325\tTest Loss: 0.1177\tAccuracy: 96.51%\n",
            "[40] Train Loss: 0.0309\tTest Loss: 0.1007\tAccuracy: 97.24%\n",
            "[41] Train Loss: 0.0294\tTest Loss: 0.2059\tAccuracy: 94.68%\n",
            "[42] Train Loss: 0.0306\tTest Loss: 0.1839\tAccuracy: 95.15%\n",
            "[43] Train Loss: 0.0277\tTest Loss: 0.0920\tAccuracy: 97.60%\n",
            "[44] Train Loss: 0.0250\tTest Loss: 0.1166\tAccuracy: 96.94%\n",
            "[45] Train Loss: 0.0247\tTest Loss: 0.1442\tAccuracy: 96.26%\n",
            "[46] Train Loss: 0.0246\tTest Loss: 0.1000\tAccuracy: 97.43%\n",
            "[47] Train Loss: 0.0220\tTest Loss: 0.1155\tAccuracy: 97.10%\n",
            "[48] Train Loss: 0.0212\tTest Loss: 0.1211\tAccuracy: 96.71%\n",
            "[49] Train Loss: 0.0206\tTest Loss: 0.1361\tAccuracy: 96.48%\n",
            "[50] Train Loss: 0.0190\tTest Loss: 0.0957\tAccuracy: 97.48%\n",
            "[51] Train Loss: 0.0180\tTest Loss: 0.0970\tAccuracy: 97.43%\n",
            "[52] Train Loss: 0.0170\tTest Loss: 0.1044\tAccuracy: 97.36%\n",
            "[53] Train Loss: 0.0163\tTest Loss: 0.3038\tAccuracy: 94.80%\n",
            "[54] Train Loss: 1.9491\tTest Loss: 0.1887\tAccuracy: 94.69%\n",
            "[55] Train Loss: 0.1388\tTest Loss: 0.1535\tAccuracy: 95.31%\n",
            "[56] Train Loss: 0.1056\tTest Loss: 0.1375\tAccuracy: 96.04%\n",
            "[57] Train Loss: 0.0913\tTest Loss: 0.1294\tAccuracy: 96.16%\n",
            "[58] Train Loss: 0.0807\tTest Loss: 0.1312\tAccuracy: 96.06%\n",
            "[59] Train Loss: 0.0717\tTest Loss: 0.1134\tAccuracy: 96.63%\n",
            "[60] Train Loss: 0.0650\tTest Loss: 0.1004\tAccuracy: 96.97%\n",
            "[61] Train Loss: 0.0596\tTest Loss: 0.1031\tAccuracy: 97.00%\n",
            "[62] Train Loss: 0.0558\tTest Loss: 0.1118\tAccuracy: 96.73%\n",
            "[63] Train Loss: 0.0532\tTest Loss: 0.1102\tAccuracy: 96.94%\n",
            "[64] Train Loss: 0.0485\tTest Loss: 0.1350\tAccuracy: 96.15%\n",
            "[65] Train Loss: 0.1116\tTest Loss: 0.1308\tAccuracy: 96.14%\n",
            "[66] Train Loss: 0.0463\tTest Loss: 0.0937\tAccuracy: 97.49%\n",
            "[67] Train Loss: 0.0423\tTest Loss: 0.0886\tAccuracy: 97.50%\n",
            "[68] Train Loss: 0.0393\tTest Loss: 0.1096\tAccuracy: 96.95%\n",
            "[69] Train Loss: 0.0363\tTest Loss: 0.1020\tAccuracy: 97.35%\n",
            "[70] Train Loss: 0.0337\tTest Loss: 0.1250\tAccuracy: 96.53%\n",
            "[71] Train Loss: 0.0329\tTest Loss: 0.0937\tAccuracy: 97.47%\n",
            "[72] Train Loss: 0.0301\tTest Loss: 0.1189\tAccuracy: 96.76%\n",
            "[73] Train Loss: 0.0292\tTest Loss: 0.0952\tAccuracy: 97.53%\n",
            "[74] Train Loss: 0.0273\tTest Loss: 0.1057\tAccuracy: 97.24%\n",
            "[75] Train Loss: 0.0254\tTest Loss: 0.0980\tAccuracy: 97.53%\n",
            "[76] Train Loss: 0.0242\tTest Loss: 0.0998\tAccuracy: 97.54%\n",
            "[77] Train Loss: 0.0233\tTest Loss: 0.1194\tAccuracy: 97.13%\n",
            "[78] Train Loss: 0.0214\tTest Loss: 0.1458\tAccuracy: 96.68%\n",
            "[79] Train Loss: 0.0206\tTest Loss: 0.1027\tAccuracy: 97.72%\n",
            "[80] Train Loss: 0.0184\tTest Loss: 0.1139\tAccuracy: 97.35%\n",
            "[81] Train Loss: 0.0179\tTest Loss: 0.1026\tAccuracy: 97.65%\n",
            "[82] Train Loss: 0.0170\tTest Loss: 0.1223\tAccuracy: 97.17%\n",
            "[83] Train Loss: 0.0168\tTest Loss: 0.1109\tAccuracy: 97.43%\n",
            "[84] Train Loss: 0.0157\tTest Loss: 0.1113\tAccuracy: 97.51%\n",
            "[85] Train Loss: 0.0144\tTest Loss: 0.1223\tAccuracy: 97.30%\n",
            "[86] Train Loss: 0.0133\tTest Loss: 0.1169\tAccuracy: 97.54%\n",
            "[87] Train Loss: 0.0125\tTest Loss: 0.1199\tAccuracy: 97.58%\n",
            "[88] Train Loss: 0.0117\tTest Loss: 0.1192\tAccuracy: 97.49%\n",
            "[89] Train Loss: 0.0112\tTest Loss: 0.1176\tAccuracy: 97.50%\n",
            "[90] Train Loss: 0.0104\tTest Loss: 0.1166\tAccuracy: 97.52%\n",
            "[91] Train Loss: 0.0106\tTest Loss: 0.1227\tAccuracy: 97.52%\n",
            "[92] Train Loss: 0.0092\tTest Loss: 0.1150\tAccuracy: 97.68%\n",
            "[93] Train Loss: 0.0085\tTest Loss: 0.1210\tAccuracy: 97.66%\n",
            "[94] Train Loss: 0.0080\tTest Loss: 0.1299\tAccuracy: 97.61%\n",
            "[95] Train Loss: 0.0077\tTest Loss: 0.1240\tAccuracy: 97.63%\n",
            "[96] Train Loss: 0.0071\tTest Loss: 0.1413\tAccuracy: 97.32%\n",
            "[97] Train Loss: 0.0069\tTest Loss: 0.1281\tAccuracy: 97.56%\n",
            "[98] Train Loss: 0.0065\tTest Loss: 0.1316\tAccuracy: 97.61%\n",
            "[99] Train Loss: 0.0062\tTest Loss: 0.1298\tAccuracy: 97.58%\n",
            "[100] Train Loss: 0.0055\tTest Loss: 0.1380\tAccuracy: 97.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ReLU 함수를 LogSigmoid 함수로 다 변경해서 학습시켰다\n",
        "- ReLU 함수 결과: Train Loss = 0.0004,Test Loss = 0.0996,Accuracy =  98.06%\n",
        "- LogSigmoid 함수 결과: Train Loss=0.0055,\tTest Loss=0.1380,\tAccuracy=97.52%\n",
        "- 결과가 향상될줄 알았는데 0.54%정도 안좋아짐"
      ],
      "metadata": {
        "id": "y1-y_UaDxwrL"
      }
    }
  ]
}