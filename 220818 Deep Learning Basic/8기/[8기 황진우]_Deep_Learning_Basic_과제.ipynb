{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[8기 황진우]_Deep_Learning_Basic_과제.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import torch\n","\n","from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"btzAVR2M2PBj","executionInfo":{"status":"ok","timestamp":1661232219807,"user_tz":-540,"elapsed":289,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz","executionInfo":{"status":"ok","timestamp":1661232220334,"user_tz":-540,"elapsed":4,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"source":["def relu(x):\n","  out = np.maximum(0, x)\n","  return out"],"execution_count":39,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","  out = np.where(x>0, 1, 0)\n","  return out"],"metadata":{"id":"Esm4jmTVijro","executionInfo":{"status":"ok","timestamp":1661232220335,"user_tz":-540,"elapsed":4,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs","executionInfo":{"status":"ok","timestamp":1661232642466,"user_tz":-540,"elapsed":327,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"source":["def backward_pass(x, y_true, params):\n","  grads = {}\n","\n","  dS3 = params[\"A3\"] - y_true\n","  \n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","  \n","  return  grads"],"execution_count":59,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk","executionInfo":{"status":"ok","timestamp":1661232220335,"user_tz":-540,"elapsed":3,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"source":["# Assignment 3 구현은 여기서 ()"],"execution_count":42,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"6AD7uV5rAAEL","executionInfo":{"status":"ok","timestamp":1661232220335,"user_tz":-540,"elapsed":3,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["from IPython import get_ipython\n","get_ipython().magic('reset -sf')\n","\n","import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"],"metadata":{"id":"1Sk08Ba0FusD","executionInfo":{"status":"ok","timestamp":1661232280579,"user_tz":-540,"elapsed":59920,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# data preprocessing\n","\n","num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1    "],"metadata":{"id":"zo9uJ9W1Fuun","executionInfo":{"status":"ok","timestamp":1661232280580,"user_tz":-540,"elapsed":9,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden1_size = 128\n","hidden2_size = 64\n","num_class = 10"],"metadata":{"id":"ZcyySIIIJZXc","executionInfo":{"status":"ok","timestamp":1661232280581,"user_tz":-540,"elapsed":8,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["params = {\"W1\": np.random.randn(hidden1_size, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden1_size, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden2_size, hidden1_size) * np.sqrt(1/ hidden1_size),\n","          \"b2\": np.zeros((hidden2_size, 1)) * np.sqrt(1/ hidden1_size),\n","          \"W3\": np.random.randn(num_class, hidden2_size) * np.sqrt(1/ hidden2_size),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden2_size)\n","          }"],"metadata":{"id":"P8hSWXxvFuxg","executionInfo":{"status":"ok","timestamp":1661232280581,"user_tz":-540,"elapsed":7,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["def compute_loss(y_true, y_pred):\n","  # loss calculation\n","\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"metadata":{"id":"ePoCelJ5KyyC","executionInfo":{"status":"ok","timestamp":1661232428613,"user_tz":-540,"elapsed":336,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def d_sigmoid(x):\n","  # derivative of sigmoid\n","  exp = np.exp(-x)\n","  return (exp)/((1+exp)**2)\n","\n","def softmax(x):\n","  exp = np.exp(x)\n","  return exp/np.sum(exp, axis=0)"],"metadata":{"id":"M_AVibOEK9UK","executionInfo":{"status":"ok","timestamp":1661232472681,"user_tz":-540,"elapsed":414,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = sigmoid(params[\"S1\"])\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = softmax(params[\"S2\"])\n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = sigmoid(params[\"S3\"])\n","\n","  return params"],"metadata":{"id":"IawOHp_bFu5d","executionInfo":{"status":"ok","timestamp":1661232494430,"user_tz":-540,"elapsed":288,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["def foward_pass_test(x, params):\n","\n","  params_test = {}\n","  \n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = sigmoid(params_test[\"S3\"])\n","\n","  return params_test"],"metadata":{"id":"Eq8zG1SUKpds","executionInfo":{"status":"ok","timestamp":1661232855121,"user_tz":-540,"elapsed":444,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"metadata":{"id":"hv_o-mLrKjYf","executionInfo":{"status":"ok","timestamp":1661232499273,"user_tz":-540,"elapsed":738,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["def backward_pass(x, y_true, params):\n","  grads = {}\n","\n","  dS3 = params[\"A3\"] - y_true\n","  \n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","  \n","  return  grads"],"metadata":{"id":"52vAT9dxKjuT","executionInfo":{"status":"ok","timestamp":1661232651309,"user_tz":-540,"elapsed":312,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.5\n","\n","for i in range(epochs):\n","\n","  if i == 0:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A3\"])\n","  train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A3\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ye1GIjBGEF0w","executionInfo":{"status":"ok","timestamp":1661233084376,"user_tz":-540,"elapsed":224120,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}},"outputId":"c0c413ef-3543-43c9-c657-d64cee79d8e2"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = 0.721277, training acuracy = 9.86%, test loss = 0.72135, training acuracy = 9.58%\n","Epoch 2: training loss = 0.727627, training acuracy = 9.86%, test loss = 0.727716, training acuracy = 9.58%\n","Epoch 3: training loss = 0.733397, training acuracy = 9.86%, test loss = 0.733498, training acuracy = 9.58%\n","Epoch 4: training loss = 0.738871, training acuracy = 9.86%, test loss = 0.738983, training acuracy = 9.58%\n","Epoch 5: training loss = 0.744188, training acuracy = 9.86%, test loss = 0.74431, training acuracy = 9.58%\n","Epoch 6: training loss = 0.749417, training acuracy = 9.86%, test loss = 0.749549, training acuracy = 9.58%\n","Epoch 7: training loss = 0.754594, training acuracy = 9.86%, test loss = 0.754735, training acuracy = 9.58%\n","Epoch 8: training loss = 0.759737, training acuracy = 9.86%, test loss = 0.759887, training acuracy = 9.58%\n","Epoch 9: training loss = 0.764858, training acuracy = 9.86%, test loss = 0.765017, training acuracy = 9.58%\n","Epoch 10: training loss = 0.769961, training acuracy = 9.86%, test loss = 0.770129, training acuracy = 9.58%\n","Epoch 11: training loss = 0.77505, training acuracy = 9.86%, test loss = 0.775227, training acuracy = 9.58%\n","Epoch 12: training loss = 0.780127, training acuracy = 9.86%, test loss = 0.780314, training acuracy = 9.58%\n","Epoch 13: training loss = 0.785193, training acuracy = 9.86%, test loss = 0.785389, training acuracy = 9.58%\n","Epoch 14: training loss = 0.790249, training acuracy = 9.86%, test loss = 0.790454, training acuracy = 9.58%\n","Epoch 15: training loss = 0.795296, training acuracy = 9.86%, test loss = 0.79551, training acuracy = 9.58%\n","Epoch 16: training loss = 0.800334, training acuracy = 9.86%, test loss = 0.800557, training acuracy = 9.58%\n","Epoch 17: training loss = 0.805363, training acuracy = 9.86%, test loss = 0.805595, training acuracy = 9.58%\n","Epoch 18: training loss = 0.810383, training acuracy = 9.86%, test loss = 0.810625, training acuracy = 9.58%\n","Epoch 19: training loss = 0.815396, training acuracy = 9.86%, test loss = 0.815646, training acuracy = 9.58%\n","Epoch 20: training loss = 0.820399, training acuracy = 9.86%, test loss = 0.820658, training acuracy = 9.58%\n","Epoch 21: training loss = 0.825394, training acuracy = 9.86%, test loss = 0.825662, training acuracy = 9.58%\n","Epoch 22: training loss = 0.83038, training acuracy = 9.86%, test loss = 0.830657, training acuracy = 9.58%\n","Epoch 23: training loss = 0.835357, training acuracy = 9.86%, test loss = 0.835643, training acuracy = 9.58%\n","Epoch 24: training loss = 0.840324, training acuracy = 9.86%, test loss = 0.840619, training acuracy = 9.58%\n","Epoch 25: training loss = 0.845282, training acuracy = 9.86%, test loss = 0.845586, training acuracy = 9.58%\n","Epoch 26: training loss = 0.850231, training acuracy = 9.86%, test loss = 0.850543, training acuracy = 9.58%\n","Epoch 27: training loss = 0.855169, training acuracy = 9.86%, test loss = 0.855491, training acuracy = 9.58%\n","Epoch 28: training loss = 0.860097, training acuracy = 9.86%, test loss = 0.860428, training acuracy = 9.58%\n","Epoch 29: training loss = 0.865015, training acuracy = 9.86%, test loss = 0.865354, training acuracy = 9.58%\n","Epoch 30: training loss = 0.869922, training acuracy = 9.86%, test loss = 0.87027, training acuracy = 9.58%\n","Epoch 31: training loss = 0.874818, training acuracy = 9.86%, test loss = 0.875175, training acuracy = 9.58%\n","Epoch 32: training loss = 0.879704, training acuracy = 9.86%, test loss = 0.880069, training acuracy = 9.58%\n","Epoch 33: training loss = 0.884578, training acuracy = 9.86%, test loss = 0.884952, training acuracy = 9.58%\n","Epoch 34: training loss = 0.88944, training acuracy = 9.86%, test loss = 0.889823, training acuracy = 9.58%\n","Epoch 35: training loss = 0.894291, training acuracy = 9.86%, test loss = 0.894682, training acuracy = 9.58%\n","Epoch 36: training loss = 0.89913, training acuracy = 9.86%, test loss = 0.899529, training acuracy = 9.58%\n","Epoch 37: training loss = 0.903956, training acuracy = 9.86%, test loss = 0.904365, training acuracy = 9.58%\n","Epoch 38: training loss = 0.908771, training acuracy = 9.86%, test loss = 0.909188, training acuracy = 9.58%\n","Epoch 39: training loss = 0.913573, training acuracy = 9.86%, test loss = 0.913999, training acuracy = 9.58%\n","Epoch 40: training loss = 0.918363, training acuracy = 9.86%, test loss = 0.918797, training acuracy = 9.58%\n","Epoch 41: training loss = 0.923139, training acuracy = 9.86%, test loss = 0.923582, training acuracy = 9.58%\n","Epoch 42: training loss = 0.927903, training acuracy = 9.86%, test loss = 0.928355, training acuracy = 9.58%\n","Epoch 43: training loss = 0.932654, training acuracy = 9.86%, test loss = 0.933114, training acuracy = 9.58%\n","Epoch 44: training loss = 0.937392, training acuracy = 9.86%, test loss = 0.93786, training acuracy = 9.58%\n","Epoch 45: training loss = 0.942116, training acuracy = 9.86%, test loss = 0.942593, training acuracy = 9.58%\n","Epoch 46: training loss = 0.946827, training acuracy = 9.86%, test loss = 0.947312, training acuracy = 9.58%\n","Epoch 47: training loss = 0.951525, training acuracy = 9.86%, test loss = 0.952018, training acuracy = 9.58%\n","Epoch 48: training loss = 0.956208, training acuracy = 9.86%, test loss = 0.95671, training acuracy = 9.58%\n","Epoch 49: training loss = 0.960878, training acuracy = 9.86%, test loss = 0.961388, training acuracy = 9.58%\n","Epoch 50: training loss = 0.965534, training acuracy = 9.86%, test loss = 0.966053, training acuracy = 9.58%\n","Epoch 51: training loss = 0.970176, training acuracy = 9.86%, test loss = 0.970703, training acuracy = 9.58%\n","Epoch 52: training loss = 0.974804, training acuracy = 9.86%, test loss = 0.975339, training acuracy = 9.58%\n","Epoch 53: training loss = 0.979418, training acuracy = 9.86%, test loss = 0.979961, training acuracy = 9.58%\n","Epoch 54: training loss = 0.984018, training acuracy = 9.86%, test loss = 0.984569, training acuracy = 9.58%\n","Epoch 55: training loss = 0.988603, training acuracy = 9.86%, test loss = 0.989162, training acuracy = 9.58%\n","Epoch 56: training loss = 0.993173, training acuracy = 9.86%, test loss = 0.993741, training acuracy = 9.58%\n","Epoch 57: training loss = 0.997729, training acuracy = 9.86%, test loss = 0.998305, training acuracy = 9.58%\n","Epoch 58: training loss = 1.002271, training acuracy = 9.86%, test loss = 1.002855, training acuracy = 9.58%\n","Epoch 59: training loss = 1.006798, training acuracy = 9.86%, test loss = 1.00739, training acuracy = 9.58%\n","Epoch 60: training loss = 1.01131, training acuracy = 9.86%, test loss = 1.01191, training acuracy = 9.58%\n","Epoch 61: training loss = 1.015807, training acuracy = 9.86%, test loss = 1.016416, training acuracy = 9.58%\n","Epoch 62: training loss = 1.02029, training acuracy = 9.86%, test loss = 1.020906, training acuracy = 9.58%\n","Epoch 63: training loss = 1.024757, training acuracy = 9.86%, test loss = 1.025382, training acuracy = 9.58%\n","Epoch 64: training loss = 1.02921, training acuracy = 9.86%, test loss = 1.029842, training acuracy = 9.58%\n","Epoch 65: training loss = 1.033648, training acuracy = 9.86%, test loss = 1.034288, training acuracy = 9.58%\n","Epoch 66: training loss = 1.03807, training acuracy = 9.86%, test loss = 1.038718, training acuracy = 9.58%\n","Epoch 67: training loss = 1.042478, training acuracy = 9.86%, test loss = 1.043134, training acuracy = 9.58%\n","Epoch 68: training loss = 1.04687, training acuracy = 9.86%, test loss = 1.047534, training acuracy = 9.58%\n","Epoch 69: training loss = 1.051247, training acuracy = 9.86%, test loss = 1.051919, training acuracy = 9.58%\n","Epoch 70: training loss = 1.05561, training acuracy = 9.86%, test loss = 1.056289, training acuracy = 9.58%\n","Epoch 71: training loss = 1.059957, training acuracy = 9.86%, test loss = 1.060644, training acuracy = 9.58%\n","Epoch 72: training loss = 1.064288, training acuracy = 9.86%, test loss = 1.064983, training acuracy = 9.58%\n","Epoch 73: training loss = 1.068605, training acuracy = 9.86%, test loss = 1.069307, training acuracy = 9.58%\n","Epoch 74: training loss = 1.072906, training acuracy = 9.87%, test loss = 1.073616, training acuracy = 9.58%\n","Epoch 75: training loss = 1.077192, training acuracy = 9.87%, test loss = 1.07791, training acuracy = 9.59%\n","Epoch 76: training loss = 1.081462, training acuracy = 9.87%, test loss = 1.082188, training acuracy = 9.59%\n","Epoch 77: training loss = 1.085718, training acuracy = 9.87%, test loss = 1.086451, training acuracy = 9.59%\n","Epoch 78: training loss = 1.089957, training acuracy = 9.87%, test loss = 1.090699, training acuracy = 9.59%\n","Epoch 79: training loss = 1.094182, training acuracy = 9.87%, test loss = 1.094931, training acuracy = 9.6%\n","Epoch 80: training loss = 1.098391, training acuracy = 9.88%, test loss = 1.099148, training acuracy = 9.6%\n","Epoch 81: training loss = 1.102585, training acuracy = 9.88%, test loss = 1.103349, training acuracy = 9.62%\n","Epoch 82: training loss = 1.106764, training acuracy = 9.88%, test loss = 1.107535, training acuracy = 9.63%\n","Epoch 83: training loss = 1.110927, training acuracy = 9.88%, test loss = 1.111706, training acuracy = 9.63%\n","Epoch 84: training loss = 1.115075, training acuracy = 9.89%, test loss = 1.115861, training acuracy = 9.63%\n","Epoch 85: training loss = 1.119208, training acuracy = 9.9%, test loss = 1.120001, training acuracy = 9.64%\n","Epoch 86: training loss = 1.123325, training acuracy = 9.91%, test loss = 1.124126, training acuracy = 9.65%\n","Epoch 87: training loss = 1.127427, training acuracy = 9.92%, test loss = 1.128235, training acuracy = 9.66%\n","Epoch 88: training loss = 1.131513, training acuracy = 9.93%, test loss = 1.132329, training acuracy = 9.66%\n","Epoch 89: training loss = 1.135585, training acuracy = 9.95%, test loss = 1.136408, training acuracy = 9.67%\n","Epoch 90: training loss = 1.139641, training acuracy = 9.97%, test loss = 1.140471, training acuracy = 9.68%\n","Epoch 91: training loss = 1.143682, training acuracy = 9.99%, test loss = 1.144519, training acuracy = 9.7%\n","Epoch 92: training loss = 1.147707, training acuracy = 10.0%, test loss = 1.148552, training acuracy = 9.7%\n","Epoch 93: training loss = 1.151717, training acuracy = 10.02%, test loss = 1.152569, training acuracy = 9.75%\n","Epoch 94: training loss = 1.155712, training acuracy = 10.05%, test loss = 1.156572, training acuracy = 9.75%\n","Epoch 95: training loss = 1.159692, training acuracy = 10.08%, test loss = 1.160559, training acuracy = 9.83%\n","Epoch 96: training loss = 1.163657, training acuracy = 10.11%, test loss = 1.16453, training acuracy = 9.85%\n","Epoch 97: training loss = 1.167606, training acuracy = 10.16%, test loss = 1.168487, training acuracy = 9.88%\n","Epoch 98: training loss = 1.171541, training acuracy = 10.2%, test loss = 1.172428, training acuracy = 9.88%\n","Epoch 99: training loss = 1.17546, training acuracy = 10.24%, test loss = 1.176355, training acuracy = 9.9%\n","Epoch 100: training loss = 1.179364, training acuracy = 10.26%, test loss = 1.180266, training acuracy = 9.96%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","metadata":{"id":"k6b82DZG6W3j","executionInfo":{"status":"aborted","timestamp":1661232281797,"user_tz":-540,"elapsed":3,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"source":["# Assignment 4 구현은 여기서 ()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython import get_ipython\n","get_ipython().magic('reset -sf')\n","\n","import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")\n","\n","# data preprocessing\n","\n","num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1    "],"metadata":{"id":"E8VDmGt9OAsv","executionInfo":{"status":"ok","timestamp":1661233552017,"user_tz":-540,"elapsed":66064,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["def relu(x):\n","  out = np.maximum(0, x)\n","  return out"],"metadata":{"id":"alZPbUIDNy1P","executionInfo":{"status":"ok","timestamp":1661233585120,"user_tz":-540,"elapsed":300,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = relu(params[\"S1\"])\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = relu(params[\"S2\"])\n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = relu(params[\"S3\"])\n","\n","  return params"],"metadata":{"id":"yOqkH3riNvxY","executionInfo":{"status":"ok","timestamp":1661233572622,"user_tz":-540,"elapsed":316,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["def foward_pass_test(x, params):\n","\n","  params_test = {}\n","  \n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = relu(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = relu(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = relu(params_test[\"S3\"])\n","\n","  return params_test"],"metadata":{"id":"USKB8yYaOAqU","executionInfo":{"status":"ok","timestamp":1661233649387,"user_tz":-540,"elapsed":292,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden1_size = 128\n","hidden2_size = 64\n","num_class = 10\n","\n","params = {\"W1\": np.random.randn(hidden1_size, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden1_size, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden2_size, hidden1_size) * np.sqrt(1/ hidden1_size),\n","          \"b2\": np.zeros((hidden2_size, 1)) * np.sqrt(1/ hidden1_size),\n","          \"W3\": np.random.randn(num_class, hidden2_size) * np.sqrt(1/ hidden2_size),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden2_size)\n","          }"],"metadata":{"id":"jgMrDO8YOAwC","executionInfo":{"status":"ok","timestamp":1661233553064,"user_tz":-540,"elapsed":10,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["def compute_loss(y_true, y_pred):\n","  # loss calculation\n","\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"metadata":{"id":"LXEudbVKOAyh","executionInfo":{"status":"ok","timestamp":1661233553065,"user_tz":-540,"elapsed":10,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"metadata":{"id":"y6PEjk1AOA09","executionInfo":{"status":"ok","timestamp":1661233553066,"user_tz":-540,"elapsed":11,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["def backward_pass(x, y_true, params):\n","  grads = {}\n","\n","  dS3 = params[\"A3\"] - y_true\n","  \n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * relu(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * relu(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","  \n","  return  grads"],"metadata":{"id":"L89OXd2bOA3I","executionInfo":{"status":"ok","timestamp":1661233636988,"user_tz":-540,"elapsed":293,"user":{"displayName":"황진우(상경대학 응용통계학과)","userId":"05375191865458090304"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.3\n","\n","for i in range(epochs):\n","\n","  if i == 0:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A3\"])\n","  train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A3\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PfZAPNanOBCT","outputId":"d5699ee6-c9e6-464f-ecc9-b62efe5c0a9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in multiply\n","  \"\"\"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = nan, training acuracy = 10.4%, test loss = nan, training acuracy = 10.23%\n","Epoch 2: training loss = nan, training acuracy = 12.62%, test loss = nan, training acuracy = 12.83%\n","Epoch 3: training loss = nan, training acuracy = 15.07%, test loss = nan, training acuracy = 15.36%\n","Epoch 4: training loss = nan, training acuracy = 17.68%, test loss = nan, training acuracy = 18.02%\n","Epoch 5: training loss = nan, training acuracy = 20.27%, test loss = nan, training acuracy = 20.84%\n","Epoch 6: training loss = nan, training acuracy = 23.09%, test loss = nan, training acuracy = 23.72%\n","Epoch 7: training loss = nan, training acuracy = 25.61%, test loss = nan, training acuracy = 26.5%\n","Epoch 8: training loss = nan, training acuracy = 28.14%, test loss = nan, training acuracy = 28.97%\n","Epoch 9: training loss = nan, training acuracy = 30.38%, test loss = nan, training acuracy = 31.62%\n","Epoch 10: training loss = nan, training acuracy = 32.42%, test loss = nan, training acuracy = 33.77%\n","Epoch 11: training loss = nan, training acuracy = 34.44%, test loss = nan, training acuracy = 35.55%\n","Epoch 12: training loss = nan, training acuracy = 36.33%, test loss = nan, training acuracy = 37.47%\n","Epoch 13: training loss = nan, training acuracy = 38.11%, test loss = nan, training acuracy = 39.34%\n","Epoch 14: training loss = nan, training acuracy = 39.85%, test loss = nan, training acuracy = 41.13%\n","Epoch 15: training loss = nan, training acuracy = 41.38%, test loss = nan, training acuracy = 42.51%\n","Epoch 16: training loss = nan, training acuracy = 42.9%, test loss = nan, training acuracy = 43.73%\n","Epoch 17: training loss = nan, training acuracy = 44.29%, test loss = nan, training acuracy = 45.13%\n","Epoch 18: training loss = nan, training acuracy = 45.55%, test loss = nan, training acuracy = 46.49%\n","Epoch 19: training loss = nan, training acuracy = 46.73%, test loss = nan, training acuracy = 47.64%\n","Epoch 20: training loss = nan, training acuracy = 47.88%, test loss = nan, training acuracy = 48.95%\n","Epoch 21: training loss = nan, training acuracy = 48.98%, test loss = nan, training acuracy = 49.97%\n","Epoch 22: training loss = nan, training acuracy = 50.04%, test loss = nan, training acuracy = 51.09%\n","Epoch 23: training loss = nan, training acuracy = 51.05%, test loss = nan, training acuracy = 52.16%\n","Epoch 24: training loss = nan, training acuracy = 52.02%, test loss = nan, training acuracy = 53.23%\n","Epoch 25: training loss = nan, training acuracy = 53.04%, test loss = nan, training acuracy = 54.18%\n","Epoch 26: training loss = nan, training acuracy = 54.03%, test loss = nan, training acuracy = 54.99%\n","Epoch 27: training loss = nan, training acuracy = 54.97%, test loss = nan, training acuracy = 55.99%\n","Epoch 28: training loss = nan, training acuracy = 55.95%, test loss = nan, training acuracy = 56.84%\n","Epoch 29: training loss = nan, training acuracy = 56.93%, test loss = nan, training acuracy = 57.72%\n","Epoch 30: training loss = nan, training acuracy = 57.87%, test loss = nan, training acuracy = 58.79%\n","Epoch 31: training loss = nan, training acuracy = 58.83%, test loss = nan, training acuracy = 59.67%\n","Epoch 32: training loss = nan, training acuracy = 59.71%, test loss = nan, training acuracy = 60.37%\n","Epoch 33: training loss = nan, training acuracy = 60.63%, test loss = nan, training acuracy = 61.31%\n","Epoch 34: training loss = nan, training acuracy = 61.42%, test loss = nan, training acuracy = 62.16%\n","Epoch 35: training loss = nan, training acuracy = 62.32%, test loss = nan, training acuracy = 62.99%\n","Epoch 36: training loss = nan, training acuracy = 63.14%, test loss = nan, training acuracy = 64.0%\n","Epoch 37: training loss = nan, training acuracy = 63.99%, test loss = nan, training acuracy = 64.89%\n","Epoch 38: training loss = nan, training acuracy = 64.86%, test loss = nan, training acuracy = 65.81%\n","Epoch 39: training loss = nan, training acuracy = 65.76%, test loss = nan, training acuracy = 66.62%\n","Epoch 40: training loss = nan, training acuracy = 66.61%, test loss = nan, training acuracy = 67.56%\n","Epoch 41: training loss = nan, training acuracy = 67.47%, test loss = nan, training acuracy = 68.59%\n","Epoch 42: training loss = nan, training acuracy = 68.32%, test loss = nan, training acuracy = 69.38%\n","Epoch 43: training loss = nan, training acuracy = 69.23%, test loss = nan, training acuracy = 70.27%\n","Epoch 44: training loss = nan, training acuracy = 70.06%, test loss = nan, training acuracy = 71.26%\n","Epoch 45: training loss = nan, training acuracy = 70.92%, test loss = nan, training acuracy = 72.08%\n","Epoch 46: training loss = nan, training acuracy = 71.75%, test loss = nan, training acuracy = 72.97%\n","Epoch 47: training loss = nan, training acuracy = 72.48%, test loss = nan, training acuracy = 73.7%\n","Epoch 48: training loss = nan, training acuracy = 73.57%, test loss = nan, training acuracy = 74.98%\n","Epoch 49: training loss = nan, training acuracy = 64.52%, test loss = nan, training acuracy = 65.57%\n","Epoch 50: training loss = nan, training acuracy = 24.16%, test loss = nan, training acuracy = 24.48%\n","Epoch 51: training loss = nan, training acuracy = 18.62%, test loss = nan, training acuracy = 17.91%\n","Epoch 52: training loss = nan, training acuracy = 18.15%, test loss = nan, training acuracy = 17.91%\n","Epoch 53: training loss = nan, training acuracy = 24.02%, test loss = nan, training acuracy = 23.82%\n","Epoch 54: training loss = nan, training acuracy = 29.72%, test loss = nan, training acuracy = 29.85%\n","Epoch 55: training loss = nan, training acuracy = 35.08%, test loss = nan, training acuracy = 35.68%\n","Epoch 56: training loss = nan, training acuracy = 39.7%, test loss = nan, training acuracy = 40.47%\n","Epoch 57: training loss = nan, training acuracy = 43.71%, test loss = nan, training acuracy = 44.38%\n","Epoch 58: training loss = nan, training acuracy = 46.78%, test loss = nan, training acuracy = 47.52%\n","Epoch 59: training loss = nan, training acuracy = 49.53%, test loss = nan, training acuracy = 50.33%\n","Epoch 60: training loss = nan, training acuracy = 51.83%, test loss = nan, training acuracy = 52.52%\n","Epoch 61: training loss = nan, training acuracy = 53.86%, test loss = nan, training acuracy = 54.53%\n","Epoch 62: training loss = nan, training acuracy = 55.73%, test loss = nan, training acuracy = 56.47%\n","Epoch 63: training loss = nan, training acuracy = 57.23%, test loss = nan, training acuracy = 58.21%\n","Epoch 64: training loss = nan, training acuracy = 58.57%, test loss = nan, training acuracy = 59.68%\n","Epoch 65: training loss = nan, training acuracy = 59.96%, test loss = nan, training acuracy = 61.02%\n","Epoch 66: training loss = nan, training acuracy = 61.11%, test loss = nan, training acuracy = 62.37%\n","Epoch 67: training loss = nan, training acuracy = 62.14%, test loss = nan, training acuracy = 63.28%\n","Epoch 68: training loss = nan, training acuracy = 63.08%, test loss = nan, training acuracy = 64.36%\n","Epoch 69: training loss = nan, training acuracy = 63.99%, test loss = nan, training acuracy = 65.09%\n","Epoch 70: training loss = nan, training acuracy = 64.72%, test loss = nan, training acuracy = 66.06%\n","Epoch 71: training loss = nan, training acuracy = 65.45%, test loss = nan, training acuracy = 66.79%\n","Epoch 72: training loss = nan, training acuracy = 66.12%, test loss = nan, training acuracy = 67.33%\n","Epoch 73: training loss = nan, training acuracy = 66.65%, test loss = nan, training acuracy = 67.88%\n","Epoch 74: training loss = nan, training acuracy = 67.11%, test loss = nan, training acuracy = 68.39%\n","Epoch 75: training loss = nan, training acuracy = 67.64%, test loss = nan, training acuracy = 68.81%\n","Epoch 76: training loss = nan, training acuracy = 68.07%, test loss = nan, training acuracy = 69.16%\n","Epoch 77: training loss = nan, training acuracy = 68.52%, test loss = nan, training acuracy = 69.55%\n","Epoch 78: training loss = nan, training acuracy = 68.94%, test loss = nan, training acuracy = 70.08%\n","Epoch 79: training loss = nan, training acuracy = 69.34%, test loss = nan, training acuracy = 70.45%\n","Epoch 80: training loss = nan, training acuracy = 69.75%, test loss = nan, training acuracy = 70.71%\n","Epoch 81: training loss = nan, training acuracy = 70.15%, test loss = nan, training acuracy = 71.26%\n","Epoch 82: training loss = nan, training acuracy = 70.49%, test loss = nan, training acuracy = 71.68%\n","Epoch 83: training loss = nan, training acuracy = 70.83%, test loss = nan, training acuracy = 72.12%\n","Epoch 84: training loss = nan, training acuracy = 71.25%, test loss = nan, training acuracy = 72.58%\n","Epoch 85: training loss = nan, training acuracy = 71.62%, test loss = nan, training acuracy = 73.03%\n","Epoch 86: training loss = nan, training acuracy = 71.99%, test loss = nan, training acuracy = 73.4%\n","Epoch 87: training loss = nan, training acuracy = 72.38%, test loss = nan, training acuracy = 73.74%\n","Epoch 88: training loss = nan, training acuracy = 72.75%, test loss = nan, training acuracy = 74.16%\n","Epoch 89: training loss = nan, training acuracy = 73.15%, test loss = nan, training acuracy = 74.42%\n","Epoch 90: training loss = nan, training acuracy = 73.56%, test loss = nan, training acuracy = 74.78%\n","Epoch 91: training loss = nan, training acuracy = 73.94%, test loss = nan, training acuracy = 75.2%\n","Epoch 92: training loss = nan, training acuracy = 74.34%, test loss = nan, training acuracy = 75.53%\n","Epoch 93: training loss = nan, training acuracy = 74.7%, test loss = nan, training acuracy = 75.89%\n","Epoch 94: training loss = nan, training acuracy = 75.07%, test loss = nan, training acuracy = 76.28%\n","Epoch 95: training loss = nan, training acuracy = 75.4%, test loss = nan, training acuracy = 76.5%\n","Epoch 96: training loss = nan, training acuracy = 75.79%, test loss = nan, training acuracy = 76.89%\n","Epoch 97: training loss = nan, training acuracy = 76.13%, test loss = nan, training acuracy = 77.16%\n","Epoch 98: training loss = nan, training acuracy = 76.47%, test loss = nan, training acuracy = 77.48%\n","Epoch 99: training loss = nan, training acuracy = 76.83%, test loss = nan, training acuracy = 77.74%\n","Epoch 100: training loss = nan, training acuracy = 77.16%, test loss = nan, training acuracy = 78.08%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","source":["학습률을 낮추고 (0.2)로 활성화함수를 relu로 바꾸었다.\n","그랬더니 정확도가 엄청나게 올랐다.\n","relu의 성능을 느낄 수 있었다.\n","그리고 loss를 계산할 때, 값이 전부 nan이 나왔는데, relu함수가 0을 반환하기도 해서 그런 결과가 나온 거 같다"],"metadata":{"id":"5_UGYe9f9gep"}}]}