{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[0818]_Deep_Learning_Basic_과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0,x)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_relu(x):\n",
        "  return np.diff(relu(x)) # 0 혹은 1"
      ],
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
      ],
      "metadata": {
        "id": "OSLv3Skzd7WR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "\n",
        "num_train = 60000\n",
        "num_class = 10\n",
        "\n",
        "x_train = np.float32(mnist.data[:num_train]).T\n",
        "y_train_index = np.int32(mnist.target[:num_train]).T\n",
        "x_test = np.float32(mnist.data[num_train:]).T\n",
        "y_test_index = np.int32(mnist.target[num_train:]).T\n",
        "\n",
        "# Normalization\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "x_size = x_train.shape[0]\n",
        "\n",
        "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
        "for idx in range(y_train_index.shape[0]):\n",
        "  y_train[y_train_index[idx], idx] = 1\n",
        "\n",
        "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
        "for idx in range(y_test_index.shape[0]):\n",
        "  y_test[y_test_index[idx], idx] = 1    "
      ],
      "metadata": {
        "id": "BZsRX9hJd7d3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_size)\n",
        "print(num_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG-36O8bh0hU",
        "outputId": "779a93d8-566b-4882-9946-901159dcc145"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "784\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter initialization\n",
        "\n",
        "hidden_size1 = 128 # hidden unit size1\n",
        "hidden_size2 = 64\n",
        "\n",
        "# two-layer neural network\n",
        "\n",
        "params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n",
        "          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/ x_size),\n",
        "          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/ hidden_size1),\n",
        "          \"b2\": np.zeros((hidden_size2, 1)) * np.sqrt(1/ hidden_size1),\n",
        "          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n",
        "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n",
        "          }\n",
        "# Xavier initialization: https://reniew.github.io/13/"
      ],
      "metadata": {
        "id": "WxBl8_cleEkj"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  # derivative of sigmoid\n",
        "  exp = np.exp(-x)\n",
        "  return (exp)/((1+exp)**2)\n",
        "\n",
        "def softmax(x):\n",
        "  exp = np.exp(x)\n",
        "  return exp/np.sum(exp, axis=0)"
      ],
      "metadata": {
        "id": "mmzPt2IkjO7j"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y_true, y_pred):\n",
        "  # loss calculation\n",
        "\n",
        "  num_sample = y_true.shape[1]\n",
        "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
        "  \n",
        "  return Li/num_sample"
      ],
      "metadata": {
        "id": "syc_XWPbjQeZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = sigmoid(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "jeCBHdZFjRql"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass_test(x, params):\n",
        "\n",
        "  params_test = {}\n",
        "  \n",
        "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
        "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
        "  params_test[\"A2\"] = sigmoid(params_test[\"S2\"])\n",
        "  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
        "  params_test[\"A3\"] = softmax(params_test[\"S32\"])\n",
        "\n",
        "  return params_test"
      ],
      "metadata": {
        "id": "6nO09y-ljYv5"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_true, y_pred):\n",
        "  y_true_idx = np.argmax(y_true, axis = 0)\n",
        "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
        "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
        "\n",
        "  accuracy = num_correct / y_true.shape[1] * 100\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "SQWGNNLJjanS"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "source": [
        "# def backward_pass(x, y_true, params):\n",
        "\n",
        "#   return \n",
        "\n",
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "  # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
        "  # dS3 is softmax + CE loss derivative\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJO249A3jhk"
      },
      "source": [
        "# Assignment 3 구현은 여기서 ()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "6NHUlHxd8E4W"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지를 텐서로 변경\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "tWLKUF7F8GSO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "id": "Z_CbuVgo8H2q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "6BqaagB68KUE"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "5j7vuTVu8L3H"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpOwXkJb8PTm",
        "outputId": "06c721c8-4842-4eea-e54c-4240d285d984"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layer1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (layer2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (layer3): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n",
        "                         # require_true 얘는 학습되는 애구나 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fz1dzf58Qft",
        "outputId": "a872c0e9-fe37-45b6-97dd-1672b6455d39"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 1.8535e-02, -3.9359e-03,  1.5177e-02,  ...,  1.4075e-02,\n",
              "          -1.5928e-02,  2.1335e-02],\n",
              "         [ 3.3270e-02,  2.0526e-02,  2.2271e-02,  ..., -2.4958e-02,\n",
              "          -1.3841e-02,  1.1795e-02],\n",
              "         [ 3.0407e-05, -1.7983e-02,  1.0255e-02,  ..., -2.7280e-02,\n",
              "          -1.2003e-02, -1.7709e-02],\n",
              "         ...,\n",
              "         [ 8.1233e-03,  2.3497e-02,  1.0214e-02,  ..., -7.7300e-03,\n",
              "          -2.4493e-02, -2.6134e-02],\n",
              "         [-2.7089e-02, -1.7551e-02, -2.4000e-02,  ..., -2.5035e-02,\n",
              "           1.0469e-02,  1.3631e-02],\n",
              "         [-2.0965e-02,  1.5920e-02,  3.1924e-02,  ..., -2.9084e-02,\n",
              "           7.9370e-03, -2.5765e-02]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-2.7589e-02, -9.2600e-03, -2.2104e-02,  2.3236e-02,  3.1727e-02,\n",
              "          4.2281e-03, -2.5305e-02,  1.8140e-02,  1.6208e-03,  1.2046e-02,\n",
              "         -6.4479e-03,  1.0262e-02, -3.5409e-02, -6.8497e-03,  5.2444e-03,\n",
              "          1.9292e-03,  5.0846e-03,  3.3134e-02,  2.0722e-02, -6.0715e-03,\n",
              "         -3.2585e-02,  3.3015e-02,  1.5882e-02, -1.2458e-03,  3.3655e-02,\n",
              "          7.6273e-03,  4.1447e-03, -1.6520e-02, -1.5749e-02, -1.0059e-02,\n",
              "         -2.1440e-02, -2.9469e-02,  4.9614e-04, -1.7821e-03,  3.2904e-02,\n",
              "          2.1028e-02, -3.5255e-02, -1.1242e-02, -3.3861e-02, -1.5295e-02,\n",
              "         -2.7978e-03,  2.3618e-03, -3.1057e-02, -8.1016e-03,  3.2606e-02,\n",
              "         -4.4856e-03, -8.5650e-03, -4.3518e-03,  2.4749e-02,  2.0408e-02,\n",
              "          1.2126e-02,  1.2126e-02,  1.1751e-02,  1.5480e-02, -1.6613e-02,\n",
              "          3.2487e-02,  6.9122e-03,  2.7431e-05, -3.2216e-02, -8.8687e-03,\n",
              "         -3.1193e-02,  1.5780e-02, -1.2649e-02,  1.2460e-02, -7.6609e-03,\n",
              "          2.6981e-02,  1.4211e-02, -1.2915e-02, -3.0391e-02,  1.3205e-03,\n",
              "          3.4945e-02, -3.1732e-02, -2.8350e-04,  5.7739e-03,  7.1378e-03,\n",
              "          1.4129e-02,  2.4250e-02, -3.3717e-02,  1.6283e-02,  2.0447e-02,\n",
              "          1.4214e-02,  1.9930e-02,  3.0581e-02,  2.3211e-02, -8.7816e-03,\n",
              "         -6.4629e-03,  9.5127e-03,  1.3255e-02, -1.2258e-02, -2.7496e-02,\n",
              "         -2.0672e-03, -3.1992e-03,  3.1737e-03,  1.9835e-02,  2.8498e-02,\n",
              "         -1.3063e-02, -1.5825e-02,  8.6646e-04,  2.2440e-02,  2.2517e-02,\n",
              "          9.3588e-03, -1.5761e-02, -2.2348e-03, -1.2590e-02,  2.5991e-03,\n",
              "          1.3767e-02,  6.1727e-03, -8.4763e-03,  2.1223e-02, -9.1817e-03,\n",
              "          4.0094e-03,  9.2532e-04,  8.9671e-03,  1.2269e-02, -1.4535e-02,\n",
              "          7.3086e-04,  1.8545e-02, -2.5456e-02,  3.3722e-03,  2.1758e-02,\n",
              "         -2.3485e-02,  1.0545e-02,  2.5890e-02,  1.6291e-02,  1.9129e-02,\n",
              "         -3.3825e-02,  1.7319e-02,  3.3573e-02], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0221,  0.0674, -0.0480,  ...,  0.0801, -0.0647, -0.0433],\n",
              "         [-0.0424,  0.0509, -0.0884,  ..., -0.0433, -0.0117,  0.0014],\n",
              "         [ 0.0178, -0.0517, -0.0635,  ..., -0.0726, -0.0336,  0.0552],\n",
              "         ...,\n",
              "         [-0.0118, -0.0024, -0.0504,  ..., -0.0094, -0.0148,  0.0042],\n",
              "         [ 0.0306,  0.0766, -0.0280,  ...,  0.0430,  0.0872,  0.0337],\n",
              "         [-0.0773, -0.0109,  0.0749,  ...,  0.0446,  0.0722,  0.0691]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0552,  0.0373, -0.0759,  0.0138,  0.0696,  0.0513, -0.0684,  0.0376,\n",
              "         -0.0243, -0.0335,  0.0769,  0.0844,  0.0747, -0.0695,  0.0731,  0.0615,\n",
              "          0.0606, -0.0191, -0.0819,  0.0578,  0.0543, -0.0244,  0.0828,  0.0771,\n",
              "          0.0469,  0.0559, -0.0576,  0.0131, -0.0531, -0.0155,  0.0376,  0.0118,\n",
              "         -0.0145, -0.0325,  0.0773,  0.0041, -0.0171, -0.0417, -0.0067,  0.0715,\n",
              "          0.0636,  0.0273,  0.0221,  0.0270, -0.0579, -0.0666, -0.0083,  0.0198,\n",
              "         -0.0787,  0.0369,  0.0804,  0.0785, -0.0861, -0.0691, -0.0273,  0.0235,\n",
              "          0.0796, -0.0407,  0.0392,  0.0693,  0.0723,  0.0806, -0.0290,  0.0269],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 8.6556e-02,  4.0344e-02,  9.3353e-02, -7.3903e-02,  9.5685e-02,\n",
              "          -8.9895e-02,  8.9914e-04,  2.5290e-02, -7.9187e-03, -9.4749e-02,\n",
              "           1.0585e-01,  4.1112e-02, -8.0024e-02,  2.1645e-02, -4.6632e-02,\n",
              "           6.5280e-03,  9.5003e-02, -6.2725e-02, -7.5005e-02,  3.0535e-03,\n",
              "           6.9732e-03,  1.6396e-03, -1.1183e-01, -2.0979e-02, -7.8494e-02,\n",
              "           7.2336e-02,  1.2040e-01, -2.3264e-02, -8.1860e-02,  1.1188e-01,\n",
              "           4.7799e-02, -5.1079e-02,  6.3353e-02, -7.7351e-02,  1.0192e-01,\n",
              "           9.2072e-02,  4.0313e-02, -2.8680e-02,  6.5467e-02,  5.6908e-02,\n",
              "           8.8442e-02, -9.9067e-02, -6.5220e-02,  4.8337e-02,  6.7750e-02,\n",
              "           7.6407e-02, -3.9356e-02,  1.0397e-01,  1.1831e-02, -7.8386e-03,\n",
              "           7.7180e-02,  7.4887e-02, -8.2813e-02, -6.6230e-02,  5.3204e-02,\n",
              "           7.2674e-02, -6.0273e-03, -4.9565e-02, -1.1504e-01, -2.8745e-02,\n",
              "           9.5721e-02, -2.0499e-02,  7.9001e-02, -3.7134e-02],\n",
              "         [-1.1169e-01, -8.9679e-02,  1.3896e-02, -7.3151e-02,  8.6279e-02,\n",
              "           7.4545e-04, -3.2801e-02, -1.8665e-04, -6.6974e-02, -7.7056e-02,\n",
              "          -1.0366e-01,  1.0021e-01,  3.6463e-02, -1.0016e-01,  6.5661e-02,\n",
              "          -1.3089e-02, -1.1191e-01,  3.8747e-02, -1.1315e-01, -7.2494e-02,\n",
              "          -1.0891e-01,  1.0799e-01,  3.1640e-02,  1.4046e-02, -3.8216e-03,\n",
              "          -1.2037e-01, -1.1563e-01,  4.0937e-02, -1.1698e-01, -5.0769e-02,\n",
              "          -7.5958e-02, -2.8855e-02, -9.2524e-02,  8.7790e-03, -4.7972e-02,\n",
              "           2.8767e-02, -1.2488e-01, -1.0139e-01, -8.4022e-03,  4.8968e-02,\n",
              "          -9.7444e-02, -1.6092e-02, -5.7624e-02,  8.8614e-02,  6.6779e-02,\n",
              "           1.1524e-01,  1.0440e-01, -3.0606e-02, -3.8905e-02, -9.5585e-02,\n",
              "           2.6004e-02,  5.0997e-02, -1.0321e-02,  7.8857e-02, -7.4658e-02,\n",
              "          -8.8737e-02, -6.9627e-02, -9.3245e-02,  7.4371e-02,  1.0529e-01,\n",
              "          -9.1175e-02,  7.6522e-02, -5.5793e-02, -9.9681e-02],\n",
              "         [-2.3318e-02,  4.7028e-02,  8.2238e-02,  9.8517e-02,  7.2893e-02,\n",
              "          -1.0676e-01,  3.2584e-02, -7.0425e-02, -8.5210e-02, -2.8669e-02,\n",
              "           1.1037e-01,  6.4547e-02, -1.1674e-01, -6.9240e-02,  4.0238e-02,\n",
              "          -6.5906e-03, -4.1088e-02,  9.4044e-02,  9.3450e-02, -3.7852e-02,\n",
              "          -1.0290e-01, -2.4142e-02, -7.6118e-02, -5.5327e-02,  1.5169e-02,\n",
              "           1.1925e-01,  2.1035e-02, -7.2814e-02, -8.5641e-02, -2.3637e-02,\n",
              "           2.1380e-02,  4.6661e-02, -1.0952e-01, -7.3716e-05,  3.8654e-02,\n",
              "          -9.1353e-02, -9.3016e-02, -8.9478e-02,  3.4220e-02,  5.8561e-02,\n",
              "           1.0839e-02,  8.0094e-03,  3.5379e-02, -9.5012e-02,  4.6094e-02,\n",
              "          -9.7656e-03, -5.1163e-02, -1.3859e-02, -4.8937e-02,  1.0710e-02,\n",
              "          -5.5549e-02,  7.4094e-02,  9.3394e-02, -1.0888e-01, -2.3798e-02,\n",
              "          -5.8987e-02,  8.6720e-02, -1.1921e-01, -2.6487e-02,  1.0307e-01,\n",
              "           4.9046e-03, -6.7401e-02,  1.1498e-01, -8.0067e-02],\n",
              "         [ 2.2122e-02,  5.0870e-02,  7.8996e-03,  1.0221e-01,  7.5983e-03,\n",
              "          -1.0014e-01,  4.3982e-02,  4.3650e-02, -1.1389e-01,  1.2378e-01,\n",
              "          -7.5688e-02, -3.4862e-02, -1.0800e-01,  8.2674e-03,  5.6438e-02,\n",
              "           5.9465e-02, -3.2530e-02,  1.0627e-01,  6.4858e-02, -9.5340e-02,\n",
              "          -4.0738e-02, -3.8081e-02,  1.1098e-01, -3.8477e-02, -7.2043e-02,\n",
              "          -5.0419e-02,  3.5988e-02, -9.0510e-02, -1.0262e-01,  4.3313e-02,\n",
              "           4.0366e-02,  9.9439e-02,  1.3345e-02,  1.1372e-01, -1.0068e-01,\n",
              "           6.9190e-02,  1.2924e-02, -8.0272e-02, -1.9148e-03, -8.6818e-02,\n",
              "           5.9435e-02,  2.5898e-02,  9.1634e-02, -3.0028e-02, -8.0148e-02,\n",
              "          -3.1752e-02,  1.2308e-01, -6.5803e-02,  9.9133e-02, -6.6257e-03,\n",
              "          -6.4003e-02, -5.9358e-02, -3.1130e-02, -1.2038e-01,  3.5343e-02,\n",
              "          -5.1329e-02, -1.1031e-01,  1.2053e-01,  6.0804e-03, -1.9902e-02,\n",
              "          -5.0609e-02,  3.3074e-02,  8.0655e-02,  2.6246e-02],\n",
              "         [ 8.1208e-02, -1.5645e-02,  9.2655e-02,  3.8711e-02,  1.1597e-01,\n",
              "          -4.3299e-02,  7.9561e-02,  1.1500e-01,  8.2410e-02, -1.0908e-01,\n",
              "          -5.1513e-02,  1.1439e-01,  6.2320e-02, -4.3254e-04, -3.3507e-02,\n",
              "           6.3872e-02,  1.6234e-02,  4.9965e-02,  1.0852e-01,  1.0710e-01,\n",
              "           8.4343e-02,  3.6114e-02, -1.1065e-01, -4.6502e-02, -2.9501e-02,\n",
              "           5.9124e-02, -8.3559e-02, -5.6531e-02, -8.4315e-02,  1.0998e-01,\n",
              "           7.6282e-02,  5.0996e-03,  2.4233e-02, -8.4679e-02,  3.5591e-02,\n",
              "           9.4591e-03, -2.5159e-02, -1.9724e-02, -6.1474e-02, -4.5831e-02,\n",
              "          -7.6188e-02, -2.7347e-02,  3.3140e-02, -5.1260e-02, -5.8181e-02,\n",
              "          -6.9758e-02, -8.7626e-02, -8.8028e-02, -1.0813e-01, -3.5548e-02,\n",
              "           1.1611e-01, -1.8332e-02,  6.5738e-02,  1.0607e-02,  3.8035e-02,\n",
              "          -6.5248e-02,  3.9385e-02,  9.8391e-03,  1.0382e-01, -1.0460e-01,\n",
              "          -9.6223e-02, -1.2647e-02,  7.0212e-02,  6.8084e-02],\n",
              "         [ 5.1568e-02, -1.0097e-01, -8.9231e-02, -7.2790e-02, -5.2565e-02,\n",
              "           1.6746e-02,  2.3618e-02,  5.6497e-02, -6.9520e-02,  1.1576e-02,\n",
              "          -4.1975e-02, -3.3927e-02,  4.3864e-03,  1.0900e-01,  9.4336e-03,\n",
              "          -1.2218e-01,  1.0256e-01, -6.1577e-02,  8.5481e-02, -1.0307e-01,\n",
              "           6.8998e-02,  2.5151e-02,  1.1709e-01, -1.1340e-01, -6.4579e-02,\n",
              "           4.8657e-02,  4.0215e-02,  1.1045e-01,  9.6231e-02,  1.0564e-01,\n",
              "          -4.2722e-02,  1.1136e-01,  1.4617e-02,  1.2065e-01, -1.0087e-01,\n",
              "          -5.5761e-02, -2.3095e-02, -1.4462e-02, -1.1449e-01, -2.4373e-03,\n",
              "          -7.0482e-02,  8.4245e-02, -5.3547e-02, -5.8332e-02, -1.1990e-01,\n",
              "           1.6900e-02,  4.2391e-02,  1.1796e-01,  1.2595e-02, -4.4827e-02,\n",
              "           4.7031e-02,  3.7125e-02, -6.7505e-02, -1.0974e-01,  5.2878e-02,\n",
              "           7.9576e-02,  5.9774e-02, -4.4237e-02, -3.6993e-02,  1.0357e-01,\n",
              "          -7.3411e-02,  6.4805e-02, -9.8769e-02, -2.0014e-02],\n",
              "         [ 4.1452e-02,  3.2173e-02,  3.3858e-02,  5.7996e-02, -2.7688e-02,\n",
              "          -8.9033e-02,  3.7624e-03,  2.1783e-02,  8.8244e-02, -1.1586e-01,\n",
              "          -6.9154e-02,  4.0731e-02, -6.7003e-03, -9.1409e-02,  1.2393e-01,\n",
              "          -1.2384e-01, -6.4403e-02, -3.0598e-02, -5.6888e-02, -1.1110e-01,\n",
              "          -3.5585e-02,  3.0862e-02,  6.8493e-02,  3.5532e-02, -9.7299e-02,\n",
              "          -3.2212e-02, -3.9094e-02, -4.0170e-02, -4.8081e-02,  5.8811e-02,\n",
              "           1.2485e-01, -7.4555e-02, -9.6567e-02,  7.0875e-02,  6.6289e-02,\n",
              "          -1.1334e-01, -1.1600e-01, -4.6880e-02,  8.1622e-02, -8.6441e-02,\n",
              "           1.0730e-01, -7.8441e-02, -5.0050e-02, -7.1285e-02,  1.2043e-01,\n",
              "           8.1489e-02,  1.4831e-02,  7.0937e-02, -1.1686e-01,  3.9081e-02,\n",
              "           2.9389e-02, -1.2358e-01, -8.8491e-02,  4.1581e-02,  1.2406e-01,\n",
              "           1.0664e-01,  2.6836e-02, -1.1284e-02, -6.0770e-02, -7.1347e-02,\n",
              "           1.0541e-01, -4.3598e-02,  4.2621e-02,  6.3940e-02],\n",
              "         [-1.5696e-03, -8.4283e-02, -6.2367e-02,  1.1583e-01,  8.3826e-03,\n",
              "           1.0094e-01, -8.7667e-02, -8.4694e-02,  8.7669e-02,  2.0770e-02,\n",
              "           6.9244e-02, -6.7235e-02,  2.3811e-02, -3.6476e-02,  2.1442e-02,\n",
              "          -8.3782e-02,  8.4486e-03,  7.2891e-02,  2.7298e-02,  1.0101e-01,\n",
              "          -9.8332e-02, -1.1335e-01, -1.0155e-01, -8.8781e-02,  2.6731e-02,\n",
              "          -6.9227e-02, -4.7037e-02,  1.1877e-01, -8.1031e-02,  5.4846e-03,\n",
              "          -7.0455e-02, -9.3170e-04,  1.3206e-02,  3.8624e-03,  1.0453e-01,\n",
              "           6.9850e-02,  9.8428e-02,  3.4177e-02,  1.1799e-02,  5.3341e-02,\n",
              "           5.7167e-04, -3.2745e-02,  4.6115e-02, -1.1684e-01,  5.9336e-02,\n",
              "          -9.5292e-02,  1.1449e-01,  1.1160e-01, -8.2006e-02,  2.8476e-02,\n",
              "          -1.5329e-02,  6.5989e-02,  1.1637e-01, -5.7686e-02, -1.1569e-01,\n",
              "          -1.0395e-01, -2.0819e-03, -3.9391e-02,  1.6797e-02, -4.1799e-02,\n",
              "           2.4565e-02,  1.1605e-01, -8.8947e-02,  7.5936e-02],\n",
              "         [-6.1534e-03, -1.1524e-01, -5.9534e-02, -6.6281e-02,  8.4453e-02,\n",
              "           3.5689e-02,  1.2353e-01,  2.5105e-02, -9.4393e-02, -3.4781e-02,\n",
              "          -2.6636e-02,  6.5109e-02,  4.3848e-02, -6.2124e-02,  9.3921e-03,\n",
              "          -1.2471e-02,  1.0059e-01, -6.0109e-02,  6.8498e-02, -2.9810e-02,\n",
              "          -1.9926e-02,  1.2297e-01,  9.4411e-02,  8.8839e-02,  5.0579e-02,\n",
              "           3.9088e-02,  1.2157e-01,  3.5503e-02, -2.0781e-02,  1.1211e-01,\n",
              "           8.3556e-02,  9.7200e-02,  4.0909e-02, -2.6237e-02,  1.2487e-01,\n",
              "           6.0143e-02,  9.7266e-02,  8.8423e-02, -5.5130e-02,  8.3409e-02,\n",
              "          -4.2760e-02,  1.2300e-01,  8.4009e-02,  3.8163e-02,  9.3726e-02,\n",
              "          -1.0907e-01, -7.8558e-02,  1.2493e-01, -5.4819e-03,  7.9909e-04,\n",
              "           3.5783e-02, -3.0800e-02, -7.5577e-02, -5.3873e-02,  8.5018e-03,\n",
              "          -6.6888e-02,  7.4042e-02,  4.9915e-02,  1.0714e-01,  1.2463e-01,\n",
              "           5.2728e-02,  6.0867e-02,  7.1088e-02,  1.1589e-01],\n",
              "         [ 1.0039e-01, -1.0255e-01,  8.6829e-02,  1.1163e-01, -4.4108e-02,\n",
              "           1.7321e-03,  1.1109e-01, -2.0476e-02,  1.0886e-01,  9.6618e-02,\n",
              "           1.2096e-01,  8.2376e-02,  1.3900e-02, -9.9061e-02, -8.6822e-02,\n",
              "          -8.5297e-02,  1.5125e-02, -7.2542e-02,  9.3473e-02, -8.9634e-02,\n",
              "          -9.8219e-02,  8.5472e-02, -1.1961e-02, -4.5707e-02,  4.7622e-02,\n",
              "          -5.3905e-02, -4.7224e-02, -9.4186e-02,  6.3135e-02,  8.3943e-02,\n",
              "          -8.3140e-02, -2.4208e-02,  4.1079e-02,  1.1843e-01,  5.8403e-02,\n",
              "           8.3948e-02, -2.0213e-02,  9.4859e-02, -6.0454e-02,  1.0226e-01,\n",
              "           4.3157e-02,  6.8035e-02, -7.6295e-02,  9.8753e-02,  8.5050e-02,\n",
              "           7.5142e-02, -2.6885e-02, -7.2023e-03,  5.1485e-03,  1.0574e-02,\n",
              "           9.6927e-03, -7.3262e-02,  7.5271e-02, -7.7299e-02,  4.6102e-02,\n",
              "          -2.6137e-02,  4.0663e-02, -6.9265e-02,  2.3233e-02,  1.1356e-01,\n",
              "          -1.0823e-01,  9.1535e-02,  6.9271e-02, -5.1108e-02]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0625, -0.0529, -0.0644,  0.1147,  0.0639, -0.0065, -0.1104,  0.0899,\n",
              "          0.1170, -0.0568], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "y--DKCny8Rd9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    # 배치 당 loss 값을 담을 리스트 생성\n",
        "    batch_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # 옵티마이저의 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # y pred 값 산출\n",
        "        output = model(data)\n",
        "        # loss 계산\n",
        "        # 정답 데이터와의 cross entropy loss 계산\n",
        "        # 이 loss를 배치 당 loss로 보관\n",
        "        loss = criterion(output, target)\n",
        "        batch_losses.append(loss)\n",
        "\n",
        "        # 기울기 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트!\n",
        "        optimizer.step()\n",
        "        \n",
        "    # 배치당 평균 loss 계산\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    \n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "-SB3MSO88Sjt"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    batch_losses = []\n",
        "    correct = 0 \n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            # 예측값 생성\n",
        "            output = model(data)\n",
        "\n",
        "            # loss 계산 (이전과 동일)\n",
        "            loss = criterion(output, target)\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "           # Accuracy 계산\n",
        "           # y pred와 y가 일치하면 correct에 1을 더해주기\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # 배치 당 평균 loss 계산 \n",
        "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    #정확도 계산\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "KfePFm8l8T6a"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocRwc3Fh8U30",
        "outputId": "560315d5-38df-46c8-d97f-20575a697b92"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.8140\tTest Loss: 0.3978\tAccuracy: 86.39%\n",
            "[2] Train Loss: 0.2286\tTest Loss: 0.2098\tAccuracy: 93.69%\n",
            "[3] Train Loss: 0.1585\tTest Loss: 0.1681\tAccuracy: 95.02%\n",
            "[4] Train Loss: 0.1211\tTest Loss: 0.1157\tAccuracy: 96.35%\n",
            "[5] Train Loss: 0.0971\tTest Loss: 0.4150\tAccuracy: 87.28%\n",
            "[6] Train Loss: 0.1816\tTest Loss: 0.1285\tAccuracy: 95.67%\n",
            "[7] Train Loss: 0.0796\tTest Loss: 0.0882\tAccuracy: 97.17%\n",
            "[8] Train Loss: 0.0679\tTest Loss: 0.0796\tAccuracy: 97.44%\n",
            "[9] Train Loss: 0.0576\tTest Loss: 0.1063\tAccuracy: 96.81%\n",
            "[10] Train Loss: 0.0498\tTest Loss: 0.0886\tAccuracy: 97.29%\n",
            "[11] Train Loss: 0.0681\tTest Loss: 0.0809\tAccuracy: 97.53%\n",
            "[12] Train Loss: 0.0432\tTest Loss: 0.0884\tAccuracy: 97.40%\n",
            "[13] Train Loss: 0.0348\tTest Loss: 0.0980\tAccuracy: 97.15%\n",
            "[14] Train Loss: 0.0300\tTest Loss: 0.0651\tAccuracy: 97.80%\n",
            "[15] Train Loss: 0.0254\tTest Loss: 0.0758\tAccuracy: 97.68%\n",
            "[16] Train Loss: 0.0231\tTest Loss: 0.0822\tAccuracy: 97.48%\n",
            "[17] Train Loss: 0.0196\tTest Loss: 0.0692\tAccuracy: 97.80%\n",
            "[18] Train Loss: 0.0177\tTest Loss: 0.1686\tAccuracy: 95.37%\n",
            "[19] Train Loss: 0.1319\tTest Loss: 0.0762\tAccuracy: 97.68%\n",
            "[20] Train Loss: 0.0277\tTest Loss: 0.0678\tAccuracy: 97.92%\n",
            "[21] Train Loss: 0.0205\tTest Loss: 0.0629\tAccuracy: 98.14%\n",
            "[22] Train Loss: 0.0170\tTest Loss: 0.0634\tAccuracy: 98.13%\n",
            "[23] Train Loss: 0.0144\tTest Loss: 0.1173\tAccuracy: 96.73%\n",
            "[24] Train Loss: 0.0145\tTest Loss: 0.0677\tAccuracy: 98.05%\n",
            "[25] Train Loss: 0.0109\tTest Loss: 0.0693\tAccuracy: 98.18%\n",
            "[26] Train Loss: 0.0093\tTest Loss: 0.1010\tAccuracy: 97.12%\n",
            "[27] Train Loss: 0.0085\tTest Loss: 0.0639\tAccuracy: 98.19%\n",
            "[28] Train Loss: 0.0068\tTest Loss: 0.0710\tAccuracy: 98.07%\n",
            "[29] Train Loss: 0.0061\tTest Loss: 0.0669\tAccuracy: 98.15%\n",
            "[30] Train Loss: 0.0052\tTest Loss: 0.0704\tAccuracy: 98.20%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6b82DZG6W3j"
      },
      "source": [
        "# Assignment 4 구현은 여기서 ()\n",
        "# 4.1 학습률 0.05\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "#충분히 좋은 결과를 내고 있기는 하나 학습률을 조금 낮춰서 천천히 학습되게 한 번 시도"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.2 활성화 함수 시그모이드로 변경. 학습률은 원래대로 0.5\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.sigmoid(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "qG46KdhiAep8"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRN9NUB8A5Hg",
        "outputId": "95541ee4-86b2-4d49-b591-29e7bfe512c8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.3 배치 크기 256\n",
        "BATCH_SIZE = 256\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "Vh_wFni3B-xA"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.4 optimizer Adam\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.5)\n",
        "#충분히 좋은 결과를 내고 있기는 하나 학습률을 조금 낮춰서 천천히 학습되게 한 번 시도"
      ],
      "metadata": {
        "id": "Tfd-G2HiEvMU"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.5 hidden size 256,128\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,256)\n",
        "        self.layer2 = nn.Linear(256,128)\n",
        "        self.layer3 = nn.Linear(128,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "aFXqnEPdG7R5"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux7mPf6E78d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "70bf6076-2037-4579-cb38-f2ff870295ad"
      },
      "source": [
        "'''\n",
        "epoch는 30으로 줄이기(충분히 좋은 결과를 내고 있음) 97.61%\n",
        "1.학습률을 조금 낮춰서 천천히 학습되게 한 번 시도 lr=0.05 accuracy = 96.54%\n",
        "2.활성화 함수 시그모이드로 변경해서 학습시켜보기 lr = 0.5 accuracy = 96.17%\n",
        "3.배치 사이즈 256으로 조절 accuracy = 98.13%\n",
        "4.optimizer Adam으로 변경 accuracy = 10.30% 매우 안 좋은 성능. 이상하게 SGD optimizer가 더 좋은 성능\n",
        "5.hidden size 258,126으로 변경 accuracy = 98.20%\n",
        "'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nepoch는 30으로 줄이기(충분히 좋은 결과를 내고 있음)\\n1.학습률을 조금 낮춰서 천천히 학습되게 한 번 시도 lr=0.05 accuracy = 96.54%\\n2.활성화 함수 시그모이드로 변경해서 학습시켜보기 lr = 0.5 accuracy = 96.17%\\n3.배치 사이즈 256으로 조절 accuracy = 98.13%\\n4.optimizer Adam으로 변경 accuracy = 10.30% 매우 안 좋은 성능\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}