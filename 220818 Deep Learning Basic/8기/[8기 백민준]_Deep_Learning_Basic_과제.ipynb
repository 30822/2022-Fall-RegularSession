{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[8기 백민준]_Deep_Learning_Basic_과제.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","source":["import numpy as np\n","from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"ZVh2pam0cDFV","executionInfo":{"status":"ok","timestamp":1661146651783,"user_tz":-540,"elapsed":3527,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz","executionInfo":{"status":"ok","timestamp":1661147613288,"user_tz":-540,"elapsed":4,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"source":["def relu(x):\n","  x = np.maximum(0, x)\n","  return x"],"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","  if x > 0:\n","    x = 1\n","  else:\n","    x = 0\n","  return x"],"metadata":{"id":"Esm4jmTVijro","executionInfo":{"status":"ok","timestamp":1661147868002,"user_tz":-540,"elapsed":2,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 확인\n","print(relu(3))\n","print(relu(-3))\n","print(d_relu(3))\n","print(d_relu(-3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"044KxUCGIPrt","executionInfo":{"status":"ok","timestamp":1661148037569,"user_tz":-540,"elapsed":437,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"d3e45cfb-2a82-4e00-e95e-77719fcfaf26"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","0\n","1\n","0\n"]}]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs","executionInfo":{"status":"ok","timestamp":1661152780956,"user_tz":-540,"elapsed":408,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"source":["def backward_pass(x, y_true, params):\n","\n","  dS3 = params[\"A3\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_relu(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_relu(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"execution_count":48,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","source":["from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"aYMJi8AoPwhz","executionInfo":{"status":"ok","timestamp":1661149573091,"user_tz":-540,"elapsed":324,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# 이미지를 텐서로 변경\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"6PI2vub8PyK9","executionInfo":{"status":"ok","timestamp":1661149573437,"user_tz":-540,"elapsed":6,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["trainset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = True,\n","    download  = True,\n","    transform = transform\n",")\n","testset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = False,\n","    download  = True,\n","    transform = transform\n",")"],"metadata":{"id":"b_AedUXiPzEL","executionInfo":{"status":"ok","timestamp":1661149573438,"user_tz":-540,"elapsed":6,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 512\n","# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n","# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n","train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"r-L0KlKDP1Xt","executionInfo":{"status":"ok","timestamp":1661149573438,"user_tz":-540,"elapsed":5,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"0Bq1HKFOP4Js","executionInfo":{"status":"ok","timestamp":1661149573438,"user_tz":-540,"elapsed":5,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"id":"m6TQ_Z6LP6OG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661149573810,"user_tz":-540,"elapsed":376,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"63080416-c675-4449-844f-890a92c1839a"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n","                         # require_true 얘는 학습되는 애구나 알 수 있음"],"metadata":{"id":"Usu7zwKcP8h1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661149573810,"user_tz":-540,"elapsed":5,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"839ce499-10b6-4118-fe14-d1e781f88e48"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[-0.0093, -0.0057,  0.0309,  ..., -0.0231,  0.0118,  0.0155],\n","         [ 0.0173,  0.0223,  0.0060,  ...,  0.0044,  0.0307,  0.0142],\n","         [ 0.0312,  0.0067,  0.0244,  ...,  0.0203, -0.0123, -0.0250],\n","         ...,\n","         [ 0.0270, -0.0056,  0.0139,  ...,  0.0078,  0.0293, -0.0171],\n","         [-0.0240, -0.0301,  0.0191,  ..., -0.0102,  0.0320, -0.0180],\n","         [ 0.0233, -0.0222,  0.0061,  ...,  0.0218,  0.0128,  0.0099]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-0.0056, -0.0282,  0.0297,  0.0225, -0.0174,  0.0254, -0.0090,  0.0216,\n","          0.0345, -0.0172,  0.0190,  0.0134, -0.0248, -0.0110,  0.0100,  0.0300,\n","          0.0127,  0.0285,  0.0185, -0.0028,  0.0283,  0.0073,  0.0332, -0.0248,\n","          0.0231,  0.0242,  0.0039,  0.0252, -0.0035, -0.0294,  0.0150, -0.0260,\n","         -0.0169, -0.0276, -0.0033,  0.0061,  0.0031,  0.0334, -0.0071, -0.0322,\n","          0.0084,  0.0234,  0.0310, -0.0108, -0.0204,  0.0154, -0.0306,  0.0254,\n","          0.0024,  0.0191,  0.0310,  0.0272,  0.0237,  0.0197, -0.0077, -0.0353,\n","          0.0182,  0.0232,  0.0347, -0.0110,  0.0332, -0.0350,  0.0077, -0.0108,\n","          0.0098, -0.0259,  0.0275, -0.0067, -0.0081,  0.0075,  0.0265,  0.0343,\n","         -0.0001, -0.0022,  0.0136, -0.0070,  0.0266, -0.0260,  0.0282, -0.0132,\n","         -0.0056, -0.0328,  0.0096,  0.0113, -0.0333,  0.0180,  0.0225, -0.0350,\n","          0.0319,  0.0007,  0.0319, -0.0256,  0.0266, -0.0324, -0.0041, -0.0227,\n","          0.0114,  0.0183,  0.0253,  0.0123,  0.0215, -0.0239, -0.0189,  0.0330,\n","         -0.0296, -0.0195, -0.0013, -0.0138, -0.0070, -0.0331,  0.0081,  0.0233,\n","          0.0281,  0.0204,  0.0272, -0.0054,  0.0141, -0.0103, -0.0308,  0.0250,\n","         -0.0290, -0.0016, -0.0007, -0.0196, -0.0158, -0.0051, -0.0355, -0.0083],\n","        requires_grad=True), Parameter containing:\n"," tensor([[ 0.0393, -0.0004,  0.0078,  ..., -0.0669, -0.0644,  0.0632],\n","         [-0.0843, -0.0007, -0.0776,  ..., -0.0843,  0.0160, -0.0748],\n","         [-0.0239,  0.0447,  0.0161,  ...,  0.0075,  0.0481,  0.0458],\n","         ...,\n","         [-0.0795, -0.0625,  0.0608,  ..., -0.0762, -0.0405, -0.0849],\n","         [-0.0676, -0.0649, -0.0055,  ..., -0.0393, -0.0339, -0.0370],\n","         [ 0.0420, -0.0156, -0.0424,  ..., -0.0027,  0.0063,  0.0022]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-0.0490, -0.0398, -0.0847, -0.0659,  0.0798,  0.0371,  0.0616,  0.0722,\n","          0.0776, -0.0707,  0.0109, -0.0280,  0.0376, -0.0066,  0.0547, -0.0630,\n","          0.0411,  0.0380, -0.0219,  0.0240, -0.0217,  0.0484,  0.0558, -0.0226,\n","         -0.0725,  0.0429,  0.0656, -0.0844,  0.0846, -0.0374,  0.0483, -0.0758,\n","          0.0329, -0.0845,  0.0324, -0.0467, -0.0593,  0.0858, -0.0591,  0.0385,\n","          0.0660, -0.0244, -0.0621,  0.0269,  0.0204,  0.0079,  0.0238,  0.0410,\n","         -0.0330,  0.0316, -0.0155, -0.0061,  0.0060,  0.0511, -0.0069, -0.0407,\n","          0.0730, -0.0266,  0.0461,  0.0186, -0.0195, -0.0052, -0.0245, -0.0083],\n","        requires_grad=True), Parameter containing:\n"," tensor([[-3.6415e-04,  9.7529e-02,  9.9933e-02,  9.9918e-03,  9.4772e-02,\n","          -1.2329e-01,  9.6278e-02,  9.0945e-02,  1.0553e-01,  1.8499e-02,\n","           6.8116e-02,  7.9403e-03,  7.9243e-02, -1.9885e-02, -8.6594e-02,\n","          -9.1881e-02,  7.3827e-02,  1.0209e-02, -1.1594e-01, -1.4773e-02,\n","          -8.0350e-02,  3.0719e-02,  1.0888e-03,  4.6119e-04,  1.0382e-01,\n","           1.2103e-01, -7.7721e-02, -1.1073e-01, -2.8841e-02, -1.8559e-02,\n","           2.9837e-02,  9.9461e-02,  3.0722e-02, -5.2940e-02, -3.5693e-02,\n","           7.0102e-03, -2.9194e-02, -1.0949e-01, -3.2981e-02, -3.4300e-02,\n","          -6.6332e-02,  4.4919e-02, -1.2248e-01, -8.2677e-02,  1.7790e-02,\n","          -4.8419e-03,  1.1608e-01,  4.7867e-02,  8.0631e-03, -3.0003e-02,\n","          -4.4813e-02,  5.6744e-02, -1.0772e-02, -1.9691e-02,  9.7880e-02,\n","           2.1244e-02,  3.9196e-02,  1.2343e-01,  4.5623e-03, -9.2727e-02,\n","          -4.1059e-03, -3.2377e-02,  3.2288e-02,  9.8378e-02],\n","         [-1.0467e-01,  3.8812e-02,  9.6001e-02, -5.3385e-03,  5.6695e-02,\n","           5.4374e-02, -2.4044e-02, -1.2383e-01, -1.6031e-02, -3.1799e-03,\n","          -2.6192e-02, -3.4750e-02,  1.0530e-01, -6.1972e-03,  7.9148e-02,\n","          -3.0892e-02,  1.1007e-01,  8.8792e-02, -9.7454e-03, -1.2016e-01,\n","           5.2982e-02,  8.2512e-02, -9.0895e-02,  1.1030e-01, -6.8092e-02,\n","           9.5235e-02, -9.6227e-03, -1.2946e-02, -8.9639e-02, -1.3702e-02,\n","           5.7041e-02,  6.4679e-03, -9.6789e-02,  8.2857e-02, -8.3542e-02,\n","          -9.2079e-02, -8.4315e-02,  1.0751e-01, -1.2410e-01,  8.0609e-02,\n","          -8.9985e-03, -2.6928e-02,  8.6898e-02,  2.5098e-02, -3.7844e-02,\n","           3.8785e-02, -1.3979e-02,  9.6576e-02,  3.0804e-02,  9.2964e-02,\n","           1.0472e-01,  2.0915e-02, -7.9762e-02, -3.9986e-02, -1.1358e-01,\n","          -4.3544e-02,  3.5720e-02, -4.2054e-02,  5.5715e-02,  4.0048e-02,\n","          -6.5847e-02, -6.4846e-02, -1.0698e-01, -1.1459e-01],\n","         [ 4.2723e-02,  5.8563e-03,  3.5843e-02, -1.0824e-01, -1.2207e-01,\n","          -1.2359e-02, -1.0473e-01, -6.4825e-03, -1.5046e-02, -4.7071e-02,\n","           6.0988e-02,  4.8399e-02,  2.1171e-02,  7.2479e-02,  5.8696e-02,\n","           9.7368e-02,  5.2099e-02,  8.7586e-02, -1.2116e-01,  2.6776e-02,\n","          -8.5756e-02,  1.6244e-02, -4.6170e-02, -8.6549e-02,  4.1181e-02,\n","          -1.1877e-01,  1.2015e-01, -2.6376e-02, -4.7813e-02,  7.3805e-02,\n","           4.4262e-02, -9.9464e-02,  6.7683e-02, -3.9457e-02, -5.2351e-02,\n","          -1.5333e-03,  3.9338e-02,  5.4383e-02, -6.9962e-02,  9.4077e-02,\n","           5.3410e-02,  2.9661e-02,  2.4407e-02, -8.5532e-02, -1.1963e-01,\n","          -7.0163e-02,  1.8129e-02, -6.1926e-02, -2.2341e-02,  1.2185e-01,\n","           1.2330e-01, -2.6610e-02,  3.2410e-02, -7.6670e-02,  6.7188e-02,\n","          -7.7970e-02,  5.9431e-02,  1.7391e-02,  6.9591e-02,  1.0226e-01,\n","          -1.1700e-01, -9.2767e-02,  1.7416e-03,  1.7048e-02],\n","         [-1.1445e-01, -1.7201e-02,  2.8971e-02, -5.6427e-02,  9.6735e-02,\n","          -1.0447e-01, -1.0736e-01,  9.2518e-02, -5.9454e-02,  8.4617e-02,\n","          -1.0541e-01,  2.2058e-02, -1.8914e-02, -1.0924e-01,  3.4741e-02,\n","          -4.4916e-02, -3.1704e-02, -2.3855e-02,  6.2908e-02,  9.5026e-02,\n","          -9.4441e-03, -2.1623e-02, -2.5836e-02, -9.2734e-02, -6.2500e-02,\n","          -5.3567e-02, -7.1471e-02, -3.5006e-02,  7.8627e-02,  1.3955e-02,\n","          -7.4017e-04, -7.4494e-04, -8.1336e-02, -9.4000e-02,  9.0817e-02,\n","          -5.4222e-02,  7.3570e-02, -8.0928e-02,  1.7199e-02, -1.1236e-01,\n","           9.9965e-02, -9.5546e-02, -1.3651e-02, -6.1275e-02,  3.5044e-02,\n","          -7.4998e-02,  1.0337e-01,  4.8708e-02, -2.1605e-02, -6.3264e-02,\n","           1.2118e-01,  1.0171e-01,  9.1900e-02, -6.6337e-02,  9.3543e-02,\n","           1.0654e-01,  8.9544e-02,  6.5538e-02,  1.1408e-01,  8.0983e-02,\n","           1.1137e-01, -3.7279e-02,  1.2055e-01, -4.9775e-02],\n","         [-3.4768e-02,  5.7675e-02,  1.1997e-01,  6.5812e-02,  1.2115e-02,\n","          -8.0572e-02, -7.6466e-02, -1.2037e-01, -5.1988e-02, -9.4156e-02,\n","           1.0581e-01, -1.0444e-01, -3.7798e-02, -7.6351e-02,  2.8327e-02,\n","          -3.8896e-02,  7.4103e-05,  1.1171e-01,  1.7032e-02,  8.8931e-02,\n","           5.2929e-02,  9.9169e-02, -1.9272e-02, -9.1002e-02, -6.1118e-02,\n","           5.3933e-02, -1.6998e-02, -1.2468e-01, -2.0484e-02, -1.9455e-02,\n","           1.1682e-01, -5.2627e-03,  5.7434e-02,  5.4782e-02,  1.1680e-01,\n","           1.0077e-02, -2.0404e-02,  8.5877e-02,  2.8376e-02,  1.2395e-01,\n","           1.1426e-01, -2.0441e-02, -1.5576e-02, -8.1804e-02, -3.0372e-04,\n","          -1.1464e-01, -6.1004e-02,  2.4499e-02, -4.5808e-02,  1.9928e-02,\n","           7.1943e-02,  4.9597e-03, -1.8622e-02,  3.3611e-02,  6.4540e-02,\n","           7.2062e-03,  1.0282e-01,  1.1036e-01, -1.2348e-01, -8.3567e-02,\n","          -4.2409e-02,  9.1004e-02,  8.6903e-02, -9.7087e-02],\n","         [ 7.6820e-02, -5.9765e-02, -7.6597e-02,  7.4094e-02, -6.8034e-02,\n","           3.7153e-02,  3.6510e-02,  7.7342e-02,  3.0456e-02,  1.8814e-02,\n","           5.1803e-03,  3.0966e-02,  6.0602e-02, -9.3213e-02, -4.7058e-02,\n","          -5.3547e-02, -8.3746e-02,  3.5992e-03, -4.4240e-02, -1.5776e-02,\n","           1.0293e-01,  1.1705e-02,  9.3017e-02,  6.1632e-02,  4.7989e-03,\n","          -3.8087e-02,  6.8642e-02,  3.2607e-02, -3.4113e-02,  7.6355e-02,\n","           2.3576e-02,  1.1310e-01, -5.7280e-02,  5.5017e-02,  7.4616e-02,\n","          -8.1170e-02,  2.8414e-02, -7.1976e-02,  1.1803e-01,  2.9072e-02,\n","          -4.7541e-02,  7.5192e-02,  5.9272e-03,  9.8878e-02, -1.1571e-01,\n","          -8.0553e-02,  1.1072e-01, -7.0513e-02, -9.4374e-02,  9.7703e-02,\n","          -5.3509e-02,  2.9248e-02, -2.9964e-03,  4.6920e-02,  2.3983e-02,\n","          -1.5993e-02, -7.6544e-02,  3.1021e-02,  6.2380e-02,  8.5677e-03,\n","           1.1245e-01, -7.7036e-02,  5.8513e-02,  1.0803e-01],\n","         [ 8.1167e-03, -9.3566e-02, -1.2449e-01,  1.0032e-01,  6.6879e-02,\n","           7.0078e-03,  1.2006e-02,  1.1087e-01,  6.5058e-03,  7.2354e-03,\n","          -6.6063e-04,  9.7912e-02,  9.8403e-02,  3.3646e-02,  5.7552e-02,\n","           2.8042e-02, -2.6086e-02, -1.1830e-01,  6.5135e-02, -1.1713e-01,\n","          -1.7995e-02, -5.4641e-02, -9.7967e-03, -3.1780e-02, -1.1529e-01,\n","          -1.0748e-01,  6.5323e-02,  4.6881e-02, -1.0119e-01, -8.4851e-02,\n","          -6.7881e-02,  5.8489e-02, -7.4640e-02, -6.0207e-04,  7.2069e-02,\n","           1.2114e-01,  6.3071e-02,  8.5554e-02, -2.0614e-02, -7.7480e-02,\n","           1.1081e-01,  2.9224e-02,  1.1600e-01, -6.1585e-02,  2.9539e-02,\n","          -7.4797e-02, -9.7595e-02, -1.1602e-01,  1.0332e-01,  5.5964e-02,\n","          -1.0450e-01,  2.3444e-02, -1.6039e-02, -7.3305e-02,  8.4583e-02,\n","           1.0397e-01, -3.1926e-03, -1.0633e-01, -5.3833e-02,  6.6673e-02,\n","          -3.9554e-02,  8.8246e-02, -1.2468e-01, -8.4766e-02],\n","         [ 6.3638e-02,  3.1663e-02, -6.6820e-02, -8.1952e-02, -7.5905e-02,\n","          -9.8472e-02, -5.6476e-02, -2.9189e-02, -6.5967e-04, -9.8760e-02,\n","          -9.4540e-02,  1.1758e-01,  1.0632e-01, -9.3199e-02, -1.8729e-02,\n","           2.4005e-02,  7.7467e-02, -1.0228e-01,  7.7273e-02,  8.0886e-04,\n","           3.4642e-03, -4.3242e-02, -6.5145e-02,  1.1551e-01,  4.3960e-02,\n","          -4.9486e-02, -9.7973e-02, -2.3664e-02,  4.0837e-02,  1.4558e-02,\n","          -6.8724e-03,  4.1495e-02,  1.3099e-02,  1.8307e-02,  3.5751e-02,\n","          -2.8506e-02,  1.0284e-01, -8.2555e-02,  1.2255e-01, -1.1339e-01,\n","          -1.1685e-01,  1.2226e-01, -1.2012e-01,  2.0291e-02, -7.7937e-02,\n","           1.2344e-01,  6.5510e-02,  8.3476e-02,  5.1286e-02,  6.6260e-02,\n","          -6.0995e-03, -1.0780e-02, -1.2815e-02,  5.1637e-02,  9.5788e-02,\n","          -1.0053e-01,  1.0192e-02, -8.4619e-02, -1.1857e-01, -1.1047e-01,\n","           1.0268e-01,  1.0030e-01,  1.0825e-02,  3.8748e-02],\n","         [-8.1436e-02, -6.4721e-02,  1.2234e-01,  9.9019e-02, -4.9043e-02,\n","          -3.2488e-02,  3.3894e-02, -1.6814e-02,  9.9121e-02, -1.1424e-01,\n","           7.8296e-02, -1.1742e-01,  8.1507e-02,  7.9816e-02,  7.4207e-02,\n","           7.7194e-02, -5.5498e-02, -7.4881e-02, -1.1605e-01, -4.9768e-02,\n","           2.0209e-02,  6.5817e-02,  5.5474e-02,  3.1746e-02, -3.8292e-02,\n","           4.1028e-02, -1.0992e-01,  4.0131e-02,  3.5391e-02, -6.6211e-02,\n","           9.2990e-02, -1.2368e-02, -2.5825e-02, -1.1947e-01,  6.0540e-02,\n","          -8.3421e-02,  5.6944e-02,  1.0163e-01, -9.7763e-02,  2.8146e-02,\n","          -5.6412e-03,  7.4954e-02, -1.2202e-01,  2.6299e-03, -4.8492e-02,\n","          -7.0542e-02, -2.6979e-02, -4.2014e-02,  6.3492e-02, -6.8934e-02,\n","           2.0596e-03,  9.4148e-02,  5.4922e-02, -2.9760e-02,  9.6378e-02,\n","           6.7165e-02,  7.8293e-02, -5.7407e-02,  1.1718e-02, -2.5271e-02,\n","           1.0523e-01,  5.6252e-02, -2.4324e-02,  6.7933e-02],\n","         [ 6.6220e-02, -6.1697e-03,  7.1133e-02,  9.3255e-02,  2.3285e-02,\n","          -5.4780e-02,  3.9975e-03, -7.4691e-02, -1.0809e-02,  4.7928e-02,\n","           1.2281e-02, -9.1967e-02,  7.8783e-02, -6.1849e-02,  1.1769e-01,\n","           9.0743e-02, -8.0207e-02,  3.8736e-02,  6.1812e-02, -6.1728e-02,\n","           1.2059e-01,  4.1781e-02, -6.1678e-03, -7.0752e-02,  7.1100e-02,\n","           7.0653e-02,  8.2120e-02,  5.8131e-02,  2.3854e-02, -5.4413e-02,\n","           1.1020e-01,  8.2340e-02,  1.1001e-02,  5.8232e-02, -3.6833e-02,\n","           1.8787e-04,  4.6077e-02, -5.4463e-02,  6.1169e-02, -1.0558e-02,\n","          -5.4356e-03,  9.0509e-02,  4.2424e-02, -1.6360e-02,  4.2838e-03,\n","          -6.1247e-02, -1.0565e-01, -3.4677e-02,  5.7504e-02,  6.9541e-02,\n","          -8.3290e-02,  1.0318e-01,  6.9563e-02, -9.5523e-02, -1.1806e-02,\n","          -8.3144e-02, -7.7656e-02,  6.0314e-02, -8.7276e-02,  4.6986e-02,\n","          -2.9587e-02,  8.2811e-02,  5.5227e-02, -3.0748e-02]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-0.0991,  0.0602,  0.0114,  0.0106, -0.0653,  0.0491,  0.0651, -0.0262,\n","          0.0246, -0.0232], requires_grad=True)]"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"3L-Aqf7en2_R","executionInfo":{"status":"ok","timestamp":1661149573810,"user_tz":-540,"elapsed":3,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"id":"ydaHoi-HP9t9","executionInfo":{"status":"ok","timestamp":1661149573810,"user_tz":-540,"elapsed":2,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"gOnDlc1PP_kt","executionInfo":{"status":"ok","timestamp":1661149573811,"user_tz":-540,"elapsed":3,"user":{"displayName":"백민준","userId":"09942822155215613800"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"id":"fVfEwSzWQEUM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661150263379,"user_tz":-540,"elapsed":689571,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"0e448b58-85ae-40d1-8774-64959f2d22df"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.8366\tTest Loss: 0.3117\tAccuracy: 90.36%\n","[2] Train Loss: 0.2432\tTest Loss: 0.2620\tAccuracy: 91.71%\n","[3] Train Loss: 0.1716\tTest Loss: 0.1963\tAccuracy: 93.74%\n","[4] Train Loss: 0.1311\tTest Loss: 0.1516\tAccuracy: 95.44%\n","[5] Train Loss: 0.2099\tTest Loss: 0.1535\tAccuracy: 95.20%\n","[6] Train Loss: 0.1121\tTest Loss: 0.2499\tAccuracy: 92.00%\n","[7] Train Loss: 0.1100\tTest Loss: 0.1855\tAccuracy: 94.51%\n","[8] Train Loss: 0.0837\tTest Loss: 0.1163\tAccuracy: 96.24%\n","[9] Train Loss: 0.0677\tTest Loss: 0.1187\tAccuracy: 96.29%\n","[10] Train Loss: 0.0711\tTest Loss: 0.0957\tAccuracy: 96.94%\n","[11] Train Loss: 0.0520\tTest Loss: 0.0857\tAccuracy: 97.36%\n","[12] Train Loss: 0.0467\tTest Loss: 0.0873\tAccuracy: 97.24%\n","[13] Train Loss: 0.0412\tTest Loss: 0.0733\tAccuracy: 97.79%\n","[14] Train Loss: 0.0381\tTest Loss: 0.1079\tAccuracy: 96.60%\n","[15] Train Loss: 0.0342\tTest Loss: 0.0724\tAccuracy: 97.76%\n","[16] Train Loss: 0.0298\tTest Loss: 0.0838\tAccuracy: 97.56%\n","[17] Train Loss: 0.0271\tTest Loss: 0.1024\tAccuracy: 97.00%\n","[18] Train Loss: 0.0243\tTest Loss: 0.1093\tAccuracy: 96.72%\n","[19] Train Loss: 0.0229\tTest Loss: 0.0738\tAccuracy: 97.74%\n","[20] Train Loss: 0.0192\tTest Loss: 0.0766\tAccuracy: 97.73%\n","[21] Train Loss: 0.0169\tTest Loss: 0.0703\tAccuracy: 97.90%\n","[22] Train Loss: 0.0153\tTest Loss: 0.2063\tAccuracy: 94.44%\n","[23] Train Loss: 1.6283\tTest Loss: 0.2647\tAccuracy: 92.21%\n","[24] Train Loss: 0.1597\tTest Loss: 0.1421\tAccuracy: 95.52%\n","[25] Train Loss: 0.1235\tTest Loss: 0.2732\tAccuracy: 91.45%\n","[26] Train Loss: 0.0931\tTest Loss: 0.1110\tAccuracy: 96.80%\n","[27] Train Loss: 0.0789\tTest Loss: 0.1062\tAccuracy: 96.76%\n","[28] Train Loss: 0.0688\tTest Loss: 0.1339\tAccuracy: 96.12%\n","[29] Train Loss: 0.0610\tTest Loss: 0.1244\tAccuracy: 96.16%\n","[30] Train Loss: 0.0553\tTest Loss: 0.0918\tAccuracy: 97.28%\n","[31] Train Loss: 0.0501\tTest Loss: 0.2095\tAccuracy: 94.14%\n","[32] Train Loss: 0.3810\tTest Loss: 0.1405\tAccuracy: 95.68%\n","[33] Train Loss: 0.1013\tTest Loss: 0.1376\tAccuracy: 95.93%\n","[34] Train Loss: 0.1032\tTest Loss: 0.2242\tAccuracy: 93.13%\n","[35] Train Loss: 0.0750\tTest Loss: 0.1351\tAccuracy: 96.09%\n","[36] Train Loss: 0.0673\tTest Loss: 0.1220\tAccuracy: 96.65%\n","[37] Train Loss: 0.0599\tTest Loss: 0.9778\tAccuracy: 77.78%\n","[38] Train Loss: 0.1428\tTest Loss: 0.1555\tAccuracy: 95.40%\n","[39] Train Loss: 0.0622\tTest Loss: 0.3248\tAccuracy: 91.37%\n","[40] Train Loss: 0.1420\tTest Loss: 0.1016\tAccuracy: 97.03%\n","[41] Train Loss: 0.0533\tTest Loss: 0.1480\tAccuracy: 95.90%\n","[42] Train Loss: 0.0459\tTest Loss: 0.1041\tAccuracy: 97.16%\n","[43] Train Loss: 0.0413\tTest Loss: 0.0992\tAccuracy: 97.43%\n","[44] Train Loss: 0.0375\tTest Loss: 0.1290\tAccuracy: 96.53%\n","[45] Train Loss: 0.0352\tTest Loss: 0.1893\tAccuracy: 94.88%\n","[46] Train Loss: 0.0330\tTest Loss: 0.1098\tAccuracy: 97.12%\n","[47] Train Loss: 0.0315\tTest Loss: 0.0982\tAccuracy: 97.51%\n","[48] Train Loss: 0.0280\tTest Loss: 0.1035\tAccuracy: 97.35%\n","[49] Train Loss: 0.0260\tTest Loss: 0.0965\tAccuracy: 97.51%\n","[50] Train Loss: 0.0242\tTest Loss: 0.0983\tAccuracy: 97.48%\n","[51] Train Loss: 0.0233\tTest Loss: 0.1115\tAccuracy: 97.34%\n","[52] Train Loss: 0.0208\tTest Loss: 0.1176\tAccuracy: 97.10%\n","[53] Train Loss: 0.0206\tTest Loss: 0.1035\tAccuracy: 97.59%\n","[54] Train Loss: 0.0189\tTest Loss: 0.1073\tAccuracy: 97.51%\n","[55] Train Loss: 0.0169\tTest Loss: 0.1115\tAccuracy: 97.49%\n","[56] Train Loss: 0.0157\tTest Loss: 0.1053\tAccuracy: 97.57%\n","[57] Train Loss: 0.0145\tTest Loss: 0.1167\tAccuracy: 97.29%\n","[58] Train Loss: 0.0143\tTest Loss: 0.1039\tAccuracy: 97.73%\n","[59] Train Loss: 0.0124\tTest Loss: 0.1097\tAccuracy: 97.50%\n","[60] Train Loss: 0.0124\tTest Loss: 0.1426\tAccuracy: 96.72%\n","[61] Train Loss: 0.0118\tTest Loss: 0.1149\tAccuracy: 97.43%\n","[62] Train Loss: 0.0100\tTest Loss: 0.1073\tAccuracy: 97.64%\n","[63] Train Loss: 0.0093\tTest Loss: 0.1108\tAccuracy: 97.53%\n","[64] Train Loss: 0.0088\tTest Loss: 0.1204\tAccuracy: 97.46%\n","[65] Train Loss: 0.0083\tTest Loss: 0.1114\tAccuracy: 97.61%\n","[66] Train Loss: 0.0073\tTest Loss: 0.1126\tAccuracy: 97.47%\n","[67] Train Loss: 0.0071\tTest Loss: 0.1163\tAccuracy: 97.49%\n","[68] Train Loss: 0.0062\tTest Loss: 0.1169\tAccuracy: 97.67%\n","[69] Train Loss: 0.0058\tTest Loss: 0.1167\tAccuracy: 97.61%\n","[70] Train Loss: 0.0053\tTest Loss: 0.1197\tAccuracy: 97.59%\n","[71] Train Loss: 0.0048\tTest Loss: 0.1206\tAccuracy: 97.69%\n","[72] Train Loss: 0.0048\tTest Loss: 0.1391\tAccuracy: 97.03%\n","[73] Train Loss: 0.0045\tTest Loss: 0.1225\tAccuracy: 97.52%\n","[74] Train Loss: 0.0040\tTest Loss: 0.1199\tAccuracy: 97.62%\n","[75] Train Loss: 0.0037\tTest Loss: 0.1301\tAccuracy: 97.39%\n","[76] Train Loss: 0.0036\tTest Loss: 0.1214\tAccuracy: 97.60%\n","[77] Train Loss: 0.0032\tTest Loss: 0.1223\tAccuracy: 97.64%\n","[78] Train Loss: 0.0030\tTest Loss: 0.1245\tAccuracy: 97.55%\n","[79] Train Loss: 0.0030\tTest Loss: 0.1255\tAccuracy: 97.66%\n","[80] Train Loss: 0.0027\tTest Loss: 0.1278\tAccuracy: 97.55%\n","[81] Train Loss: 0.0025\tTest Loss: 0.1262\tAccuracy: 97.57%\n","[82] Train Loss: 0.0024\tTest Loss: 0.1267\tAccuracy: 97.68%\n","[83] Train Loss: 0.0023\tTest Loss: 0.1274\tAccuracy: 97.66%\n","[84] Train Loss: 0.0022\tTest Loss: 0.1267\tAccuracy: 97.75%\n","[85] Train Loss: 0.0021\tTest Loss: 0.1290\tAccuracy: 97.64%\n","[86] Train Loss: 0.0020\tTest Loss: 0.1269\tAccuracy: 97.63%\n","[87] Train Loss: 0.0020\tTest Loss: 0.1298\tAccuracy: 97.68%\n","[88] Train Loss: 0.0018\tTest Loss: 0.1311\tAccuracy: 97.63%\n","[89] Train Loss: 0.0018\tTest Loss: 0.1327\tAccuracy: 97.60%\n","[90] Train Loss: 0.0017\tTest Loss: 0.1325\tAccuracy: 97.68%\n","[91] Train Loss: 0.0016\tTest Loss: 0.1306\tAccuracy: 97.58%\n","[92] Train Loss: 0.0016\tTest Loss: 0.1335\tAccuracy: 97.62%\n","[93] Train Loss: 0.0015\tTest Loss: 0.1357\tAccuracy: 97.64%\n","[94] Train Loss: 0.0015\tTest Loss: 0.1339\tAccuracy: 97.66%\n","[95] Train Loss: 0.0014\tTest Loss: 0.1345\tAccuracy: 97.67%\n","[96] Train Loss: 0.0014\tTest Loss: 0.1370\tAccuracy: 97.65%\n","[97] Train Loss: 0.0013\tTest Loss: 0.1419\tAccuracy: 97.58%\n","[98] Train Loss: 0.0013\tTest Loss: 0.1361\tAccuracy: 97.58%\n","[99] Train Loss: 0.0012\tTest Loss: 0.1385\tAccuracy: 97.66%\n","[100] Train Loss: 0.0012\tTest Loss: 0.1363\tAccuracy: 97.64%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","\n","        return out"],"metadata":{"executionInfo":{"status":"ok","timestamp":1661150951382,"user_tz":-540,"elapsed":1,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"id":"S3sf9fM7Tc7r"},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661150952941,"user_tz":-540,"elapsed":4,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"aca2087a-428e-4372-c717-299816b7c47a","id":"Bg7hVwf4Tc7r"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.05)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1661150962881,"user_tz":-540,"elapsed":425,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"id":"6frxW4hOTc7s"},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"executionInfo":{"status":"ok","timestamp":1661150964263,"user_tz":-540,"elapsed":2,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"id":"kY0vLTbuTc7s"},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"executionInfo":{"status":"ok","timestamp":1661150966582,"user_tz":-540,"elapsed":335,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"id":"DlSk7cVDTc7s"},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 50\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661151418720,"user_tz":-540,"elapsed":341536,"user":{"displayName":"백민준","userId":"09942822155215613800"}},"outputId":"ed3826fd-6bd8-4846-bb75-29330efc37c2","id":"pHsxeT8vTc7s"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.2699\tTest Loss: 0.2592\tAccuracy: 92.61%\n","[2] Train Loss: 0.2635\tTest Loss: 0.2529\tAccuracy: 92.77%\n","[3] Train Loss: 0.2572\tTest Loss: 0.2493\tAccuracy: 92.89%\n","[4] Train Loss: 0.2514\tTest Loss: 0.2435\tAccuracy: 93.12%\n","[5] Train Loss: 0.2455\tTest Loss: 0.2379\tAccuracy: 93.33%\n","[6] Train Loss: 0.2402\tTest Loss: 0.2321\tAccuracy: 93.44%\n","[7] Train Loss: 0.2352\tTest Loss: 0.2271\tAccuracy: 93.52%\n","[8] Train Loss: 0.2302\tTest Loss: 0.2242\tAccuracy: 93.64%\n","[9] Train Loss: 0.2253\tTest Loss: 0.2208\tAccuracy: 93.84%\n","[10] Train Loss: 0.2206\tTest Loss: 0.2145\tAccuracy: 93.87%\n","[11] Train Loss: 0.2163\tTest Loss: 0.2102\tAccuracy: 93.94%\n","[12] Train Loss: 0.2116\tTest Loss: 0.2081\tAccuracy: 94.05%\n","[13] Train Loss: 0.2078\tTest Loss: 0.2031\tAccuracy: 94.13%\n","[14] Train Loss: 0.2044\tTest Loss: 0.1997\tAccuracy: 94.27%\n","[15] Train Loss: 0.2005\tTest Loss: 0.1961\tAccuracy: 94.28%\n","[16] Train Loss: 0.1973\tTest Loss: 0.1937\tAccuracy: 94.41%\n","[17] Train Loss: 0.1927\tTest Loss: 0.1919\tAccuracy: 94.53%\n","[18] Train Loss: 0.1899\tTest Loss: 0.1885\tAccuracy: 94.60%\n","[19] Train Loss: 0.1861\tTest Loss: 0.1853\tAccuracy: 94.68%\n","[20] Train Loss: 0.1843\tTest Loss: 0.1821\tAccuracy: 94.85%\n","[21] Train Loss: 0.1796\tTest Loss: 0.1771\tAccuracy: 94.92%\n","[22] Train Loss: 0.1778\tTest Loss: 0.1794\tAccuracy: 95.01%\n","[23] Train Loss: 0.1738\tTest Loss: 0.1727\tAccuracy: 95.05%\n","[24] Train Loss: 0.1713\tTest Loss: 0.1700\tAccuracy: 95.09%\n","[25] Train Loss: 0.1686\tTest Loss: 0.1694\tAccuracy: 95.20%\n","[26] Train Loss: 0.1656\tTest Loss: 0.1675\tAccuracy: 95.23%\n","[27] Train Loss: 0.1630\tTest Loss: 0.1649\tAccuracy: 95.27%\n","[28] Train Loss: 0.1606\tTest Loss: 0.1623\tAccuracy: 95.37%\n","[29] Train Loss: 0.1585\tTest Loss: 0.1601\tAccuracy: 95.36%\n","[30] Train Loss: 0.1558\tTest Loss: 0.1590\tAccuracy: 95.35%\n","[31] Train Loss: 0.1546\tTest Loss: 0.1578\tAccuracy: 95.54%\n","[32] Train Loss: 0.1515\tTest Loss: 0.1538\tAccuracy: 95.62%\n","[33] Train Loss: 0.1490\tTest Loss: 0.1532\tAccuracy: 95.58%\n","[34] Train Loss: 0.1479\tTest Loss: 0.1520\tAccuracy: 95.61%\n","[35] Train Loss: 0.1450\tTest Loss: 0.1478\tAccuracy: 95.63%\n","[36] Train Loss: 0.1431\tTest Loss: 0.1476\tAccuracy: 95.75%\n","[37] Train Loss: 0.1418\tTest Loss: 0.1463\tAccuracy: 95.63%\n","[38] Train Loss: 0.1390\tTest Loss: 0.1437\tAccuracy: 95.85%\n","[39] Train Loss: 0.1374\tTest Loss: 0.1409\tAccuracy: 95.86%\n","[40] Train Loss: 0.1361\tTest Loss: 0.1437\tAccuracy: 95.86%\n","[41] Train Loss: 0.1334\tTest Loss: 0.1380\tAccuracy: 95.96%\n","[42] Train Loss: 0.1321\tTest Loss: 0.1400\tAccuracy: 96.04%\n","[43] Train Loss: 0.1295\tTest Loss: 0.1362\tAccuracy: 96.02%\n","[44] Train Loss: 0.1290\tTest Loss: 0.1360\tAccuracy: 96.07%\n","[45] Train Loss: 0.1264\tTest Loss: 0.1328\tAccuracy: 96.15%\n","[46] Train Loss: 0.1251\tTest Loss: 0.1346\tAccuracy: 96.17%\n","[47] Train Loss: 0.1236\tTest Loss: 0.1322\tAccuracy: 96.18%\n","[48] Train Loss: 0.1228\tTest Loss: 0.1307\tAccuracy: 96.29%\n","[49] Train Loss: 0.1204\tTest Loss: 0.1292\tAccuracy: 96.34%\n","[50] Train Loss: 0.1188\tTest Loss: 0.1287\tAccuracy: 96.34%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","source":["과제 3번 모델의 출력 결과를 살펴보자. 훈련세트의 손실함수 값은 작은 값으로 계속 줄어들고 있다. 그러나 테스트세트의 손실함수 값은 일정 구간부터 증가와 감소를 반복하고 있다. 모델의 정확도 역시 13번째 에포크 이후 유의미한 증가추세를 보이고 있지 않다. 이는 모델이 훈련세트에만 과적합되었음을 의미한다.\n","\n","이를 해결하기 위해 다음과 같이 보완하였다.   \n","1) learning rate = 0.05  \n","학습률이 너무 크면 손실함수 값이 오히려 더 커지는 오버슈팅 문제가 발생한다. 조금씩 가중치를 업데이트해가면서 이를 방지하도록 하겠다.    \n","2) epoch = 50: 에포크 횟수를 줄여 과적합 문제를 완화해보겠다.   \n","3) layer 개수 : 2  \n","레이어 개수가 많아질 수록 모델이 더 정교해진다는 장점이 있지만, 과적합되기도 쉬워진다.    \n","레이어 개수를 2개로 줄여 일반화 성능을 높여보겠다. 대신 첫 레이어 노드 수는 128개를 그대로 유지하겠다.\n"],"metadata":{"id":"VndXZczXO2sR"}},{"cell_type":"code","source":[""],"metadata":{"id":"o2yUOzgZbLCr"},"execution_count":null,"outputs":[]}]}