{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "outputs": [],
      "source": [
        "def d_relu(x):\n",
        "    return np.greater(x,0).astype(np.int32)\n",
        "#np.greater=return the truth value of (x1 > x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "outputs": [],
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_relu(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_relu(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bxJO249A3jhk"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Rbpra-Qj2CLe"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n22yLJcu2CLi"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 512\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net1,self).__init__()\n",
        "    self.layer1=nn.Linear(784,128)\n",
        "    self.layer2=nn.Linear(128,64)\n",
        "    self.layer3=nn.Linear(64,10)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "  def forward(self,x):\n",
        "    x=x.view(-1,784)\n",
        "    out=self.layer1(x)\n",
        "    out=self.sigmoid(out)\n",
        "    out=self.layer2(out)\n",
        "    out=self.sigmoid(out)\n",
        "    out=self.layer3(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "we8Jia_Q79Si"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=Net1()\n",
        "criterion=nn.CrossEntropyLoss() #손실함수\n",
        "optimizer=optim.SGD(model1.parameters(),lr=0.5) #optimizer"
      ],
      "metadata": {
        "id": "9ItWaIvz9Mw-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,train_loader,optimizer):\n",
        "    model.train()\n",
        "    batch_losses=[]\n",
        "    for data,target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output=model(data)\n",
        "        loss=criterion(output,target)\n",
        "        batch_losses.append(loss)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss=sum(batch_losses)/len(batch_losses)\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "iR3wHLOC9i7B"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    batch_losses=[]\n",
        "    correct=0\n",
        "    with torch.no_grad():\n",
        "        for data,target in test_loader:\n",
        "            output=model(data)\n",
        "            loss=criterion(output,target)\n",
        "            batch_losses.append(loss)\n",
        "            \n",
        "            pred=output.max(1,keepdim=True)[1]\n",
        "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
        "    \n",
        "    avg_loss=sum(batch_losses)/len(batch_losses)\n",
        "    accuracy=100*correct/len(test_loader.dataset)\n",
        "    return avg_loss,accuracy"
      ],
      "metadata": {
        "id": "PnUawoNy9mKR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=100\n",
        "for epoch in range(1,epochs+1):\n",
        "    train_loss=train(model1, train_loader, optimizer)\n",
        "    test_loss, test_accuracy=evaluate(model1,test_loader)\n",
        "    print('{} train loss:{:.2f}, test loss:{:.2f}, accuracy:{:.2f}'.format(epoch,train_loss,test_loss,test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wD3jvnLB7OT",
        "outputId": "9f440987-7413-480e-a9b6-c1c6af40d54c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 train loss:2.25, test loss:2.07, accuracy:22.09\n",
            "2 train loss:1.38, test loss:0.95, accuracy:66.24\n",
            "3 train loss:0.77, test loss:0.67, accuracy:79.21\n",
            "4 train loss:0.55, test loss:0.49, accuracy:85.31\n",
            "5 train loss:0.44, test loss:0.42, accuracy:87.68\n",
            "6 train loss:0.38, test loss:0.36, accuracy:89.30\n",
            "7 train loss:0.35, test loss:0.33, accuracy:90.52\n",
            "8 train loss:0.33, test loss:0.32, accuracy:91.11\n",
            "9 train loss:0.31, test loss:0.32, accuracy:90.49\n",
            "10 train loss:0.29, test loss:0.28, accuracy:91.78\n",
            "11 train loss:0.27, test loss:0.26, accuracy:91.98\n",
            "12 train loss:0.26, test loss:0.27, accuracy:92.09\n",
            "13 train loss:0.25, test loss:0.24, accuracy:93.11\n",
            "14 train loss:0.23, test loss:0.23, accuracy:93.11\n",
            "15 train loss:0.22, test loss:0.23, accuracy:92.84\n",
            "16 train loss:0.21, test loss:0.21, accuracy:93.66\n",
            "17 train loss:0.20, test loss:0.20, accuracy:94.20\n",
            "18 train loss:0.20, test loss:0.20, accuracy:93.83\n",
            "19 train loss:0.19, test loss:0.19, accuracy:94.42\n",
            "20 train loss:0.18, test loss:0.18, accuracy:94.34\n",
            "21 train loss:0.17, test loss:0.17, accuracy:94.82\n",
            "22 train loss:0.17, test loss:0.18, accuracy:94.79\n",
            "23 train loss:0.16, test loss:0.17, accuracy:94.96\n",
            "24 train loss:0.15, test loss:0.17, accuracy:94.79\n",
            "25 train loss:0.15, test loss:0.16, accuracy:95.01\n",
            "26 train loss:0.14, test loss:0.15, accuracy:95.36\n",
            "27 train loss:0.14, test loss:0.16, accuracy:95.35\n",
            "28 train loss:0.13, test loss:0.15, accuracy:95.29\n",
            "29 train loss:0.13, test loss:0.14, accuracy:95.77\n",
            "30 train loss:0.13, test loss:0.14, accuracy:95.70\n",
            "31 train loss:0.12, test loss:0.13, accuracy:95.86\n",
            "32 train loss:0.12, test loss:0.13, accuracy:96.03\n",
            "33 train loss:0.12, test loss:0.13, accuracy:96.11\n",
            "34 train loss:0.11, test loss:0.16, accuracy:95.12\n",
            "35 train loss:0.11, test loss:0.13, accuracy:95.98\n",
            "36 train loss:0.11, test loss:0.13, accuracy:96.16\n",
            "37 train loss:0.10, test loss:0.12, accuracy:96.16\n",
            "38 train loss:0.10, test loss:0.11, accuracy:96.54\n",
            "39 train loss:0.10, test loss:0.13, accuracy:95.90\n",
            "40 train loss:0.10, test loss:0.12, accuracy:96.37\n",
            "41 train loss:0.09, test loss:0.11, accuracy:96.52\n",
            "42 train loss:0.09, test loss:0.12, accuracy:96.56\n",
            "43 train loss:0.09, test loss:0.11, accuracy:96.58\n",
            "44 train loss:0.09, test loss:0.11, accuracy:96.72\n",
            "45 train loss:0.08, test loss:0.10, accuracy:96.83\n",
            "46 train loss:0.08, test loss:0.11, accuracy:96.50\n",
            "47 train loss:0.08, test loss:0.11, accuracy:96.61\n",
            "48 train loss:0.08, test loss:0.10, accuracy:96.96\n",
            "49 train loss:0.08, test loss:0.10, accuracy:96.76\n",
            "50 train loss:0.08, test loss:0.11, accuracy:96.54\n",
            "51 train loss:0.07, test loss:0.11, accuracy:96.54\n",
            "52 train loss:0.07, test loss:0.11, accuracy:96.68\n",
            "53 train loss:0.07, test loss:0.09, accuracy:96.95\n",
            "54 train loss:0.07, test loss:0.09, accuracy:97.13\n",
            "55 train loss:0.07, test loss:0.09, accuracy:97.19\n",
            "56 train loss:0.07, test loss:0.09, accuracy:97.05\n",
            "57 train loss:0.06, test loss:0.09, accuracy:96.98\n",
            "58 train loss:0.06, test loss:0.11, accuracy:96.55\n",
            "59 train loss:0.06, test loss:0.08, accuracy:97.32\n",
            "60 train loss:0.06, test loss:0.10, accuracy:96.81\n",
            "61 train loss:0.06, test loss:0.09, accuracy:97.27\n",
            "62 train loss:0.06, test loss:0.09, accuracy:97.23\n",
            "63 train loss:0.06, test loss:0.08, accuracy:97.40\n",
            "64 train loss:0.06, test loss:0.08, accuracy:97.39\n",
            "65 train loss:0.05, test loss:0.09, accuracy:97.30\n",
            "66 train loss:0.05, test loss:0.08, accuracy:97.38\n",
            "67 train loss:0.05, test loss:0.08, accuracy:97.43\n",
            "68 train loss:0.05, test loss:0.08, accuracy:97.36\n",
            "69 train loss:0.05, test loss:0.08, accuracy:97.59\n",
            "70 train loss:0.05, test loss:0.08, accuracy:97.33\n",
            "71 train loss:0.05, test loss:0.08, accuracy:97.61\n",
            "72 train loss:0.05, test loss:0.08, accuracy:97.62\n",
            "73 train loss:0.05, test loss:0.08, accuracy:97.57\n",
            "74 train loss:0.04, test loss:0.08, accuracy:97.52\n",
            "75 train loss:0.04, test loss:0.08, accuracy:97.52\n",
            "76 train loss:0.04, test loss:0.10, accuracy:96.90\n",
            "77 train loss:0.04, test loss:0.07, accuracy:97.57\n",
            "78 train loss:0.04, test loss:0.08, accuracy:97.49\n",
            "79 train loss:0.04, test loss:0.08, accuracy:97.64\n",
            "80 train loss:0.04, test loss:0.07, accuracy:97.62\n",
            "81 train loss:0.04, test loss:0.09, accuracy:97.08\n",
            "82 train loss:0.04, test loss:0.08, accuracy:97.47\n",
            "83 train loss:0.04, test loss:0.07, accuracy:97.72\n",
            "84 train loss:0.04, test loss:0.08, accuracy:97.60\n",
            "85 train loss:0.04, test loss:0.10, accuracy:96.76\n",
            "86 train loss:0.04, test loss:0.07, accuracy:97.67\n",
            "87 train loss:0.03, test loss:0.07, accuracy:97.67\n",
            "88 train loss:0.03, test loss:0.08, accuracy:97.28\n",
            "89 train loss:0.03, test loss:0.07, accuracy:97.57\n",
            "90 train loss:0.03, test loss:0.07, accuracy:97.66\n",
            "91 train loss:0.03, test loss:0.07, accuracy:97.68\n",
            "92 train loss:0.03, test loss:0.07, accuracy:97.76\n",
            "93 train loss:0.03, test loss:0.08, accuracy:97.40\n",
            "94 train loss:0.03, test loss:0.08, accuracy:97.47\n",
            "95 train loss:0.03, test loss:0.07, accuracy:97.76\n",
            "96 train loss:0.03, test loss:0.07, accuracy:97.88\n",
            "97 train loss:0.03, test loss:0.07, accuracy:97.80\n",
            "98 train loss:0.03, test loss:0.08, accuracy:97.45\n",
            "99 train loss:0.03, test loss:0.07, accuracy:97.67\n",
            "100 train loss:0.03, test loss:0.07, accuracy:97.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "kDzBkxeQ2CLj"
      },
      "outputs": [],
      "source": [
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2,self).__init__()\n",
        "        self.layer1=nn.Linear(784,128)\n",
        "        self.layer2=nn.Linear(128,64)\n",
        "        self.layer3=nn.Linear(64,10)\n",
        "        self.relu=nn.ReLU()\n",
        "    def forward(self,x):\n",
        "        x=x.view(-1,784)\n",
        "        out=self.layer1(x)\n",
        "        out=self.relu(out)\n",
        "        out=self.layer2(out)\n",
        "        out=self.relu(out)\n",
        "        out=self.layer3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OrSo8kVx2CLl"
      },
      "outputs": [],
      "source": [
        "model2=Net2()\n",
        "criterion=nn.CrossEntropyLoss() #손실함수\n",
        "optimizer=optim.SGD(model2.parameters(),lr=0.5) #optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzW0U3qG2CLr",
        "outputId": "0c08d7d4-e501-489e-90a3-6c339a44cbef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 train loss:0.81, test loss:0.34, accuracy:89.33\n",
            "2 train loss:0.24, test loss:0.23, accuracy:92.79\n",
            "3 train loss:0.17, test loss:0.16, accuracy:94.84\n",
            "4 train loss:0.13, test loss:0.15, accuracy:95.05\n",
            "5 train loss:0.11, test loss:0.12, accuracy:95.97\n",
            "6 train loss:0.09, test loss:0.11, accuracy:96.65\n",
            "7 train loss:0.08, test loss:0.15, accuracy:95.19\n",
            "8 train loss:0.07, test loss:0.11, accuracy:96.39\n",
            "9 train loss:0.06, test loss:0.10, accuracy:97.09\n",
            "10 train loss:0.05, test loss:0.12, accuracy:96.13\n",
            "11 train loss:0.05, test loss:0.52, accuracy:86.61\n",
            "12 train loss:0.46, test loss:0.18, accuracy:94.14\n",
            "13 train loss:0.10, test loss:0.23, accuracy:92.84\n",
            "14 train loss:0.08, test loss:0.17, accuracy:94.55\n",
            "15 train loss:0.08, test loss:0.12, accuracy:96.13\n",
            "16 train loss:0.06, test loss:0.14, accuracy:95.61\n",
            "17 train loss:0.05, test loss:0.11, accuracy:96.43\n",
            "18 train loss:0.05, test loss:0.10, accuracy:97.04\n",
            "19 train loss:0.04, test loss:0.11, accuracy:96.48\n",
            "20 train loss:0.04, test loss:0.28, accuracy:92.35\n",
            "21 train loss:0.13, test loss:0.09, accuracy:97.30\n",
            "22 train loss:0.04, test loss:0.11, accuracy:96.77\n",
            "23 train loss:0.04, test loss:0.07, accuracy:97.67\n",
            "24 train loss:0.03, test loss:0.08, accuracy:97.69\n",
            "25 train loss:0.03, test loss:0.08, accuracy:97.40\n",
            "26 train loss:0.03, test loss:0.07, accuracy:97.78\n",
            "27 train loss:0.02, test loss:0.08, accuracy:97.74\n",
            "28 train loss:0.02, test loss:0.21, accuracy:93.90\n",
            "29 train loss:0.22, test loss:0.10, accuracy:96.95\n",
            "30 train loss:0.05, test loss:0.11, accuracy:96.66\n",
            "31 train loss:0.04, test loss:0.10, accuracy:97.15\n",
            "32 train loss:0.04, test loss:0.15, accuracy:95.84\n",
            "33 train loss:0.14, test loss:0.10, accuracy:97.24\n",
            "34 train loss:0.04, test loss:0.09, accuracy:97.44\n",
            "35 train loss:0.03, test loss:0.09, accuracy:97.27\n",
            "36 train loss:0.03, test loss:0.11, accuracy:96.90\n",
            "37 train loss:0.02, test loss:0.13, accuracy:96.51\n",
            "38 train loss:0.02, test loss:0.09, accuracy:97.48\n",
            "39 train loss:0.02, test loss:0.10, accuracy:97.28\n",
            "40 train loss:0.02, test loss:0.09, accuracy:97.47\n",
            "41 train loss:0.02, test loss:0.11, accuracy:97.07\n",
            "42 train loss:0.01, test loss:0.09, accuracy:97.85\n",
            "43 train loss:0.01, test loss:0.14, accuracy:95.86\n",
            "44 train loss:0.01, test loss:0.10, accuracy:97.66\n",
            "45 train loss:0.01, test loss:0.09, accuracy:97.83\n",
            "46 train loss:0.01, test loss:0.09, accuracy:97.69\n",
            "47 train loss:0.01, test loss:0.15, accuracy:96.50\n",
            "48 train loss:0.10, test loss:0.12, accuracy:97.17\n",
            "49 train loss:0.02, test loss:0.09, accuracy:97.59\n",
            "50 train loss:0.02, test loss:0.10, accuracy:97.69\n",
            "51 train loss:0.01, test loss:0.10, accuracy:97.55\n",
            "52 train loss:0.01, test loss:0.11, accuracy:97.50\n",
            "53 train loss:0.01, test loss:0.10, accuracy:97.79\n",
            "54 train loss:0.01, test loss:0.10, accuracy:97.62\n",
            "55 train loss:0.01, test loss:0.12, accuracy:97.33\n",
            "56 train loss:0.01, test loss:0.10, accuracy:97.80\n",
            "57 train loss:0.01, test loss:0.10, accuracy:97.82\n",
            "58 train loss:0.00, test loss:0.12, accuracy:97.38\n",
            "59 train loss:0.00, test loss:0.10, accuracy:97.81\n",
            "60 train loss:0.00, test loss:0.11, accuracy:97.78\n",
            "61 train loss:0.00, test loss:0.11, accuracy:97.69\n",
            "62 train loss:0.00, test loss:0.10, accuracy:97.82\n",
            "63 train loss:0.00, test loss:0.10, accuracy:97.81\n",
            "64 train loss:0.00, test loss:0.11, accuracy:97.87\n",
            "65 train loss:0.00, test loss:0.11, accuracy:97.84\n",
            "66 train loss:0.00, test loss:0.11, accuracy:97.81\n",
            "67 train loss:0.00, test loss:0.11, accuracy:97.82\n",
            "68 train loss:0.00, test loss:0.11, accuracy:97.74\n",
            "69 train loss:0.00, test loss:0.11, accuracy:97.79\n",
            "70 train loss:0.00, test loss:0.11, accuracy:97.77\n",
            "71 train loss:0.00, test loss:0.11, accuracy:97.76\n",
            "72 train loss:0.00, test loss:0.11, accuracy:97.81\n",
            "73 train loss:0.00, test loss:0.12, accuracy:97.70\n",
            "74 train loss:0.00, test loss:0.11, accuracy:97.83\n",
            "75 train loss:0.00, test loss:0.11, accuracy:97.82\n",
            "76 train loss:0.00, test loss:0.11, accuracy:97.82\n",
            "77 train loss:0.00, test loss:0.11, accuracy:97.81\n",
            "78 train loss:0.00, test loss:0.11, accuracy:97.76\n",
            "79 train loss:0.00, test loss:0.11, accuracy:97.85\n",
            "80 train loss:0.00, test loss:0.11, accuracy:97.83\n",
            "81 train loss:0.00, test loss:0.12, accuracy:97.82\n",
            "82 train loss:0.00, test loss:0.11, accuracy:97.82\n",
            "83 train loss:0.00, test loss:0.12, accuracy:97.79\n",
            "84 train loss:0.00, test loss:0.12, accuracy:97.84\n",
            "85 train loss:0.00, test loss:0.12, accuracy:97.82\n",
            "86 train loss:0.00, test loss:0.12, accuracy:97.83\n",
            "87 train loss:0.00, test loss:0.12, accuracy:97.84\n",
            "88 train loss:0.00, test loss:0.12, accuracy:97.82\n",
            "89 train loss:0.00, test loss:0.12, accuracy:97.81\n",
            "90 train loss:0.00, test loss:0.12, accuracy:97.84\n",
            "91 train loss:0.00, test loss:0.12, accuracy:97.83\n",
            "92 train loss:0.00, test loss:0.12, accuracy:97.83\n",
            "93 train loss:0.00, test loss:0.12, accuracy:97.81\n",
            "94 train loss:0.00, test loss:0.12, accuracy:97.83\n",
            "95 train loss:0.00, test loss:0.12, accuracy:97.82\n",
            "96 train loss:0.00, test loss:0.12, accuracy:97.80\n",
            "97 train loss:0.00, test loss:0.12, accuracy:97.80\n",
            "98 train loss:0.00, test loss:0.12, accuracy:97.82\n",
            "99 train loss:0.00, test loss:0.12, accuracy:97.80\n",
            "100 train loss:0.00, test loss:0.12, accuracy:97.78\n"
          ]
        }
      ],
      "source": [
        "epochs=100\n",
        "for epoch in range(1,epochs+1):\n",
        "    train_loss=train(model2, train_loader, optimizer)\n",
        "    test_loss, test_accuracy=evaluate(model2,test_loader)\n",
        "    print('{} train loss:{:.2f}, test loss:{:.2f}, accuracy:{:.2f}'.format(epoch,train_loss,test_loss,test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**<br>\n",
        "활성화 함수로 sigmoid 대신 relu를 사용하였다. <br>\n",
        "sigmoid를 사용했을 때에는 계속 작은 미분값이 곱해지기 때문에 중간에 gradient vanishing 문제가 일어날 수 있다. 하지만, relu를 사용함으로써 기울기값이 양수일 경우에는 손실이 발생하지 않게 된다.\n",
        "relu를 사용했을 때 더 작은 손실함수(train loss)에 도달하고 정확도 역시 더 높게 나타난다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[8기 최윤서] Deep_Learning_Basic_과제.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}