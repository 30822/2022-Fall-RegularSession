{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rjuQY9f2mdS"
   },
   "source": [
    "## 과제 1\n",
    "ReLu activation function과 derivative function을 구현해보세요\n",
    "- Hint : np.maximum 함수 사용하면 편리합니다\n",
    "- 다른 방법 사용하셔도 무방합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "puH0YVGI2uLz"
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    x = np.where(x>0, x, 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Esm4jmTVijro"
   },
   "outputs": [],
   "source": [
    "def d_relu(x):\n",
    "    dx = np.where(x>0, 1, 0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz8Hi0Rc2-yJ"
   },
   "source": [
    "## 과제 2\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
    "- Hint : 코드 파일의 예시는 Two layer MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_grads(grad):\n",
    "    for k, v in grad.items():\n",
    "        print(f\"Key {k} : {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "    dS3 = params[\"A3\"] - y_true\n",
    "    # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
    "    # dS2 is softmax + CE loss derivative\n",
    "\n",
    "    grads = {}\n",
    "\n",
    "    grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
    "    grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
    "    \n",
    "    dA2 = np.dot(params[\"W3\"].T, dS3)\n",
    "    dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
    "    \n",
    "    grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "    grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "    dS1 = dA1 * d_relu(params[\"S1\"])\n",
    "\n",
    "    grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "    grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twf-R8s-34zT"
   },
   "source": [
    "## 과제 3\n",
    "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
    "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
    "\n",
    "hyperparameter는 다음과 같이 설정\n",
    "\n",
    "- epochs : 100\n",
    "- hiddensize : 128, 64 (two layer)\n",
    "- learning_rate : 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "num_train = 60000\n",
    "num_class = 10\n",
    "\n",
    "x_train = np.float32(mnist.data[:num_train]).T\n",
    "y_train_index = np.int32(mnist.target[:num_train]).T\n",
    "x_test = np.float32(mnist.data[num_train:]).T\n",
    "y_test_index = np.int32(mnist.target[num_train:]).T\n",
    "\n",
    "# Normalization\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_size = x_train.shape[0]\n",
    "\n",
    "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
    "for idx in range(y_train_index.shape[0]):\n",
    "    y_train[y_train_index[idx], idx] = 1\n",
    "\n",
    "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
    "for idx in range(y_test_index.shape[0]):\n",
    "    y_test[y_test_index[idx], idx] = 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier(input_dim, hidden_dim):\n",
    "    weight = np.random.randn(hidden_dim, input_dim) * np.sqrt(1/ input_dim)\n",
    "    bias = np.zeros((hidden_dim, 1)) * np.sqrt(1/ input_dim)\n",
    "    return weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#parameter initialization\n",
    "\n",
    "hidden_size = [128, 64] # hidden unit size\n",
    "\n",
    "dims = [x_size]+hidden_size+[num_class]\n",
    "params = dict()\n",
    "for i in range(len(dims)-1):\n",
    "    weight, bias = xavier(dims[i], dims[i+1])\n",
    "    params[f\"W{i+1}\"] = weight\n",
    "    params[f\"b{i+1}\"] = bias\n",
    "# Xavier initialization: https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    # derivative of sigmoid\n",
    "    exp = np.exp(-x)\n",
    "    return (exp)/((1+exp)**2)\n",
    "\n",
    "def softmax(x):\n",
    "    exp = np.exp(x)\n",
    "    return exp/np.sum(exp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    # loss calculation\n",
    "\n",
    "    num_sample = y_true.shape[1]\n",
    "    Li = -1 * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return Li/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "  y_true_idx = np.argmax(y_true, axis = 0)\n",
    "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
    "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
    "\n",
    "  accuracy = num_correct / y_true.shape[1] * 100\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(x, params):\n",
    "  \n",
    "    params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "    params[\"A1\"] = relu(params[\"S1\"])\n",
    "    \n",
    "    params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
    "    params[\"A2\"] = sigmoid(params[\"S2\"])\n",
    "    \n",
    "    params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
    "    params[\"A3\"] = softmax(params[\"S3\"])\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass_test(x, params):\n",
    "\n",
    "    params_test = {}\n",
    "\n",
    "    params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "    params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
    "    params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
    "    params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
    "\n",
    "    params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
    "    params_test[\"A3\"] = softmax(params_test[\"S3\"])\n",
    "    return params_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.304747, training acuracy = 12.34%, test loss = 2.302674, training acuracy = 8.25%\n",
      "Epoch 2: training loss = 2.276494, training acuracy = 20.33%, test loss = 2.302275, training acuracy = 8.61%\n",
      "Epoch 3: training loss = 2.258614, training acuracy = 27.62%, test loss = 2.301907, training acuracy = 10.98%\n",
      "Epoch 4: training loss = 2.240996, training acuracy = 34.25%, test loss = 2.30155, training acuracy = 12.88%\n",
      "Epoch 5: training loss = 2.222263, training acuracy = 40.01%, test loss = 2.301184, training acuracy = 14.68%\n",
      "Epoch 6: training loss = 2.202012, training acuracy = 44.85%, test loss = 2.300799, training acuracy = 16.03%\n",
      "Epoch 7: training loss = 2.180044, training acuracy = 48.88%, test loss = 2.300386, training acuracy = 17.44%\n",
      "Epoch 8: training loss = 2.156241, training acuracy = 52.17%, test loss = 2.299942, training acuracy = 19.47%\n",
      "Epoch 9: training loss = 2.130472, training acuracy = 54.75%, test loss = 2.299462, training acuracy = 22.58%\n",
      "Epoch 10: training loss = 2.102572, training acuracy = 56.92%, test loss = 2.29894, training acuracy = 26.76%\n",
      "Epoch 11: training loss = 2.072357, training acuracy = 58.86%, test loss = 2.298371, training acuracy = 32.46%\n",
      "Epoch 12: training loss = 2.039652, training acuracy = 60.52%, test loss = 2.29775, training acuracy = 37.54%\n",
      "Epoch 13: training loss = 2.004323, training acuracy = 61.94%, test loss = 2.297072, training acuracy = 41.47%\n",
      "Epoch 14: training loss = 1.966325, training acuracy = 63.14%, test loss = 2.296333, training acuracy = 44.54%\n",
      "Epoch 15: training loss = 1.925695, training acuracy = 64.29%, test loss = 2.29553, training acuracy = 46.99%\n",
      "Epoch 16: training loss = 1.882532, training acuracy = 65.22%, test loss = 2.294659, training acuracy = 48.39%\n",
      "Epoch 17: training loss = 1.836988, training acuracy = 66.17%, test loss = 2.293719, training acuracy = 49.85%\n",
      "Epoch 18: training loss = 1.789308, training acuracy = 67.03%, test loss = 2.292711, training acuracy = 50.95%\n",
      "Epoch 19: training loss = 1.73981, training acuracy = 67.87%, test loss = 2.291634, training acuracy = 51.94%\n",
      "Epoch 20: training loss = 1.688881, training acuracy = 68.66%, test loss = 2.290492, training acuracy = 52.7%\n",
      "Epoch 21: training loss = 1.636973, training acuracy = 69.37%, test loss = 2.289289, training acuracy = 53.64%\n",
      "Epoch 22: training loss = 1.584572, training acuracy = 70.03%, test loss = 2.28803, training acuracy = 54.37%\n",
      "Epoch 23: training loss = 1.5322, training acuracy = 70.65%, test loss = 2.286723, training acuracy = 55.29%\n",
      "Epoch 24: training loss = 1.480386, training acuracy = 71.23%, test loss = 2.285376, training acuracy = 56.11%\n",
      "Epoch 25: training loss = 1.429633, training acuracy = 71.79%, test loss = 2.283998, training acuracy = 57.03%\n",
      "Epoch 26: training loss = 1.38039, training acuracy = 72.39%, test loss = 2.282599, training acuracy = 57.93%\n",
      "Epoch 27: training loss = 1.333022, training acuracy = 72.95%, test loss = 2.281188, training acuracy = 58.79%\n",
      "Epoch 28: training loss = 1.287806, training acuracy = 73.45%, test loss = 2.279776, training acuracy = 59.7%\n",
      "Epoch 29: training loss = 1.24492, training acuracy = 73.92%, test loss = 2.27837, training acuracy = 60.63%\n",
      "Epoch 30: training loss = 1.204449, training acuracy = 74.4%, test loss = 2.276978, training acuracy = 61.39%\n",
      "Epoch 31: training loss = 1.166404, training acuracy = 74.91%, test loss = 2.275607, training acuracy = 62.4%\n",
      "Epoch 32: training loss = 1.130731, training acuracy = 75.44%, test loss = 2.274262, training acuracy = 63.18%\n",
      "Epoch 33: training loss = 1.09733, training acuracy = 76.0%, test loss = 2.272948, training acuracy = 64.07%\n",
      "Epoch 34: training loss = 1.066073, training acuracy = 76.45%, test loss = 2.271665, training acuracy = 64.77%\n",
      "Epoch 35: training loss = 1.036815, training acuracy = 76.93%, test loss = 2.270417, training acuracy = 65.48%\n",
      "Epoch 36: training loss = 1.009403, training acuracy = 77.42%, test loss = 2.269205, training acuracy = 66.11%\n",
      "Epoch 37: training loss = 0.98369, training acuracy = 77.9%, test loss = 2.268029, training acuracy = 66.76%\n",
      "Epoch 38: training loss = 0.959532, training acuracy = 78.32%, test loss = 2.266888, training acuracy = 67.39%\n",
      "Epoch 39: training loss = 0.936796, training acuracy = 78.74%, test loss = 2.265782, training acuracy = 67.95%\n",
      "Epoch 40: training loss = 0.91536, training acuracy = 79.18%, test loss = 2.26471, training acuracy = 68.4%\n",
      "Epoch 41: training loss = 0.895113, training acuracy = 79.56%, test loss = 2.263671, training acuracy = 68.79%\n",
      "Epoch 42: training loss = 0.875957, training acuracy = 79.95%, test loss = 2.262663, training acuracy = 69.14%\n",
      "Epoch 43: training loss = 0.8578, training acuracy = 80.31%, test loss = 2.261685, training acuracy = 69.37%\n",
      "Epoch 44: training loss = 0.840564, training acuracy = 80.68%, test loss = 2.260736, training acuracy = 69.74%\n",
      "Epoch 45: training loss = 0.824177, training acuracy = 81.02%, test loss = 2.259815, training acuracy = 70.02%\n",
      "Epoch 46: training loss = 0.808576, training acuracy = 81.31%, test loss = 2.258919, training acuracy = 70.31%\n",
      "Epoch 47: training loss = 0.793705, training acuracy = 81.57%, test loss = 2.258048, training acuracy = 70.48%\n",
      "Epoch 48: training loss = 0.779512, training acuracy = 81.86%, test loss = 2.257201, training acuracy = 70.67%\n",
      "Epoch 49: training loss = 0.76595, training acuracy = 82.14%, test loss = 2.256376, training acuracy = 70.78%\n",
      "Epoch 50: training loss = 0.752979, training acuracy = 82.42%, test loss = 2.255572, training acuracy = 70.93%\n",
      "Epoch 51: training loss = 0.74056, training acuracy = 82.63%, test loss = 2.254789, training acuracy = 71.03%\n",
      "Epoch 52: training loss = 0.728658, training acuracy = 82.87%, test loss = 2.254025, training acuracy = 71.23%\n",
      "Epoch 53: training loss = 0.717244, training acuracy = 83.1%, test loss = 2.25328, training acuracy = 71.23%\n",
      "Epoch 54: training loss = 0.706289, training acuracy = 83.36%, test loss = 2.252552, training acuracy = 71.46%\n",
      "Epoch 55: training loss = 0.695767, training acuracy = 83.54%, test loss = 2.251841, training acuracy = 71.61%\n",
      "Epoch 56: training loss = 0.685654, training acuracy = 83.76%, test loss = 2.251147, training acuracy = 71.67%\n",
      "Epoch 57: training loss = 0.675927, training acuracy = 83.91%, test loss = 2.250468, training acuracy = 71.71%\n",
      "Epoch 58: training loss = 0.666567, training acuracy = 84.09%, test loss = 2.249805, training acuracy = 71.74%\n",
      "Epoch 59: training loss = 0.657553, training acuracy = 84.29%, test loss = 2.249156, training acuracy = 71.84%\n",
      "Epoch 60: training loss = 0.648867, training acuracy = 84.44%, test loss = 2.24852, training acuracy = 71.89%\n",
      "Epoch 61: training loss = 0.640494, training acuracy = 84.6%, test loss = 2.247898, training acuracy = 71.94%\n",
      "Epoch 62: training loss = 0.632417, training acuracy = 84.74%, test loss = 2.247289, training acuracy = 71.85%\n",
      "Epoch 63: training loss = 0.624622, training acuracy = 84.91%, test loss = 2.246693, training acuracy = 71.83%\n",
      "Epoch 64: training loss = 0.617095, training acuracy = 85.03%, test loss = 2.246107, training acuracy = 71.87%\n",
      "Epoch 65: training loss = 0.609824, training acuracy = 85.18%, test loss = 2.245534, training acuracy = 71.78%\n",
      "Epoch 66: training loss = 0.602796, training acuracy = 85.34%, test loss = 2.244971, training acuracy = 71.76%\n",
      "Epoch 67: training loss = 0.595999, training acuracy = 85.44%, test loss = 2.24442, training acuracy = 71.74%\n",
      "Epoch 68: training loss = 0.589423, training acuracy = 85.54%, test loss = 2.243878, training acuracy = 71.78%\n",
      "Epoch 69: training loss = 0.583057, training acuracy = 85.64%, test loss = 2.243347, training acuracy = 71.7%\n",
      "Epoch 70: training loss = 0.576892, training acuracy = 85.77%, test loss = 2.242826, training acuracy = 71.68%\n",
      "Epoch 71: training loss = 0.570919, training acuracy = 85.87%, test loss = 2.242314, training acuracy = 71.61%\n",
      "Epoch 72: training loss = 0.56513, training acuracy = 85.98%, test loss = 2.241811, training acuracy = 71.52%\n",
      "Epoch 73: training loss = 0.559516, training acuracy = 86.14%, test loss = 2.241317, training acuracy = 71.5%\n",
      "Epoch 74: training loss = 0.554071, training acuracy = 86.26%, test loss = 2.240832, training acuracy = 71.47%\n",
      "Epoch 75: training loss = 0.548787, training acuracy = 86.36%, test loss = 2.240355, training acuracy = 71.44%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76: training loss = 0.543657, training acuracy = 86.46%, test loss = 2.239886, training acuracy = 71.47%\n",
      "Epoch 77: training loss = 0.538675, training acuracy = 86.56%, test loss = 2.239426, training acuracy = 71.45%\n",
      "Epoch 78: training loss = 0.533834, training acuracy = 86.65%, test loss = 2.238972, training acuracy = 71.36%\n",
      "Epoch 79: training loss = 0.52913, training acuracy = 86.74%, test loss = 2.238527, training acuracy = 71.39%\n",
      "Epoch 80: training loss = 0.524555, training acuracy = 86.82%, test loss = 2.238088, training acuracy = 71.35%\n",
      "Epoch 81: training loss = 0.520106, training acuracy = 86.9%, test loss = 2.237657, training acuracy = 71.33%\n",
      "Epoch 82: training loss = 0.515777, training acuracy = 86.96%, test loss = 2.237232, training acuracy = 71.23%\n",
      "Epoch 83: training loss = 0.511564, training acuracy = 87.06%, test loss = 2.236814, training acuracy = 71.16%\n",
      "Epoch 84: training loss = 0.507462, training acuracy = 87.12%, test loss = 2.236402, training acuracy = 71.07%\n",
      "Epoch 85: training loss = 0.503466, training acuracy = 87.2%, test loss = 2.235997, training acuracy = 71.02%\n",
      "Epoch 86: training loss = 0.499573, training acuracy = 87.29%, test loss = 2.235597, training acuracy = 71.0%\n",
      "Epoch 87: training loss = 0.495779, training acuracy = 87.37%, test loss = 2.235204, training acuracy = 70.98%\n",
      "Epoch 88: training loss = 0.49208, training acuracy = 87.47%, test loss = 2.234816, training acuracy = 70.94%\n",
      "Epoch 89: training loss = 0.488473, training acuracy = 87.55%, test loss = 2.234435, training acuracy = 70.9%\n",
      "Epoch 90: training loss = 0.484954, training acuracy = 87.63%, test loss = 2.234058, training acuracy = 70.78%\n",
      "Epoch 91: training loss = 0.48152, training acuracy = 87.71%, test loss = 2.233688, training acuracy = 70.72%\n",
      "Epoch 92: training loss = 0.478168, training acuracy = 87.79%, test loss = 2.233322, training acuracy = 70.7%\n",
      "Epoch 93: training loss = 0.474895, training acuracy = 87.84%, test loss = 2.232961, training acuracy = 70.65%\n",
      "Epoch 94: training loss = 0.471699, training acuracy = 87.92%, test loss = 2.232605, training acuracy = 70.6%\n",
      "Epoch 95: training loss = 0.468577, training acuracy = 87.99%, test loss = 2.232254, training acuracy = 70.53%\n",
      "Epoch 96: training loss = 0.465525, training acuracy = 88.04%, test loss = 2.231908, training acuracy = 70.48%\n",
      "Epoch 97: training loss = 0.462542, training acuracy = 88.07%, test loss = 2.231566, training acuracy = 70.45%\n",
      "Epoch 98: training loss = 0.459625, training acuracy = 88.15%, test loss = 2.231228, training acuracy = 70.42%\n",
      "Epoch 99: training loss = 0.456773, training acuracy = 88.21%, test loss = 2.230895, training acuracy = 70.32%\n",
      "Epoch 100: training loss = 0.453983, training acuracy = 88.26%, test loss = 2.230566, training acuracy = 70.26%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    if i == 0:\n",
    "        params = foward_pass(x_train, params)\n",
    "        \n",
    "    grads = backward_pass(x_train, y_train, params)\n",
    "\n",
    "    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "    \n",
    "    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
    "    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
    "\n",
    "    params = foward_pass(x_train, params)\n",
    "    train_loss = compute_loss(y_train, params[\"A3\"])\n",
    "    train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
    "\n",
    "    params_test = foward_pass_test(x_test, params)\n",
    "    test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
    "    test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
    "\n",
    "    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
    "    .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaqqRzF73oBu"
   },
   "source": [
    "## 과제 4\n",
    "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
    "\n",
    "- Hint : Activation function, hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter initialization\n",
    "\n",
    "hidden_size = [256, 32] # hidden unit size\n",
    "\n",
    "dims = [x_size]+hidden_size+[num_class]\n",
    "params = dict()\n",
    "for i in range(len(dims)-1):\n",
    "    weight, bias = xavier(dims[i], dims[i+1])\n",
    "    params[f\"W{i+1}\"] = weight\n",
    "    params[f\"b{i+1}\"] = bias\n",
    "# Xavier initialization: https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "k6b82DZG6W3j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.286303, training acuracy = 17.5%, test loss = 2.301917, training acuracy = 13.47%\n",
      "Epoch 2: training loss = 2.250438, training acuracy = 37.74%, test loss = 2.300512, training acuracy = 10.32%\n",
      "Epoch 3: training loss = 2.221862, training acuracy = 40.28%, test loss = 2.29938, training acuracy = 10.32%\n",
      "Epoch 4: training loss = 2.190093, training acuracy = 46.52%, test loss = 2.298203, training acuracy = 10.38%\n",
      "Epoch 5: training loss = 2.153225, training acuracy = 52.45%, test loss = 2.296877, training acuracy = 11.05%\n",
      "Epoch 6: training loss = 2.11002, training acuracy = 57.13%, test loss = 2.295336, training acuracy = 12.23%\n",
      "Epoch 7: training loss = 2.059501, training acuracy = 60.64%, test loss = 2.293514, training acuracy = 13.39%\n",
      "Epoch 8: training loss = 2.000912, training acuracy = 63.42%, test loss = 2.291346, training acuracy = 14.96%\n",
      "Epoch 9: training loss = 1.933876, training acuracy = 65.54%, test loss = 2.288764, training acuracy = 17.39%\n",
      "Epoch 10: training loss = 1.858734, training acuracy = 67.47%, test loss = 2.285716, training acuracy = 20.93%\n",
      "Epoch 11: training loss = 1.77678, training acuracy = 69.22%, test loss = 2.28218, training acuracy = 24.96%\n",
      "Epoch 12: training loss = 1.69019, training acuracy = 71.01%, test loss = 2.278167, training acuracy = 29.78%\n",
      "Epoch 13: training loss = 1.601653, training acuracy = 72.74%, test loss = 2.27373, training acuracy = 34.03%\n",
      "Epoch 14: training loss = 1.513891, training acuracy = 74.26%, test loss = 2.268945, training acuracy = 37.62%\n",
      "Epoch 15: training loss = 1.429191, training acuracy = 75.49%, test loss = 2.263908, training acuracy = 40.26%\n",
      "Epoch 16: training loss = 1.349205, training acuracy = 76.62%, test loss = 2.258711, training acuracy = 42.67%\n",
      "Epoch 17: training loss = 1.274915, training acuracy = 77.67%, test loss = 2.253453, training acuracy = 44.38%\n",
      "Epoch 18: training loss = 1.206734, training acuracy = 78.47%, test loss = 2.248187, training acuracy = 46.13%\n",
      "Epoch 19: training loss = 1.144659, training acuracy = 79.17%, test loss = 2.243011, training acuracy = 47.49%\n",
      "Epoch 20: training loss = 1.088416, training acuracy = 79.72%, test loss = 2.237887, training acuracy = 49.06%\n",
      "Epoch 21: training loss = 1.03759, training acuracy = 80.22%, test loss = 2.233017, training acuracy = 50.06%\n",
      "Epoch 22: training loss = 0.991721, training acuracy = 80.73%, test loss = 2.228073, training acuracy = 52.46%\n",
      "Epoch 23: training loss = 0.950449, training acuracy = 81.15%, test loss = 2.223783, training acuracy = 50.95%\n",
      "Epoch 24: training loss = 0.913935, training acuracy = 81.6%, test loss = 2.218718, training acuracy = 58.22%\n",
      "Epoch 25: training loss = 0.884685, training acuracy = 81.1%, test loss = 2.215871, training acuracy = 45.37%\n",
      "Epoch 26: training loss = 0.875477, training acuracy = 81.09%, test loss = 2.209744, training acuracy = 71.3%\n",
      "Epoch 27: training loss = 0.943717, training acuracy = 72.88%, test loss = 2.212652, training acuracy = 26.84%\n",
      "Epoch 28: training loss = 1.248574, training acuracy = 60.48%, test loss = 2.211789, training acuracy = 33.17%\n",
      "Epoch 29: training loss = 1.550178, training acuracy = 51.87%, test loss = 2.211891, training acuracy = 21.1%\n",
      "Epoch 30: training loss = 1.181437, training acuracy = 66.36%, test loss = 2.201823, training acuracy = 36.29%\n",
      "Epoch 31: training loss = 0.836108, training acuracy = 74.06%, test loss = 2.180119, training acuracy = 56.71%\n",
      "Epoch 32: training loss = 0.765066, training acuracy = 81.64%, test loss = 2.174035, training acuracy = 62.49%\n",
      "Epoch 33: training loss = 0.746828, training acuracy = 82.04%, test loss = 2.172486, training acuracy = 67.0%\n",
      "Epoch 34: training loss = 0.751747, training acuracy = 79.35%, test loss = 2.167801, training acuracy = 50.31%\n",
      "Epoch 35: training loss = 0.809615, training acuracy = 76.08%, test loss = 2.170432, training acuracy = 58.42%\n",
      "Epoch 36: training loss = 0.895301, training acuracy = 67.69%, test loss = 2.162037, training acuracy = 37.69%\n",
      "Epoch 37: training loss = 1.039083, training acuracy = 64.32%, test loss = 2.172902, training acuracy = 45.49%\n",
      "Epoch 38: training loss = 0.901075, training acuracy = 67.11%, test loss = 2.152515, training acuracy = 37.23%\n",
      "Epoch 39: training loss = 0.840619, training acuracy = 74.11%, test loss = 2.161561, training acuracy = 60.13%\n",
      "Epoch 40: training loss = 0.712987, training acuracy = 77.7%, test loss = 2.142077, training acuracy = 43.55%\n",
      "Epoch 41: training loss = 0.657387, training acuracy = 83.42%, test loss = 2.146574, training acuracy = 72.47%\n",
      "Epoch 42: training loss = 0.623768, training acuracy = 83.08%, test loss = 2.136943, training acuracy = 54.67%\n",
      "Epoch 43: training loss = 0.602636, training acuracy = 85.28%, test loss = 2.138968, training acuracy = 72.98%\n",
      "Epoch 44: training loss = 0.587305, training acuracy = 84.6%, test loss = 2.133123, training acuracy = 59.28%\n",
      "Epoch 45: training loss = 0.57382, training acuracy = 85.92%, test loss = 2.133978, training acuracy = 71.85%\n",
      "Epoch 46: training loss = 0.562567, training acuracy = 85.41%, test loss = 2.129485, training acuracy = 61.1%\n",
      "Epoch 47: training loss = 0.551888, training acuracy = 86.39%, test loss = 2.129797, training acuracy = 71.43%\n",
      "Epoch 48: training loss = 0.542463, training acuracy = 85.94%, test loss = 2.125971, training acuracy = 62.1%\n",
      "Epoch 49: training loss = 0.533329, training acuracy = 86.78%, test loss = 2.12602, training acuracy = 71.69%\n",
      "Epoch 50: training loss = 0.525134, training acuracy = 86.38%, test loss = 2.122578, training acuracy = 62.53%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "learning_rate = 1\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    if i == 0:\n",
    "        params = foward_pass(x_train, params)\n",
    "        \n",
    "    grads = backward_pass(x_train, y_train, params)\n",
    "\n",
    "    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "    \n",
    "    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
    "    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
    "\n",
    "    params = foward_pass(x_train, params)\n",
    "    train_loss = compute_loss(y_train, params[\"A3\"])\n",
    "    train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
    "\n",
    "    params_test = foward_pass_test(x_test, params)\n",
    "    test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
    "    test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
    "\n",
    "    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
    "    .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pboMIBQq7onH"
   },
   "source": [
    "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**\n",
    "\n",
    "기존에 sigmoid에서 relu를 사용하면서 업데이트가 느리게 되었던 gradient vanishing이 어느정도 해결되었고, 이와 더불어 lr을 개선해서 학습을 빠르게 진행했다. optimizer 자체를 개선하기 위해 params를 개선하는 것도 방법이 될 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux7mPf6E78d4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "[0818] Deep Learning Basic_과제.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsl",
   "language": "python",
   "name": "dsl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
