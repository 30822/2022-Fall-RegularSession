{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "CIqosdOvJ3JO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    for i in range(0, len(x)):\n",
        "        x[i] = max(0, x[i])\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEBMoSKLERmB",
        "outputId": "36e7f112-c94f-42a1-e43f-c86cc60976dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0.5, 10, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "relu([-2, -3, 0.5, 10, -4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "outputs": [],
      "source": [
        "def d_relu(x):\n",
        "    for i in range(0, len(x)):\n",
        "        if x[i] > 0:\n",
        "            x[i] = 1\n",
        "        else:\n",
        "            x[i] = 0\n",
        "    \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56B3AsZtE8xC",
        "outputId": "28888c2b-0653-40eb-e144-b572408b7513"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "d_relu([-2, -3, 0.5, 10, -4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "outputs": [],
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bxJO249A3jhk"
      },
      "outputs": [],
      "source": [
        "# Assignment 3 구현은 여기서\n",
        "\n",
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OHVNckhAGUxG"
      },
      "outputs": [],
      "source": [
        "# data preprocessing\n",
        "\n",
        "num_train = 60000\n",
        "num_class = 10\n",
        "\n",
        "x_train = np.float32(mnist.data[:num_train]).T\n",
        "y_train_index = np.int32(mnist.target[:num_train]).T\n",
        "x_test = np.float32(mnist.data[num_train:]).T\n",
        "y_test_index = np.int32(mnist.target[num_train:]).T\n",
        "\n",
        "# Normalization\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "x_size = x_train.shape[0]\n",
        "\n",
        "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
        "for idx in range(y_train_index.shape[0]):\n",
        "  y_train[y_train_index[idx], idx] = 1\n",
        "\n",
        "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
        "for idx in range(y_test_index.shape[0]):\n",
        "  y_test[y_test_index[idx], idx] = 1    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#parameter initialization\n",
        "\n",
        "hidden_size1 = 128\n",
        "hidden_size2 = 64\n",
        "\n",
        "# three-layer neural network\n",
        "\n",
        "params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n",
        "          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/x_size),\n",
        "          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/hidden_size1),\n",
        "          \"b2\": np.zeros((hidden_size2, 1)) * np.sqrt(1/hidden_size1),\n",
        "          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n",
        "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n",
        "          }\n",
        "# Xavier initialization: https://reniew.github.io/13/"
      ],
      "metadata": {
        "id": "l6OgMZbDE2G3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  # derivative of sigmoid\n",
        "  exp = np.exp(-x)\n",
        "  return (exp)/((1+exp)**2)\n",
        "\n",
        "def softmax(x):\n",
        "  exp = np.exp(x)\n",
        "  return exp/np.sum(exp, axis=0)\n",
        "\n",
        "def compute_loss(y_true, y_pred):\n",
        "  # loss calculation\n",
        "\n",
        "  num_sample = y_true.shape[1]\n",
        "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
        "  \n",
        "  return Li/num_sample\n",
        "\n",
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = softmax(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params\n",
        "\n",
        "def foward_pass_test(x, params):\n",
        "\n",
        "  params_test = {}\n",
        "  \n",
        "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
        "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
        "  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
        "  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
        "  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n",
        "\n",
        "  return params_test\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "  y_true_idx = np.argmax(y_true, axis = 0)\n",
        "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
        "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
        "\n",
        "  accuracy = num_correct / y_true.shape[1] * 100\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "hr4zGZtYFQ3O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.5\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  if i == 0:\n",
        "    params = foward_pass(x_train, params)\n",
        "    \n",
        "  grads = backward_pass(x_train, y_train, params)\n",
        "\n",
        "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "\n",
        "  params = foward_pass(x_train, params)\n",
        "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
        "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
        "\n",
        "  params_test = foward_pass_test(x_test, params)\n",
        "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
        "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
        "\n",
        "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
        "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJD430fuF0L7",
        "outputId": "d9dcbc93-3b8a-4e09-f2b1-34d21b9f6f97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: training loss = 2.302552, training acuracy = 9.75%, test loss = 2.302432, training acuracy = 9.74%\n",
            "Epoch 2: training loss = 2.30239, training acuracy = 9.75%, test loss = 2.302267, training acuracy = 9.74%\n",
            "Epoch 3: training loss = 2.302227, training acuracy = 9.75%, test loss = 2.302103, training acuracy = 9.74%\n",
            "Epoch 4: training loss = 2.302065, training acuracy = 9.75%, test loss = 2.301938, training acuracy = 9.74%\n",
            "Epoch 5: training loss = 2.301902, training acuracy = 9.75%, test loss = 2.301774, training acuracy = 9.74%\n",
            "Epoch 6: training loss = 2.301739, training acuracy = 9.75%, test loss = 2.301609, training acuracy = 9.74%\n",
            "Epoch 7: training loss = 2.301575, training acuracy = 9.75%, test loss = 2.301443, training acuracy = 9.74%\n",
            "Epoch 8: training loss = 2.301412, training acuracy = 9.75%, test loss = 2.301278, training acuracy = 9.74%\n",
            "Epoch 9: training loss = 2.301248, training acuracy = 9.75%, test loss = 2.301112, training acuracy = 9.74%\n",
            "Epoch 10: training loss = 2.301084, training acuracy = 9.75%, test loss = 2.300946, training acuracy = 9.74%\n",
            "Epoch 11: training loss = 2.300919, training acuracy = 9.75%, test loss = 2.300779, training acuracy = 9.74%\n",
            "Epoch 12: training loss = 2.300754, training acuracy = 9.75%, test loss = 2.300612, training acuracy = 9.74%\n",
            "Epoch 13: training loss = 2.300589, training acuracy = 9.75%, test loss = 2.300445, training acuracy = 9.74%\n",
            "Epoch 14: training loss = 2.300423, training acuracy = 9.75%, test loss = 2.300276, training acuracy = 9.74%\n",
            "Epoch 15: training loss = 2.300256, training acuracy = 9.75%, test loss = 2.300107, training acuracy = 9.74%\n",
            "Epoch 16: training loss = 2.300088, training acuracy = 9.75%, test loss = 2.299938, training acuracy = 9.74%\n",
            "Epoch 17: training loss = 2.29992, training acuracy = 9.75%, test loss = 2.299767, training acuracy = 9.74%\n",
            "Epoch 18: training loss = 2.299751, training acuracy = 9.75%, test loss = 2.299596, training acuracy = 9.74%\n",
            "Epoch 19: training loss = 2.299581, training acuracy = 9.75%, test loss = 2.299424, training acuracy = 9.74%\n",
            "Epoch 20: training loss = 2.29941, training acuracy = 9.75%, test loss = 2.299251, training acuracy = 9.74%\n",
            "Epoch 21: training loss = 2.299238, training acuracy = 9.75%, test loss = 2.299076, training acuracy = 9.74%\n",
            "Epoch 22: training loss = 2.299065, training acuracy = 9.75%, test loss = 2.298901, training acuracy = 9.74%\n",
            "Epoch 23: training loss = 2.29889, training acuracy = 9.75%, test loss = 2.298724, training acuracy = 9.74%\n",
            "Epoch 24: training loss = 2.298715, training acuracy = 9.75%, test loss = 2.298546, training acuracy = 9.74%\n",
            "Epoch 25: training loss = 2.298538, training acuracy = 9.76%, test loss = 2.298367, training acuracy = 9.74%\n",
            "Epoch 26: training loss = 2.298359, training acuracy = 9.76%, test loss = 2.298186, training acuracy = 9.74%\n",
            "Epoch 27: training loss = 2.298179, training acuracy = 9.77%, test loss = 2.298004, training acuracy = 9.75%\n",
            "Epoch 28: training loss = 2.297997, training acuracy = 9.79%, test loss = 2.29782, training acuracy = 9.76%\n",
            "Epoch 29: training loss = 2.297814, training acuracy = 9.83%, test loss = 2.297634, training acuracy = 9.84%\n",
            "Epoch 30: training loss = 2.297628, training acuracy = 9.92%, test loss = 2.297446, training acuracy = 9.92%\n",
            "Epoch 31: training loss = 2.297441, training acuracy = 11.17%, test loss = 2.297256, training acuracy = 11.21%\n",
            "Epoch 32: training loss = 2.297251, training acuracy = 13.35%, test loss = 2.297064, training acuracy = 13.62%\n",
            "Epoch 33: training loss = 2.29706, training acuracy = 15.25%, test loss = 2.29687, training acuracy = 15.66%\n",
            "Epoch 34: training loss = 2.296866, training acuracy = 16.77%, test loss = 2.296673, training acuracy = 17.14%\n",
            "Epoch 35: training loss = 2.296669, training acuracy = 18.22%, test loss = 2.296474, training acuracy = 18.5%\n",
            "Epoch 36: training loss = 2.29647, training acuracy = 19.56%, test loss = 2.296272, training acuracy = 19.76%\n",
            "Epoch 37: training loss = 2.296268, training acuracy = 20.7%, test loss = 2.296067, training acuracy = 20.96%\n",
            "Epoch 38: training loss = 2.296063, training acuracy = 21.69%, test loss = 2.295859, training acuracy = 21.91%\n",
            "Epoch 39: training loss = 2.295855, training acuracy = 22.32%, test loss = 2.295648, training acuracy = 22.64%\n",
            "Epoch 40: training loss = 2.295643, training acuracy = 22.84%, test loss = 2.295434, training acuracy = 23.23%\n",
            "Epoch 41: training loss = 2.295429, training acuracy = 23.31%, test loss = 2.295217, training acuracy = 23.69%\n",
            "Epoch 42: training loss = 2.29521, training acuracy = 23.72%, test loss = 2.294995, training acuracy = 24.04%\n",
            "Epoch 43: training loss = 2.294988, training acuracy = 24.05%, test loss = 2.29477, training acuracy = 24.54%\n",
            "Epoch 44: training loss = 2.294762, training acuracy = 24.31%, test loss = 2.294541, training acuracy = 24.84%\n",
            "Epoch 45: training loss = 2.294532, training acuracy = 24.47%, test loss = 2.294308, training acuracy = 25.23%\n",
            "Epoch 46: training loss = 2.294298, training acuracy = 24.68%, test loss = 2.294071, training acuracy = 25.72%\n",
            "Epoch 47: training loss = 2.294059, training acuracy = 24.99%, test loss = 2.293829, training acuracy = 25.98%\n",
            "Epoch 48: training loss = 2.293816, training acuracy = 25.22%, test loss = 2.293582, training acuracy = 26.27%\n",
            "Epoch 49: training loss = 2.293568, training acuracy = 25.48%, test loss = 2.293331, training acuracy = 26.6%\n",
            "Epoch 50: training loss = 2.293315, training acuracy = 25.69%, test loss = 2.293074, training acuracy = 26.73%\n",
            "Epoch 51: training loss = 2.293056, training acuracy = 25.94%, test loss = 2.292812, training acuracy = 26.97%\n",
            "Epoch 52: training loss = 2.292793, training acuracy = 26.16%, test loss = 2.292544, training acuracy = 27.07%\n",
            "Epoch 53: training loss = 2.292523, training acuracy = 26.43%, test loss = 2.292271, training acuracy = 27.11%\n",
            "Epoch 54: training loss = 2.292248, training acuracy = 26.68%, test loss = 2.291992, training acuracy = 27.15%\n",
            "Epoch 55: training loss = 2.291967, training acuracy = 26.81%, test loss = 2.291707, training acuracy = 27.12%\n",
            "Epoch 56: training loss = 2.291679, training acuracy = 26.96%, test loss = 2.291416, training acuracy = 27.19%\n",
            "Epoch 57: training loss = 2.291386, training acuracy = 27.11%, test loss = 2.291118, training acuracy = 27.42%\n",
            "Epoch 58: training loss = 2.291085, training acuracy = 27.33%, test loss = 2.290814, training acuracy = 27.67%\n",
            "Epoch 59: training loss = 2.290778, training acuracy = 27.51%, test loss = 2.290502, training acuracy = 27.85%\n",
            "Epoch 60: training loss = 2.290464, training acuracy = 27.76%, test loss = 2.290184, training acuracy = 28.15%\n",
            "Epoch 61: training loss = 2.290142, training acuracy = 28.12%, test loss = 2.289858, training acuracy = 28.58%\n",
            "Epoch 62: training loss = 2.289813, training acuracy = 28.5%, test loss = 2.289524, training acuracy = 29.05%\n",
            "Epoch 63: training loss = 2.289477, training acuracy = 28.96%, test loss = 2.289183, training acuracy = 29.62%\n",
            "Epoch 64: training loss = 2.289133, training acuracy = 29.52%, test loss = 2.288834, training acuracy = 30.25%\n",
            "Epoch 65: training loss = 2.28878, training acuracy = 30.12%, test loss = 2.288477, training acuracy = 30.8%\n",
            "Epoch 66: training loss = 2.28842, training acuracy = 30.72%, test loss = 2.288112, training acuracy = 31.42%\n",
            "Epoch 67: training loss = 2.288051, training acuracy = 31.34%, test loss = 2.287738, training acuracy = 32.13%\n",
            "Epoch 68: training loss = 2.287673, training acuracy = 32.06%, test loss = 2.287355, training acuracy = 32.87%\n",
            "Epoch 69: training loss = 2.287286, training acuracy = 32.76%, test loss = 2.286963, training acuracy = 33.54%\n",
            "Epoch 70: training loss = 2.28689, training acuracy = 33.46%, test loss = 2.286562, training acuracy = 34.38%\n",
            "Epoch 71: training loss = 2.286485, training acuracy = 34.25%, test loss = 2.286152, training acuracy = 35.28%\n",
            "Epoch 72: training loss = 2.286071, training acuracy = 34.99%, test loss = 2.285732, training acuracy = 36.15%\n",
            "Epoch 73: training loss = 2.285647, training acuracy = 35.73%, test loss = 2.285302, training acuracy = 37.03%\n",
            "Epoch 74: training loss = 2.285212, training acuracy = 36.48%, test loss = 2.284862, training acuracy = 37.94%\n",
            "Epoch 75: training loss = 2.284768, training acuracy = 37.3%, test loss = 2.284412, training acuracy = 38.77%\n",
            "Epoch 76: training loss = 2.284313, training acuracy = 38.12%, test loss = 2.283952, training acuracy = 39.42%\n",
            "Epoch 77: training loss = 2.283848, training acuracy = 38.96%, test loss = 2.283481, training acuracy = 40.18%\n",
            "Epoch 78: training loss = 2.283373, training acuracy = 39.76%, test loss = 2.282999, training acuracy = 40.77%\n",
            "Epoch 79: training loss = 2.282886, training acuracy = 40.56%, test loss = 2.282506, training acuracy = 41.58%\n",
            "Epoch 80: training loss = 2.282388, training acuracy = 41.36%, test loss = 2.282002, training acuracy = 42.36%\n",
            "Epoch 81: training loss = 2.281879, training acuracy = 42.17%, test loss = 2.281487, training acuracy = 43.08%\n",
            "Epoch 82: training loss = 2.281358, training acuracy = 42.89%, test loss = 2.28096, training acuracy = 43.82%\n",
            "Epoch 83: training loss = 2.280826, training acuracy = 43.64%, test loss = 2.280421, training acuracy = 44.62%\n",
            "Epoch 84: training loss = 2.280282, training acuracy = 44.35%, test loss = 2.27987, training acuracy = 45.32%\n",
            "Epoch 85: training loss = 2.279725, training acuracy = 45.07%, test loss = 2.279308, training acuracy = 46.06%\n",
            "Epoch 86: training loss = 2.279157, training acuracy = 45.69%, test loss = 2.278733, training acuracy = 46.68%\n",
            "Epoch 87: training loss = 2.278576, training acuracy = 46.34%, test loss = 2.278145, training acuracy = 47.45%\n",
            "Epoch 88: training loss = 2.277983, training acuracy = 46.95%, test loss = 2.277546, training acuracy = 47.96%\n",
            "Epoch 89: training loss = 2.277378, training acuracy = 47.56%, test loss = 2.276933, training acuracy = 48.36%\n",
            "Epoch 90: training loss = 2.276759, training acuracy = 48.13%, test loss = 2.276308, training acuracy = 48.86%\n",
            "Epoch 91: training loss = 2.276128, training acuracy = 48.71%, test loss = 2.275669, training acuracy = 49.34%\n",
            "Epoch 92: training loss = 2.275484, training acuracy = 49.28%, test loss = 2.275018, training acuracy = 49.73%\n",
            "Epoch 93: training loss = 2.274826, training acuracy = 49.82%, test loss = 2.274354, training acuracy = 50.26%\n",
            "Epoch 94: training loss = 2.274156, training acuracy = 50.34%, test loss = 2.273676, training acuracy = 50.8%\n",
            "Epoch 95: training loss = 2.273472, training acuracy = 50.81%, test loss = 2.272985, training acuracy = 51.37%\n",
            "Epoch 96: training loss = 2.272775, training acuracy = 51.28%, test loss = 2.272281, training acuracy = 51.79%\n",
            "Epoch 97: training loss = 2.272064, training acuracy = 51.74%, test loss = 2.271564, training acuracy = 52.19%\n",
            "Epoch 98: training loss = 2.271341, training acuracy = 52.13%, test loss = 2.270833, training acuracy = 52.41%\n",
            "Epoch 99: training loss = 2.270603, training acuracy = 52.5%, test loss = 2.270088, training acuracy = 52.78%\n",
            "Epoch 100: training loss = 2.269853, training acuracy = 52.84%, test loss = 2.269331, training acuracy = 53.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test accuracy: 53.17%"
      ],
      "metadata": {
        "id": "nBL3HD_OG-yj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "k6b82DZG6W3j"
      },
      "outputs": [],
      "source": [
        "# Assignment 4 구현은 여기서\n",
        "\n",
        "# data preprocessing\n",
        "\n",
        "num_train = 60000\n",
        "num_class = 10\n",
        "\n",
        "x_train = np.float32(mnist.data[:num_train]).T\n",
        "y_train_index = np.int32(mnist.target[:num_train]).T\n",
        "x_test = np.float32(mnist.data[num_train:]).T\n",
        "y_test_index = np.int32(mnist.target[num_train:]).T\n",
        "\n",
        "# Normalization\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "x_size = x_train.shape[0]\n",
        "\n",
        "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
        "for idx in range(y_train_index.shape[0]):\n",
        "  y_train[y_train_index[idx], idx] = 1\n",
        "\n",
        "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
        "for idx in range(y_test_index.shape[0]):\n",
        "  y_test[y_test_index[idx], idx] = 1    \n",
        "\n",
        "\n",
        "\n",
        "#parameter initialization\n",
        "\n",
        "hidden_size1 = 64\n",
        "hidden_size2 = 100\n",
        "\n",
        "# three-layer neural network\n",
        "\n",
        "params = {\"W1\": np.random.randn(hidden_size1, x_size) * np.sqrt(1/ x_size),\n",
        "          \"b1\": np.zeros((hidden_size1, 1)) * np.sqrt(1/x_size),\n",
        "          \"W2\": np.random.randn(hidden_size2, hidden_size1) * np.sqrt(1/hidden_size1),\n",
        "          \"b2\": np.zeros((hidden_size2, 1)) * np.sqrt(1/hidden_size1),\n",
        "          \"W3\": np.random.randn(num_class, hidden_size2) * np.sqrt(1/ hidden_size2),\n",
        "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size2)\n",
        "          }\n",
        "# Xavier initialization: https://reniew.github.io/13/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(1e-4, x)\n",
        "\n",
        "def d_relu(x):\n",
        "    return np.greater(x, 1e-4).astype(int)"
      ],
      "metadata": {
        "id": "KYa3k941HY1K"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = softmax(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params\n",
        "\n",
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ],
      "metadata": {
        "id": "Q_WqkRWiHCMO"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.65\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  if i == 0:\n",
        "    params = foward_pass(x_train, params)\n",
        "    \n",
        "  grads = backward_pass(x_train, y_train, params)\n",
        "\n",
        "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "\n",
        "  params = foward_pass(x_train, params)\n",
        "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
        "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
        "\n",
        "  params_test = foward_pass_test(x_test, params)\n",
        "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
        "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
        "\n",
        "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
        "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6Vq1FAnHLEB",
        "outputId": "989e2795-2630-4769-ed80-e45c708a5128"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: training loss = 2.302711, training acuracy = 10.11%, test loss = 2.302795, training acuracy = 9.75%\n",
            "Epoch 2: training loss = 2.302607, training acuracy = 10.27%, test loss = 2.302688, training acuracy = 9.88%\n",
            "Epoch 3: training loss = 2.302504, training acuracy = 10.46%, test loss = 2.302582, training acuracy = 10.02%\n",
            "Epoch 4: training loss = 2.3024, training acuracy = 10.76%, test loss = 2.302475, training acuracy = 10.29%\n",
            "Epoch 5: training loss = 2.302296, training acuracy = 11.14%, test loss = 2.302368, training acuracy = 10.65%\n",
            "Epoch 6: training loss = 2.302191, training acuracy = 11.73%, test loss = 2.30226, training acuracy = 11.17%\n",
            "Epoch 7: training loss = 2.302086, training acuracy = 12.42%, test loss = 2.302152, training acuracy = 11.74%\n",
            "Epoch 8: training loss = 2.301981, training acuracy = 13.23%, test loss = 2.302044, training acuracy = 12.5%\n",
            "Epoch 9: training loss = 2.301875, training acuracy = 14.12%, test loss = 2.301934, training acuracy = 13.51%\n",
            "Epoch 10: training loss = 2.301768, training acuracy = 14.94%, test loss = 2.301824, training acuracy = 14.43%\n",
            "Epoch 11: training loss = 2.30166, training acuracy = 15.74%, test loss = 2.301714, training acuracy = 15.25%\n",
            "Epoch 12: training loss = 2.301552, training acuracy = 16.41%, test loss = 2.301602, training acuracy = 15.98%\n",
            "Epoch 13: training loss = 2.301442, training acuracy = 17.03%, test loss = 2.301489, training acuracy = 16.59%\n",
            "Epoch 14: training loss = 2.301332, training acuracy = 17.55%, test loss = 2.301376, training acuracy = 16.96%\n",
            "Epoch 15: training loss = 2.30122, training acuracy = 17.98%, test loss = 2.301261, training acuracy = 17.51%\n",
            "Epoch 16: training loss = 2.301108, training acuracy = 18.38%, test loss = 2.301145, training acuracy = 17.94%\n",
            "Epoch 17: training loss = 2.300994, training acuracy = 18.68%, test loss = 2.301027, training acuracy = 18.19%\n",
            "Epoch 18: training loss = 2.300878, training acuracy = 18.88%, test loss = 2.300909, training acuracy = 18.45%\n",
            "Epoch 19: training loss = 2.300761, training acuracy = 19.03%, test loss = 2.300788, training acuracy = 18.69%\n",
            "Epoch 20: training loss = 2.300643, training acuracy = 19.15%, test loss = 2.300666, training acuracy = 18.79%\n",
            "Epoch 21: training loss = 2.300523, training acuracy = 19.26%, test loss = 2.300543, training acuracy = 18.82%\n",
            "Epoch 22: training loss = 2.300401, training acuracy = 19.32%, test loss = 2.300417, training acuracy = 18.89%\n",
            "Epoch 23: training loss = 2.300277, training acuracy = 19.37%, test loss = 2.30029, training acuracy = 18.97%\n",
            "Epoch 24: training loss = 2.300151, training acuracy = 19.42%, test loss = 2.30016, training acuracy = 19.01%\n",
            "Epoch 25: training loss = 2.300024, training acuracy = 19.46%, test loss = 2.300028, training acuracy = 19.1%\n",
            "Epoch 26: training loss = 2.299894, training acuracy = 19.5%, test loss = 2.299895, training acuracy = 19.12%\n",
            "Epoch 27: training loss = 2.299761, training acuracy = 19.59%, test loss = 2.299759, training acuracy = 19.16%\n",
            "Epoch 28: training loss = 2.299627, training acuracy = 19.74%, test loss = 2.29962, training acuracy = 19.31%\n",
            "Epoch 29: training loss = 2.29949, training acuracy = 20.13%, test loss = 2.299479, training acuracy = 19.67%\n",
            "Epoch 30: training loss = 2.29935, training acuracy = 20.67%, test loss = 2.299335, training acuracy = 20.17%\n",
            "Epoch 31: training loss = 2.299208, training acuracy = 21.35%, test loss = 2.299188, training acuracy = 21.06%\n",
            "Epoch 32: training loss = 2.299063, training acuracy = 22.13%, test loss = 2.299039, training acuracy = 21.92%\n",
            "Epoch 33: training loss = 2.298914, training acuracy = 22.99%, test loss = 2.298886, training acuracy = 22.86%\n",
            "Epoch 34: training loss = 2.298763, training acuracy = 23.96%, test loss = 2.298731, training acuracy = 23.83%\n",
            "Epoch 35: training loss = 2.298609, training acuracy = 24.96%, test loss = 2.298572, training acuracy = 24.6%\n",
            "Epoch 36: training loss = 2.298451, training acuracy = 25.95%, test loss = 2.298409, training acuracy = 25.59%\n",
            "Epoch 37: training loss = 2.29829, training acuracy = 26.88%, test loss = 2.298244, training acuracy = 26.41%\n",
            "Epoch 38: training loss = 2.298126, training acuracy = 27.79%, test loss = 2.298074, training acuracy = 27.24%\n",
            "Epoch 39: training loss = 2.297957, training acuracy = 28.69%, test loss = 2.297901, training acuracy = 28.09%\n",
            "Epoch 40: training loss = 2.297785, training acuracy = 29.55%, test loss = 2.297724, training acuracy = 28.84%\n",
            "Epoch 41: training loss = 2.29761, training acuracy = 30.39%, test loss = 2.297543, training acuracy = 29.46%\n",
            "Epoch 42: training loss = 2.29743, training acuracy = 31.12%, test loss = 2.297357, training acuracy = 30.24%\n",
            "Epoch 43: training loss = 2.297245, training acuracy = 31.76%, test loss = 2.297168, training acuracy = 30.82%\n",
            "Epoch 44: training loss = 2.297057, training acuracy = 32.33%, test loss = 2.296974, training acuracy = 31.42%\n",
            "Epoch 45: training loss = 2.296864, training acuracy = 32.89%, test loss = 2.296775, training acuracy = 32.0%\n",
            "Epoch 46: training loss = 2.296667, training acuracy = 33.4%, test loss = 2.296572, training acuracy = 32.61%\n",
            "Epoch 47: training loss = 2.296465, training acuracy = 33.89%, test loss = 2.296364, training acuracy = 33.07%\n",
            "Epoch 48: training loss = 2.296258, training acuracy = 34.32%, test loss = 2.296151, training acuracy = 33.48%\n",
            "Epoch 49: training loss = 2.296046, training acuracy = 34.96%, test loss = 2.295933, training acuracy = 34.26%\n",
            "Epoch 50: training loss = 2.295829, training acuracy = 36.14%, test loss = 2.29571, training acuracy = 35.58%\n",
            "Epoch 51: training loss = 2.295606, training acuracy = 37.8%, test loss = 2.295481, training acuracy = 37.28%\n",
            "Epoch 52: training loss = 2.295379, training acuracy = 39.52%, test loss = 2.295247, training acuracy = 39.14%\n",
            "Epoch 53: training loss = 2.295146, training acuracy = 41.06%, test loss = 2.295008, training acuracy = 40.75%\n",
            "Epoch 54: training loss = 2.294907, training acuracy = 42.37%, test loss = 2.294762, training acuracy = 42.05%\n",
            "Epoch 55: training loss = 2.294663, training acuracy = 43.53%, test loss = 2.294511, training acuracy = 43.36%\n",
            "Epoch 56: training loss = 2.294412, training acuracy = 44.56%, test loss = 2.294253, training acuracy = 44.45%\n",
            "Epoch 57: training loss = 2.294156, training acuracy = 45.45%, test loss = 2.29399, training acuracy = 45.22%\n",
            "Epoch 58: training loss = 2.293893, training acuracy = 46.19%, test loss = 2.29372, training acuracy = 45.94%\n",
            "Epoch 59: training loss = 2.293624, training acuracy = 46.87%, test loss = 2.293444, training acuracy = 46.53%\n",
            "Epoch 60: training loss = 2.293349, training acuracy = 47.48%, test loss = 2.293161, training acuracy = 47.25%\n",
            "Epoch 61: training loss = 2.293067, training acuracy = 48.02%, test loss = 2.292871, training acuracy = 47.83%\n",
            "Epoch 62: training loss = 2.292778, training acuracy = 48.47%, test loss = 2.292575, training acuracy = 48.43%\n",
            "Epoch 63: training loss = 2.292483, training acuracy = 48.94%, test loss = 2.292271, training acuracy = 48.93%\n",
            "Epoch 64: training loss = 2.292181, training acuracy = 49.36%, test loss = 2.291961, training acuracy = 49.34%\n",
            "Epoch 65: training loss = 2.291871, training acuracy = 49.74%, test loss = 2.291643, training acuracy = 49.85%\n",
            "Epoch 66: training loss = 2.291554, training acuracy = 50.1%, test loss = 2.291318, training acuracy = 50.2%\n",
            "Epoch 67: training loss = 2.29123, training acuracy = 50.48%, test loss = 2.290986, training acuracy = 50.67%\n",
            "Epoch 68: training loss = 2.290899, training acuracy = 50.82%, test loss = 2.290645, training acuracy = 50.9%\n",
            "Epoch 69: training loss = 2.29056, training acuracy = 51.25%, test loss = 2.290298, training acuracy = 51.36%\n",
            "Epoch 70: training loss = 2.290213, training acuracy = 51.73%, test loss = 2.289942, training acuracy = 52.01%\n",
            "Epoch 71: training loss = 2.289858, training acuracy = 52.17%, test loss = 2.289578, training acuracy = 52.4%\n",
            "Epoch 72: training loss = 2.289496, training acuracy = 52.57%, test loss = 2.289207, training acuracy = 52.77%\n",
            "Epoch 73: training loss = 2.289125, training acuracy = 53.01%, test loss = 2.288827, training acuracy = 53.27%\n",
            "Epoch 74: training loss = 2.288747, training acuracy = 53.41%, test loss = 2.288439, training acuracy = 53.64%\n",
            "Epoch 75: training loss = 2.28836, training acuracy = 53.81%, test loss = 2.288042, training acuracy = 54.21%\n",
            "Epoch 76: training loss = 2.287964, training acuracy = 54.23%, test loss = 2.287637, training acuracy = 54.55%\n",
            "Epoch 77: training loss = 2.28756, training acuracy = 54.55%, test loss = 2.287223, training acuracy = 54.98%\n",
            "Epoch 78: training loss = 2.287148, training acuracy = 54.86%, test loss = 2.286801, training acuracy = 55.52%\n",
            "Epoch 79: training loss = 2.286727, training acuracy = 55.27%, test loss = 2.28637, training acuracy = 55.91%\n",
            "Epoch 80: training loss = 2.286297, training acuracy = 55.58%, test loss = 2.28593, training acuracy = 56.21%\n",
            "Epoch 81: training loss = 2.285858, training acuracy = 55.93%, test loss = 2.285481, training acuracy = 56.57%\n",
            "Epoch 82: training loss = 2.285411, training acuracy = 56.28%, test loss = 2.285023, training acuracy = 57.07%\n",
            "Epoch 83: training loss = 2.284954, training acuracy = 56.6%, test loss = 2.284556, training acuracy = 57.32%\n",
            "Epoch 84: training loss = 2.284488, training acuracy = 56.92%, test loss = 2.28408, training acuracy = 57.66%\n",
            "Epoch 85: training loss = 2.284014, training acuracy = 57.25%, test loss = 2.283595, training acuracy = 58.02%\n",
            "Epoch 86: training loss = 2.283529, training acuracy = 57.49%, test loss = 2.2831, training acuracy = 58.13%\n",
            "Epoch 87: training loss = 2.283036, training acuracy = 57.72%, test loss = 2.282596, training acuracy = 58.36%\n",
            "Epoch 88: training loss = 2.282533, training acuracy = 57.92%, test loss = 2.282082, training acuracy = 58.57%\n",
            "Epoch 89: training loss = 2.282021, training acuracy = 58.13%, test loss = 2.281559, training acuracy = 58.67%\n",
            "Epoch 90: training loss = 2.2815, training acuracy = 58.28%, test loss = 2.281027, training acuracy = 58.86%\n",
            "Epoch 91: training loss = 2.280969, training acuracy = 58.48%, test loss = 2.280485, training acuracy = 59.03%\n",
            "Epoch 92: training loss = 2.280429, training acuracy = 58.64%, test loss = 2.279934, training acuracy = 59.19%\n",
            "Epoch 93: training loss = 2.279879, training acuracy = 58.78%, test loss = 2.279372, training acuracy = 59.29%\n",
            "Epoch 94: training loss = 2.279319, training acuracy = 58.9%, test loss = 2.278802, training acuracy = 59.46%\n",
            "Epoch 95: training loss = 2.27875, training acuracy = 59.05%, test loss = 2.278222, training acuracy = 59.55%\n",
            "Epoch 96: training loss = 2.278172, training acuracy = 59.16%, test loss = 2.277632, training acuracy = 59.62%\n",
            "Epoch 97: training loss = 2.277583, training acuracy = 59.2%, test loss = 2.277032, training acuracy = 59.6%\n",
            "Epoch 98: training loss = 2.276986, training acuracy = 59.3%, test loss = 2.276423, training acuracy = 59.77%\n",
            "Epoch 99: training loss = 2.276378, training acuracy = 59.36%, test loss = 2.275805, training acuracy = 59.89%\n",
            "Epoch 100: training loss = 2.275762, training acuracy = 59.45%, test loss = 2.275176, training acuracy = 59.98%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ux7mPf6E78d4"
      },
      "outputs": [],
      "source": [
        "# Final training accuracy: 59.98%\n",
        "\n",
        "# 1) Forward pass, backward pass에는 sigmoid, d_sigmoid만 사용. 즉, 변화 없음.\n",
        "# (왜인지는 모르겠지만 ReLU를 사용하면 overfitting이 너무 심해서 중간에 정확도가 도로 하락함)\n",
        "\n",
        "# 2) Hidden layer node size 변경\n",
        "# 종전: 128 & 64 -> 변경: 64 & 100\n",
        "\n",
        "# 정확도가 6%p 상승함."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RF2JW3vVNVWm"
      },
      "execution_count": 109,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "[0818]_Deep_Learning_Basic_과제.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}