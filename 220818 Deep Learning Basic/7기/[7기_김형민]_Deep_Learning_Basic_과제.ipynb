{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[7기 김형민]_Deep_Learning_Basic_과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "bndPSLglmFv3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "source": [
        "def relu(x):\n",
        "  return(np.maximum(0,x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_relu(x):\n",
        "  x = np.maximum(0,x)\n",
        "  \n",
        "  if x > 0 :\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = sigmoid(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params"
      ],
      "metadata": {
        "id": "FjZXd42T_0kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "\n",
        "  dS3 = params[\"A3\"] - y_true\n",
        "\n",
        "  grads = {}\n",
        "\n",
        "  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "  grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "  grads[\"dW2\"] = np.dot(dS2, x.T)/x.shape[1]\n",
        "  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "  return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxJO249A3jhk"
      },
      "source": [
        "# Assignment 3 구현은 여기서 ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "w83JXbt-B5xS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "-nTJ_Dl1CDbf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ],
      "metadata": {
        "id": "kG6bsqVxCKtW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "tC1DGZ8VCMzQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "cMhxCyI2CT-I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlh04T6sCpbO",
        "outputId": "f1319a3f-c4e4-41e1-8765-96c06393db4d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n",
        "                         # require_true 얘는 학습되는 애구나 알 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTN6q5yDC1t8",
        "outputId": "5db19535-e5aa-4547-b12c-f2663340a130"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0215, -0.0170, -0.0219,  ..., -0.0127, -0.0134, -0.0280],\n",
              "         [-0.0331, -0.0222,  0.0274,  ..., -0.0044,  0.0231,  0.0206],\n",
              "         [-0.0243,  0.0201, -0.0031,  ...,  0.0257, -0.0089, -0.0073],\n",
              "         ...,\n",
              "         [-0.0304,  0.0011,  0.0344,  ..., -0.0203,  0.0129, -0.0159],\n",
              "         [ 0.0197,  0.0098,  0.0253,  ...,  0.0134, -0.0252,  0.0231],\n",
              "         [-0.0239, -0.0064,  0.0247,  ..., -0.0032, -0.0143,  0.0107]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0024, -0.0061, -0.0159, -0.0241, -0.0247, -0.0356, -0.0003,  0.0096,\n",
              "          0.0102, -0.0121, -0.0142, -0.0266, -0.0295,  0.0263, -0.0253, -0.0082,\n",
              "          0.0179,  0.0345, -0.0144,  0.0356,  0.0055, -0.0272, -0.0312, -0.0056,\n",
              "         -0.0352, -0.0043, -0.0214,  0.0311, -0.0288, -0.0297, -0.0075,  0.0323,\n",
              "          0.0074,  0.0138, -0.0167,  0.0166,  0.0193, -0.0270, -0.0285, -0.0141,\n",
              "          0.0032, -0.0115, -0.0064,  0.0252, -0.0177, -0.0025,  0.0096, -0.0190,\n",
              "          0.0280,  0.0061,  0.0114,  0.0149, -0.0167, -0.0295,  0.0293,  0.0281,\n",
              "          0.0120, -0.0298,  0.0309,  0.0201,  0.0100, -0.0230, -0.0211,  0.0081,\n",
              "         -0.0211, -0.0097, -0.0024,  0.0163,  0.0291,  0.0205, -0.0168, -0.0264,\n",
              "          0.0007, -0.0252,  0.0351,  0.0016,  0.0320, -0.0165,  0.0242,  0.0353,\n",
              "         -0.0354,  0.0140, -0.0252,  0.0303, -0.0224, -0.0059,  0.0036,  0.0111,\n",
              "         -0.0286, -0.0189, -0.0101,  0.0287,  0.0267,  0.0060,  0.0191, -0.0167,\n",
              "         -0.0334, -0.0258,  0.0053, -0.0219,  0.0131,  0.0226,  0.0307, -0.0055,\n",
              "         -0.0108,  0.0022,  0.0171,  0.0278,  0.0113,  0.0257,  0.0168,  0.0001,\n",
              "          0.0142,  0.0316,  0.0168,  0.0321,  0.0310, -0.0090,  0.0175, -0.0071,\n",
              "         -0.0054, -0.0277, -0.0218,  0.0237,  0.0303, -0.0278, -0.0063,  0.0346],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 0.0418,  0.0186,  0.0234,  ...,  0.0534,  0.0870,  0.0165],\n",
              "         [-0.0089,  0.0272,  0.0091,  ..., -0.0173, -0.0472,  0.0145],\n",
              "         [-0.0815, -0.0217,  0.0126,  ..., -0.0129, -0.0236, -0.0674],\n",
              "         ...,\n",
              "         [ 0.0072, -0.0366, -0.0451,  ..., -0.0797,  0.0789, -0.0659],\n",
              "         [-0.0689,  0.0192, -0.0624,  ..., -0.0825,  0.0626, -0.0026],\n",
              "         [-0.0129, -0.0104,  0.0293,  ...,  0.0030,  0.0703,  0.0121]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0381, -0.0777, -0.0649,  0.0433,  0.0784, -0.0665, -0.0498, -0.0116,\n",
              "         -0.0675,  0.0217,  0.0692, -0.0437, -0.0550, -0.0444,  0.0016, -0.0104,\n",
              "          0.0735, -0.0780, -0.0274, -0.0865,  0.0351,  0.0085,  0.0623,  0.0163,\n",
              "          0.0091,  0.0141,  0.0582,  0.0789, -0.0823, -0.0681,  0.0664,  0.0306,\n",
              "         -0.0066, -0.0337, -0.0569,  0.0568, -0.0023,  0.0302,  0.0483,  0.0388,\n",
              "          0.0699, -0.0103,  0.0599, -0.0166, -0.0643,  0.0411,  0.0032, -0.0727,\n",
              "         -0.0101,  0.0057,  0.0673,  0.0429, -0.0452, -0.0664, -0.0016, -0.0328,\n",
              "          0.0255,  0.0810, -0.0053, -0.0215,  0.0373, -0.0269,  0.0828,  0.0769],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([[ 3.8261e-02,  4.7611e-03,  3.7654e-02,  7.3952e-02, -6.7665e-02,\n",
              "          -1.2365e-01,  1.4670e-02,  1.5025e-02,  1.1579e-01,  5.1430e-02,\n",
              "          -7.1551e-02, -5.5579e-02,  1.1863e-01, -1.5300e-02, -1.0881e-01,\n",
              "           9.0486e-02, -7.9013e-03, -4.1770e-02, -1.0118e-01,  4.5263e-02,\n",
              "           1.0348e-01, -9.0918e-02, -1.1889e-01,  4.0235e-02,  1.1252e-01,\n",
              "           3.1766e-03,  6.7299e-02, -9.5304e-02, -8.4584e-03,  1.0660e-01,\n",
              "          -8.5042e-02,  5.6394e-03, -1.5245e-02, -8.2870e-02, -1.7966e-03,\n",
              "          -2.9973e-02, -1.0750e-01,  1.0446e-01, -6.3965e-02,  9.9246e-02,\n",
              "           6.0223e-02,  1.0112e-01, -4.7094e-02, -2.5072e-02,  3.1405e-02,\n",
              "          -4.8426e-02,  1.1117e-01,  7.6202e-02,  2.6301e-02,  6.0492e-02,\n",
              "          -7.9702e-02,  2.2736e-02, -1.1407e-01, -4.6529e-02, -1.1746e-01,\n",
              "           6.9800e-02, -5.0701e-02,  9.2891e-02, -4.2140e-05,  4.4007e-02,\n",
              "           1.0470e-01,  8.3694e-02,  6.4181e-02,  1.2227e-01],\n",
              "         [ 4.2854e-02, -6.3273e-02, -4.3032e-02, -1.7071e-02, -5.7984e-02,\n",
              "          -1.1660e-02,  1.0307e-01,  1.4864e-02,  1.1828e-01, -1.1013e-01,\n",
              "           2.5671e-02,  4.6209e-02, -8.6350e-02, -4.2530e-02, -2.4743e-02,\n",
              "           8.0030e-02,  1.1307e-01, -1.2355e-01,  1.1419e-01, -1.0732e-01,\n",
              "          -1.0855e-01, -1.0878e-01, -5.7473e-02,  1.0318e-01,  4.3981e-03,\n",
              "          -1.9822e-02,  1.0107e-01,  9.7283e-02,  7.5976e-02,  1.2952e-02,\n",
              "          -3.0106e-02,  7.4464e-02,  1.0690e-01, -1.2402e-01,  8.0879e-02,\n",
              "          -3.5633e-02, -4.5253e-03, -3.5989e-03, -5.0847e-02,  6.5833e-02,\n",
              "           9.3713e-02,  2.8433e-02,  1.2188e-01,  1.1745e-01,  6.5757e-02,\n",
              "          -6.1904e-02, -3.7083e-02,  1.2127e-01,  6.8159e-02, -9.8140e-02,\n",
              "          -1.0027e-01, -3.0787e-02,  6.5252e-02, -6.3570e-02, -7.7286e-03,\n",
              "          -7.7093e-02, -6.6670e-02, -2.1476e-03, -1.7320e-02,  6.8974e-02,\n",
              "           6.1426e-02, -2.7954e-02,  1.3945e-02,  1.1080e-01],\n",
              "         [ 3.5250e-02, -7.3248e-02, -6.2296e-02,  7.5123e-03,  2.1045e-02,\n",
              "          -2.5010e-02,  8.7204e-02,  1.0168e-01, -3.4887e-02, -1.1893e-01,\n",
              "          -1.1663e-01, -5.1470e-02,  9.8603e-02, -1.0123e-01,  5.9252e-02,\n",
              "           1.2114e-01,  3.7738e-02, -9.6091e-02, -9.2448e-02, -3.1498e-02,\n",
              "          -1.2303e-01,  1.2393e-01,  2.1452e-02, -1.1290e-01, -4.6673e-02,\n",
              "          -4.2458e-02, -3.3253e-02, -8.3593e-02,  3.1153e-02,  4.0777e-02,\n",
              "          -9.0867e-02,  6.0009e-02, -4.3503e-02,  8.3990e-02,  7.2275e-02,\n",
              "           7.0014e-02, -1.8256e-02, -6.4068e-02, -6.6587e-02,  4.1624e-02,\n",
              "          -1.0294e-01, -9.6886e-02, -1.0264e-01, -1.0985e-01, -7.0064e-02,\n",
              "          -1.0760e-01, -1.6886e-02, -2.7426e-02, -5.2963e-02,  9.7345e-03,\n",
              "          -4.1894e-02,  5.0770e-02,  2.6188e-02,  2.3339e-02,  5.5572e-02,\n",
              "           9.9778e-02,  7.7890e-02, -7.7268e-02, -8.9763e-02,  9.3930e-02,\n",
              "          -5.6043e-02, -1.2851e-02,  1.7400e-02, -1.1440e-01],\n",
              "         [-9.9150e-02,  1.9960e-02, -1.1575e-01,  1.3111e-02, -8.8846e-02,\n",
              "           1.1799e-01, -6.0022e-02,  8.7159e-02,  1.2025e-01,  9.6971e-02,\n",
              "          -1.1373e-01, -4.8697e-02,  1.2381e-01, -8.3200e-02, -4.0539e-02,\n",
              "          -1.1774e-01, -9.9690e-02,  4.2134e-02,  1.1004e-01, -7.1950e-02,\n",
              "           6.2668e-02, -6.4702e-03, -5.8513e-02, -6.9101e-03,  6.7737e-02,\n",
              "           1.9227e-02,  1.0985e-02,  5.3274e-02, -2.7359e-02,  1.1690e-01,\n",
              "          -7.8640e-02,  8.2761e-02,  9.9329e-02,  5.4723e-02, -1.1399e-01,\n",
              "           9.5050e-02,  9.9405e-02,  1.7798e-02, -2.1885e-02, -5.9440e-03,\n",
              "          -9.5504e-02, -1.1871e-01, -8.0765e-02,  8.9306e-02, -9.9750e-02,\n",
              "          -1.0609e-01, -1.1089e-01, -1.0152e-01,  8.1862e-02,  7.7870e-02,\n",
              "          -4.4763e-02,  8.8199e-02, -1.1895e-02, -9.1058e-02, -1.1914e-01,\n",
              "           5.6775e-02, -8.1886e-02,  9.4126e-02, -5.4612e-02,  1.1984e-01,\n",
              "          -4.4195e-02,  1.3038e-03, -5.4865e-02,  7.0598e-02],\n",
              "         [-6.6012e-02,  7.2316e-02,  1.2126e-01,  7.9272e-02,  5.2643e-02,\n",
              "           1.0466e-01, -1.9219e-03, -3.2108e-02, -7.5314e-02, -4.6915e-02,\n",
              "           1.0153e-01,  1.2229e-01, -3.1376e-02, -8.1814e-02, -2.6444e-02,\n",
              "           7.6904e-02,  7.7862e-02, -7.6152e-02, -5.0958e-02,  5.5690e-02,\n",
              "           1.7000e-02, -4.6458e-02,  8.6163e-02,  6.3195e-02, -1.6925e-02,\n",
              "           1.4112e-02, -8.9597e-02, -4.1906e-02, -1.2281e-01, -6.0817e-02,\n",
              "           5.7046e-04, -5.1216e-02, -2.2523e-02,  1.2116e-01, -2.0933e-02,\n",
              "           1.2100e-01, -5.7055e-03, -7.7233e-02,  8.7275e-02,  2.2229e-02,\n",
              "          -9.2790e-02,  6.5570e-04,  4.7592e-03, -7.2269e-02,  3.7820e-02,\n",
              "           4.0065e-02,  3.9194e-02, -4.7837e-02, -1.0046e-01, -1.1562e-01,\n",
              "          -9.4214e-02,  9.9426e-02, -6.3782e-02, -9.4836e-02,  1.1353e-01,\n",
              "          -7.5634e-02, -6.3855e-02,  1.0026e-01, -7.6829e-02,  1.0849e-01,\n",
              "           8.5116e-03,  3.9481e-02,  5.1697e-02,  5.1314e-02],\n",
              "         [-6.6902e-02,  1.0383e-01, -1.3080e-02, -6.9435e-02, -1.0388e-01,\n",
              "          -4.2924e-02, -3.3756e-02,  7.0605e-02, -1.9841e-02,  7.4055e-02,\n",
              "          -1.1960e-01,  3.7309e-02, -1.1531e-01,  5.5510e-02,  3.4162e-02,\n",
              "           4.4627e-02, -5.5625e-02, -4.7749e-02, -1.0303e-01,  9.3773e-02,\n",
              "          -4.5290e-02,  6.3868e-02,  6.0396e-02, -1.6652e-03, -6.1782e-02,\n",
              "           7.5511e-02, -2.1793e-02,  7.6497e-02,  5.3909e-02, -6.0416e-03,\n",
              "          -7.3975e-04,  1.9920e-02, -1.1956e-01,  9.4994e-02, -4.7343e-02,\n",
              "          -1.0276e-01,  4.0032e-02, -8.0394e-02,  8.4376e-02,  1.0463e-01,\n",
              "          -1.5260e-02, -1.0635e-01,  9.7689e-02,  2.2835e-03,  7.3896e-02,\n",
              "          -5.0429e-02, -8.7291e-02,  9.1299e-02, -4.1109e-02,  6.7745e-02,\n",
              "          -7.0468e-02, -1.2426e-01,  5.3302e-02,  1.6459e-02, -8.7900e-02,\n",
              "           9.5372e-02, -4.8870e-02,  1.2476e-01,  4.9505e-02, -7.3218e-02,\n",
              "          -1.4395e-02,  6.0844e-02, -2.9232e-02, -8.0443e-02],\n",
              "         [-8.9242e-02,  1.2246e-01,  1.9793e-02, -1.1759e-01, -9.4319e-02,\n",
              "          -9.1753e-02,  6.3999e-02,  7.3312e-02, -2.2303e-02,  5.2998e-02,\n",
              "          -1.0741e-01,  3.2034e-02,  2.4221e-02,  1.1470e-02,  5.6046e-02,\n",
              "          -1.7417e-02, -4.3510e-02, -2.0896e-02, -4.0261e-02, -2.6942e-02,\n",
              "          -5.0802e-02,  1.0787e-01, -1.0655e-01, -6.7393e-02,  1.2360e-01,\n",
              "          -8.9630e-02, -7.9293e-02, -7.9715e-02,  8.5581e-02,  7.9241e-02,\n",
              "          -1.1183e-01, -1.0055e-01,  3.4363e-03,  1.2218e-01, -9.3901e-02,\n",
              "          -1.2491e-01,  5.0389e-02,  5.1267e-02,  7.4429e-02,  1.0136e-01,\n",
              "           7.3069e-02,  6.4297e-02, -8.6737e-02,  3.1295e-03,  9.7029e-02,\n",
              "          -1.1344e-02, -1.0555e-01, -5.8684e-02,  1.1289e-01,  9.8095e-02,\n",
              "           1.9048e-03,  1.1680e-01,  1.0231e-01,  4.1439e-02,  9.9133e-02,\n",
              "           8.9496e-02, -3.1688e-02, -4.4656e-03,  5.5219e-02, -9.0498e-02,\n",
              "          -8.3466e-02,  8.7871e-02,  5.9678e-02,  9.0559e-02],\n",
              "         [-4.3661e-02,  1.1061e-01, -1.5544e-02,  8.1439e-02,  5.2227e-02,\n",
              "           2.1884e-02,  1.4692e-02,  9.4714e-02, -6.3371e-02,  9.4984e-02,\n",
              "           2.1436e-02, -1.0376e-01,  1.4393e-03, -2.9672e-02, -8.5292e-02,\n",
              "          -5.8992e-02, -2.0177e-02,  7.2148e-02, -6.4974e-02,  7.6971e-02,\n",
              "           9.7587e-02, -7.2726e-02,  3.4095e-02, -1.0030e-01, -9.3219e-02,\n",
              "           3.0289e-02, -5.7135e-02,  5.3821e-02, -6.7091e-02,  6.7851e-02,\n",
              "          -1.0936e-01, -7.1673e-02, -4.6493e-02, -9.8520e-02,  9.3761e-02,\n",
              "          -1.0343e-01,  1.0829e-01, -6.6715e-02,  5.5283e-02, -1.1081e-01,\n",
              "           8.8751e-02, -4.6027e-02,  9.8130e-02, -6.8775e-02,  6.5703e-02,\n",
              "          -7.7769e-02, -3.2762e-03,  8.5017e-02,  6.0446e-02,  1.0942e-02,\n",
              "           9.0302e-02, -2.4130e-02,  1.1254e-01, -9.7183e-02,  1.3062e-02,\n",
              "           9.7478e-02, -3.5063e-02,  3.8117e-02,  5.5577e-02, -5.9185e-02,\n",
              "          -2.9553e-02,  8.1372e-02,  2.5003e-02,  4.1127e-02],\n",
              "         [ 5.0882e-02, -1.2427e-01,  4.8443e-02,  3.3472e-02,  5.0412e-02,\n",
              "          -6.1724e-02, -4.3765e-02, -3.8107e-02, -3.5031e-02,  1.1908e-01,\n",
              "           1.1762e-01, -1.0097e-01,  2.5542e-02, -1.0108e-01,  3.2063e-02,\n",
              "           9.8713e-02,  1.2077e-01,  2.7531e-02, -5.5728e-02, -5.8789e-03,\n",
              "           1.0607e-01, -4.9709e-02, -7.6945e-02,  1.9743e-02, -6.7064e-02,\n",
              "          -4.0103e-02, -4.0171e-02, -4.3461e-02, -2.9828e-02, -1.1311e-01,\n",
              "          -8.6890e-02,  1.0048e-01, -2.2882e-02, -5.6866e-03, -1.0405e-02,\n",
              "           1.9549e-02,  1.7066e-02, -1.0390e-01, -7.1812e-02,  5.9245e-02,\n",
              "           5.4783e-02, -1.0426e-02, -2.2679e-02, -1.0990e-02, -4.3647e-02,\n",
              "          -4.8774e-03,  1.1476e-01, -8.9128e-02, -6.3769e-02, -6.9237e-02,\n",
              "          -1.2420e-01,  4.5510e-02,  8.3364e-03,  8.4055e-02,  7.4293e-02,\n",
              "          -1.3489e-03,  6.4780e-02,  3.1746e-02,  1.0969e-01, -9.6253e-02,\n",
              "          -1.1798e-01, -1.1676e-01,  9.2047e-02,  7.7344e-02],\n",
              "         [ 7.0016e-02, -8.6260e-02,  1.0219e-01, -1.2952e-02,  4.0451e-02,\n",
              "          -7.0547e-02, -9.5054e-02,  9.1031e-02,  9.6135e-02,  3.6185e-02,\n",
              "          -8.5174e-02,  1.2025e-01,  4.8135e-02, -1.9979e-02, -9.4700e-02,\n",
              "          -8.3569e-03, -8.5325e-02,  8.7261e-02,  1.0692e-01,  1.1758e-01,\n",
              "           1.0893e-01, -8.6847e-02,  4.3172e-02,  2.0279e-02,  5.5011e-02,\n",
              "           1.0420e-01,  5.1207e-02, -8.7939e-02,  5.7785e-03, -1.5949e-02,\n",
              "           6.0723e-02,  8.3049e-02, -8.2703e-02, -9.0435e-03, -6.7532e-02,\n",
              "          -4.8424e-02, -6.5712e-02, -1.7607e-02,  2.2536e-02,  1.5251e-03,\n",
              "          -8.7772e-02, -5.2026e-02,  3.5100e-02,  4.0110e-02,  4.5574e-02,\n",
              "          -1.1401e-01,  5.1203e-03, -2.4077e-02, -8.6184e-02,  8.4724e-02,\n",
              "          -1.2295e-01, -6.5307e-02, -9.9707e-02, -7.3736e-02,  2.8365e-02,\n",
              "          -3.7988e-02, -5.2460e-02,  8.5743e-02, -5.7293e-02, -6.0971e-02,\n",
              "           1.2481e-02,  8.3733e-02,  1.2372e-02, -1.0887e-01]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([ 0.0441,  0.1249, -0.0224, -0.0054, -0.0130, -0.0675,  0.0917, -0.0462,\n",
              "         -0.0297,  0.0884], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.5)"
      ],
      "metadata": {
        "id": "QfY2Uw0hCuJb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "    # 배치 당 loss 값을 담을 리스트 생성\n",
        "    batch_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        # 옵티마이저의 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # y pred 값 산출\n",
        "        output = model(data)\n",
        "        # loss 계산\n",
        "        # 정답 데이터와의 cross entropy loss 계산\n",
        "        # 이 loss를 배치 당 loss로 보관\n",
        "        loss = criterion(output, target)\n",
        "        batch_losses.append(loss)\n",
        "\n",
        "        # 기울기 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 가중치 업데이트!\n",
        "        optimizer.step()\n",
        "        \n",
        "    # 배치당 평균 loss 계산\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    \n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "psgONOB-DEU6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    batch_losses = []\n",
        "    correct = 0 \n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            # 예측값 생성\n",
        "            output = model(data)\n",
        "\n",
        "            # loss 계산 (이전과 동일)\n",
        "            loss = criterion(output, target)\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "           # Accuracy 계산\n",
        "           # y pred와 y가 일치하면 correct에 1을 더해주기\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # 배치 당 평균 loss 계산 \n",
        "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    #정확도 계산\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "A6VMhz9rDae2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia792wg8DYJE",
        "outputId": "32dad992-64f5-4e98-de7a-741ff203c9ef"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.7955\tTest Loss: 0.2814\tAccuracy: 91.49%\n",
            "[2] Train Loss: 0.2360\tTest Loss: 0.1890\tAccuracy: 94.18%\n",
            "[3] Train Loss: 0.1646\tTest Loss: 0.1554\tAccuracy: 95.03%\n",
            "[4] Train Loss: 0.1310\tTest Loss: 0.1586\tAccuracy: 95.06%\n",
            "[5] Train Loss: 0.1080\tTest Loss: 0.2659\tAccuracy: 91.72%\n",
            "[6] Train Loss: 0.2214\tTest Loss: 0.1540\tAccuracy: 95.11%\n",
            "[7] Train Loss: 0.0964\tTest Loss: 0.1077\tAccuracy: 96.69%\n",
            "[8] Train Loss: 0.0806\tTest Loss: 0.1049\tAccuracy: 96.65%\n",
            "[9] Train Loss: 0.0695\tTest Loss: 0.1000\tAccuracy: 96.91%\n",
            "[10] Train Loss: 0.0611\tTest Loss: 0.0932\tAccuracy: 96.91%\n",
            "[11] Train Loss: 0.0544\tTest Loss: 0.1058\tAccuracy: 96.79%\n",
            "[12] Train Loss: 0.0491\tTest Loss: 0.2320\tAccuracy: 93.76%\n",
            "[13] Train Loss: 0.0467\tTest Loss: 0.0863\tAccuracy: 97.29%\n",
            "[14] Train Loss: 0.0383\tTest Loss: 0.0746\tAccuracy: 97.60%\n",
            "[15] Train Loss: 0.0360\tTest Loss: 0.0795\tAccuracy: 97.58%\n",
            "[16] Train Loss: 0.0319\tTest Loss: 0.0743\tAccuracy: 97.72%\n",
            "[17] Train Loss: 0.0288\tTest Loss: 0.0942\tAccuracy: 97.39%\n",
            "[18] Train Loss: 0.0258\tTest Loss: 0.1191\tAccuracy: 96.73%\n",
            "[19] Train Loss: 0.0247\tTest Loss: 0.0829\tAccuracy: 97.54%\n",
            "[20] Train Loss: 0.0208\tTest Loss: 0.0703\tAccuracy: 97.83%\n",
            "[21] Train Loss: 0.0184\tTest Loss: 0.0858\tAccuracy: 97.48%\n",
            "[22] Train Loss: 0.0160\tTest Loss: 0.0797\tAccuracy: 97.74%\n",
            "[23] Train Loss: 0.0153\tTest Loss: 0.0958\tAccuracy: 97.24%\n",
            "[24] Train Loss: 0.0134\tTest Loss: 0.0726\tAccuracy: 97.92%\n",
            "[25] Train Loss: 0.0111\tTest Loss: 0.0726\tAccuracy: 97.84%\n",
            "[26] Train Loss: 0.0101\tTest Loss: 0.0784\tAccuracy: 97.79%\n",
            "[27] Train Loss: 0.0089\tTest Loss: 0.0770\tAccuracy: 97.73%\n",
            "[28] Train Loss: 0.0082\tTest Loss: 0.1236\tAccuracy: 96.65%\n",
            "[29] Train Loss: 0.0074\tTest Loss: 0.0792\tAccuracy: 97.69%\n",
            "[30] Train Loss: 0.0067\tTest Loss: 0.0878\tAccuracy: 97.64%\n",
            "[31] Train Loss: 0.0056\tTest Loss: 0.0766\tAccuracy: 97.95%\n",
            "[32] Train Loss: 0.0050\tTest Loss: 0.0849\tAccuracy: 97.76%\n",
            "[33] Train Loss: 0.0045\tTest Loss: 0.0756\tAccuracy: 97.97%\n",
            "[34] Train Loss: 0.0041\tTest Loss: 0.0854\tAccuracy: 97.67%\n",
            "[35] Train Loss: 0.0037\tTest Loss: 0.0801\tAccuracy: 97.83%\n",
            "[36] Train Loss: 0.0034\tTest Loss: 0.0773\tAccuracy: 97.97%\n",
            "[37] Train Loss: 0.0031\tTest Loss: 0.0813\tAccuracy: 97.92%\n",
            "[38] Train Loss: 0.0028\tTest Loss: 0.0806\tAccuracy: 97.92%\n",
            "[39] Train Loss: 0.0026\tTest Loss: 0.0811\tAccuracy: 97.92%\n",
            "[40] Train Loss: 0.0024\tTest Loss: 0.0852\tAccuracy: 97.77%\n",
            "[41] Train Loss: 0.0024\tTest Loss: 0.0804\tAccuracy: 97.92%\n",
            "[42] Train Loss: 0.0022\tTest Loss: 0.0835\tAccuracy: 97.93%\n",
            "[43] Train Loss: 0.0021\tTest Loss: 0.0812\tAccuracy: 97.95%\n",
            "[44] Train Loss: 0.0019\tTest Loss: 0.0826\tAccuracy: 97.88%\n",
            "[45] Train Loss: 0.0018\tTest Loss: 0.0836\tAccuracy: 97.89%\n",
            "[46] Train Loss: 0.0017\tTest Loss: 0.0849\tAccuracy: 97.87%\n",
            "[47] Train Loss: 0.0017\tTest Loss: 0.0833\tAccuracy: 97.96%\n",
            "[48] Train Loss: 0.0016\tTest Loss: 0.0846\tAccuracy: 97.99%\n",
            "[49] Train Loss: 0.0015\tTest Loss: 0.0852\tAccuracy: 97.93%\n",
            "[50] Train Loss: 0.0014\tTest Loss: 0.0856\tAccuracy: 97.93%\n",
            "[51] Train Loss: 0.0014\tTest Loss: 0.0854\tAccuracy: 97.93%\n",
            "[52] Train Loss: 0.0013\tTest Loss: 0.0864\tAccuracy: 97.97%\n",
            "[53] Train Loss: 0.0012\tTest Loss: 0.0863\tAccuracy: 97.94%\n",
            "[54] Train Loss: 0.0012\tTest Loss: 0.0872\tAccuracy: 97.98%\n",
            "[55] Train Loss: 0.0011\tTest Loss: 0.0867\tAccuracy: 97.92%\n",
            "[56] Train Loss: 0.0011\tTest Loss: 0.0902\tAccuracy: 97.85%\n",
            "[57] Train Loss: 0.0011\tTest Loss: 0.0870\tAccuracy: 97.94%\n",
            "[58] Train Loss: 0.0010\tTest Loss: 0.0889\tAccuracy: 97.92%\n",
            "[59] Train Loss: 0.0010\tTest Loss: 0.0888\tAccuracy: 97.96%\n",
            "[60] Train Loss: 0.0009\tTest Loss: 0.0911\tAccuracy: 97.97%\n",
            "[61] Train Loss: 0.0009\tTest Loss: 0.0888\tAccuracy: 97.97%\n",
            "[62] Train Loss: 0.0009\tTest Loss: 0.0900\tAccuracy: 97.98%\n",
            "[63] Train Loss: 0.0008\tTest Loss: 0.0912\tAccuracy: 97.95%\n",
            "[64] Train Loss: 0.0008\tTest Loss: 0.0922\tAccuracy: 97.91%\n",
            "[65] Train Loss: 0.0008\tTest Loss: 0.0900\tAccuracy: 97.92%\n",
            "[66] Train Loss: 0.0008\tTest Loss: 0.0917\tAccuracy: 97.93%\n",
            "[67] Train Loss: 0.0007\tTest Loss: 0.0930\tAccuracy: 97.96%\n",
            "[68] Train Loss: 0.0007\tTest Loss: 0.0906\tAccuracy: 97.93%\n",
            "[69] Train Loss: 0.0007\tTest Loss: 0.0917\tAccuracy: 97.96%\n",
            "[70] Train Loss: 0.0007\tTest Loss: 0.0934\tAccuracy: 97.96%\n",
            "[71] Train Loss: 0.0007\tTest Loss: 0.0910\tAccuracy: 97.95%\n",
            "[72] Train Loss: 0.0007\tTest Loss: 0.0917\tAccuracy: 97.92%\n",
            "[73] Train Loss: 0.0006\tTest Loss: 0.0920\tAccuracy: 97.96%\n",
            "[74] Train Loss: 0.0006\tTest Loss: 0.0918\tAccuracy: 97.96%\n",
            "[75] Train Loss: 0.0006\tTest Loss: 0.0946\tAccuracy: 97.97%\n",
            "[76] Train Loss: 0.0006\tTest Loss: 0.0941\tAccuracy: 97.97%\n",
            "[77] Train Loss: 0.0006\tTest Loss: 0.0932\tAccuracy: 97.96%\n",
            "[78] Train Loss: 0.0006\tTest Loss: 0.0939\tAccuracy: 97.95%\n",
            "[79] Train Loss: 0.0005\tTest Loss: 0.0979\tAccuracy: 98.00%\n",
            "[80] Train Loss: 0.0005\tTest Loss: 0.0940\tAccuracy: 97.95%\n",
            "[81] Train Loss: 0.0005\tTest Loss: 0.0949\tAccuracy: 97.95%\n",
            "[82] Train Loss: 0.0005\tTest Loss: 0.0953\tAccuracy: 98.00%\n",
            "[83] Train Loss: 0.0005\tTest Loss: 0.0941\tAccuracy: 97.96%\n",
            "[84] Train Loss: 0.0005\tTest Loss: 0.0990\tAccuracy: 97.94%\n",
            "[85] Train Loss: 0.0005\tTest Loss: 0.0966\tAccuracy: 97.96%\n",
            "[86] Train Loss: 0.0005\tTest Loss: 0.0964\tAccuracy: 97.97%\n",
            "[87] Train Loss: 0.0005\tTest Loss: 0.0953\tAccuracy: 98.00%\n",
            "[88] Train Loss: 0.0005\tTest Loss: 0.0952\tAccuracy: 97.97%\n",
            "[89] Train Loss: 0.0004\tTest Loss: 0.0954\tAccuracy: 97.96%\n",
            "[90] Train Loss: 0.0004\tTest Loss: 0.0979\tAccuracy: 97.97%\n",
            "[91] Train Loss: 0.0004\tTest Loss: 0.0970\tAccuracy: 97.98%\n",
            "[92] Train Loss: 0.0004\tTest Loss: 0.0965\tAccuracy: 97.97%\n",
            "[93] Train Loss: 0.0004\tTest Loss: 0.0956\tAccuracy: 97.99%\n",
            "[94] Train Loss: 0.0004\tTest Loss: 0.0961\tAccuracy: 97.98%\n",
            "[95] Train Loss: 0.0004\tTest Loss: 0.0974\tAccuracy: 97.99%\n",
            "[96] Train Loss: 0.0004\tTest Loss: 0.1003\tAccuracy: 97.98%\n",
            "[97] Train Loss: 0.0004\tTest Loss: 0.0970\tAccuracy: 97.99%\n",
            "[98] Train Loss: 0.0004\tTest Loss: 0.0974\tAccuracy: 97.95%\n",
            "[99] Train Loss: 0.0004\tTest Loss: 0.0984\tAccuracy: 97.97%\n",
            "[100] Train Loss: 0.0004\tTest Loss: 0.0987\tAccuracy: 97.97%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6b82DZG6W3j"
      },
      "source": [
        "# Assignment 4 구현은 여기서 ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "rAVKHU8_RW7Y"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "sTJqpWU6LwH_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Net()\n",
        "model2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBA8XCUeQAcy",
        "outputId": "c524e576-3248-40be-b4f7-5d9fa694792b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layer1): Linear(in_features=784, out_features=128, bias=True)\n",
              "  (layer2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (layer3): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (relu): LeakyReLU(negative_slope=0.1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.05)"
      ],
      "metadata": {
        "id": "dqOFGj8YQGIO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model2, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model2, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAqYZaMHQVPa",
        "outputId": "ec1120d4-3596-4d56-c8aa-a263110f8097"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Train Loss: 0.9551\tTest Loss: 0.6251\tAccuracy: 83.67%\n",
            "[2] Train Loss: 0.5370\tTest Loss: 0.4497\tAccuracy: 87.25%\n",
            "[3] Train Loss: 0.4263\tTest Loss: 0.3835\tAccuracy: 89.16%\n",
            "[4] Train Loss: 0.3775\tTest Loss: 0.3591\tAccuracy: 89.80%\n",
            "[5] Train Loss: 0.3488\tTest Loss: 0.3246\tAccuracy: 90.58%\n",
            "[6] Train Loss: 0.3280\tTest Loss: 0.3088\tAccuracy: 91.09%\n",
            "[7] Train Loss: 0.3119\tTest Loss: 0.2983\tAccuracy: 91.23%\n",
            "[8] Train Loss: 0.2993\tTest Loss: 0.2832\tAccuracy: 91.72%\n",
            "[9] Train Loss: 0.2860\tTest Loss: 0.2719\tAccuracy: 92.15%\n",
            "[10] Train Loss: 0.2770\tTest Loss: 0.2656\tAccuracy: 92.26%\n",
            "[11] Train Loss: 0.2668\tTest Loss: 0.2730\tAccuracy: 92.22%\n",
            "[12] Train Loss: 0.2576\tTest Loss: 0.2484\tAccuracy: 93.07%\n",
            "[13] Train Loss: 0.2501\tTest Loss: 0.2454\tAccuracy: 92.81%\n",
            "[14] Train Loss: 0.2411\tTest Loss: 0.2390\tAccuracy: 92.82%\n",
            "[15] Train Loss: 0.2332\tTest Loss: 0.2285\tAccuracy: 93.43%\n",
            "[16] Train Loss: 0.2255\tTest Loss: 0.2197\tAccuracy: 93.46%\n",
            "[17] Train Loss: 0.2186\tTest Loss: 0.2120\tAccuracy: 93.75%\n",
            "[18] Train Loss: 0.2121\tTest Loss: 0.2210\tAccuracy: 93.52%\n",
            "[19] Train Loss: 0.2080\tTest Loss: 0.2035\tAccuracy: 94.05%\n",
            "[20] Train Loss: 0.2003\tTest Loss: 0.2061\tAccuracy: 93.91%\n",
            "[21] Train Loss: 0.1955\tTest Loss: 0.1924\tAccuracy: 94.46%\n",
            "[22] Train Loss: 0.1900\tTest Loss: 0.1942\tAccuracy: 94.30%\n",
            "[23] Train Loss: 0.1843\tTest Loss: 0.1922\tAccuracy: 94.29%\n",
            "[24] Train Loss: 0.1794\tTest Loss: 0.1876\tAccuracy: 94.43%\n",
            "[25] Train Loss: 0.1741\tTest Loss: 0.1732\tAccuracy: 94.90%\n",
            "[26] Train Loss: 0.1694\tTest Loss: 0.1832\tAccuracy: 94.71%\n",
            "[27] Train Loss: 0.1649\tTest Loss: 0.1707\tAccuracy: 95.04%\n",
            "[28] Train Loss: 0.1606\tTest Loss: 0.1630\tAccuracy: 95.14%\n",
            "[29] Train Loss: 0.1565\tTest Loss: 0.1613\tAccuracy: 95.24%\n",
            "[30] Train Loss: 0.1529\tTest Loss: 0.1647\tAccuracy: 95.32%\n",
            "[31] Train Loss: 0.1493\tTest Loss: 0.1614\tAccuracy: 95.56%\n",
            "[32] Train Loss: 0.1449\tTest Loss: 0.1523\tAccuracy: 95.57%\n",
            "[33] Train Loss: 0.1428\tTest Loss: 0.1748\tAccuracy: 94.73%\n",
            "[34] Train Loss: 0.1390\tTest Loss: 0.1533\tAccuracy: 95.53%\n",
            "[35] Train Loss: 0.1352\tTest Loss: 0.1413\tAccuracy: 95.96%\n",
            "[36] Train Loss: 0.1323\tTest Loss: 0.1446\tAccuracy: 95.96%\n",
            "[37] Train Loss: 0.1287\tTest Loss: 0.1431\tAccuracy: 95.81%\n",
            "[38] Train Loss: 0.1261\tTest Loss: 0.1404\tAccuracy: 95.78%\n",
            "[39] Train Loss: 0.1239\tTest Loss: 0.1366\tAccuracy: 96.07%\n",
            "[40] Train Loss: 0.1202\tTest Loss: 0.1311\tAccuracy: 96.21%\n",
            "[41] Train Loss: 0.1182\tTest Loss: 0.1295\tAccuracy: 96.23%\n",
            "[42] Train Loss: 0.1153\tTest Loss: 0.1360\tAccuracy: 95.99%\n",
            "[43] Train Loss: 0.1131\tTest Loss: 0.1339\tAccuracy: 95.91%\n",
            "[44] Train Loss: 0.1112\tTest Loss: 0.1263\tAccuracy: 96.29%\n",
            "[45] Train Loss: 0.1082\tTest Loss: 0.1205\tAccuracy: 96.52%\n",
            "[46] Train Loss: 0.1065\tTest Loss: 0.1216\tAccuracy: 96.56%\n",
            "[47] Train Loss: 0.1045\tTest Loss: 0.1209\tAccuracy: 96.39%\n",
            "[48] Train Loss: 0.1024\tTest Loss: 0.1172\tAccuracy: 96.63%\n",
            "[49] Train Loss: 0.0998\tTest Loss: 0.1172\tAccuracy: 96.61%\n",
            "[50] Train Loss: 0.0985\tTest Loss: 0.1166\tAccuracy: 96.42%\n",
            "[51] Train Loss: 0.0955\tTest Loss: 0.1134\tAccuracy: 96.54%\n",
            "[52] Train Loss: 0.0941\tTest Loss: 0.1125\tAccuracy: 96.71%\n",
            "[53] Train Loss: 0.0930\tTest Loss: 0.1133\tAccuracy: 96.57%\n",
            "[54] Train Loss: 0.0916\tTest Loss: 0.1146\tAccuracy: 96.59%\n",
            "[55] Train Loss: 0.0891\tTest Loss: 0.1076\tAccuracy: 96.75%\n",
            "[56] Train Loss: 0.0871\tTest Loss: 0.1088\tAccuracy: 96.80%\n",
            "[57] Train Loss: 0.0857\tTest Loss: 0.1107\tAccuracy: 96.79%\n",
            "[58] Train Loss: 0.0842\tTest Loss: 0.1060\tAccuracy: 96.83%\n",
            "[59] Train Loss: 0.0825\tTest Loss: 0.1064\tAccuracy: 96.84%\n",
            "[60] Train Loss: 0.0809\tTest Loss: 0.1032\tAccuracy: 96.85%\n",
            "[61] Train Loss: 0.0796\tTest Loss: 0.1040\tAccuracy: 96.92%\n",
            "[62] Train Loss: 0.0782\tTest Loss: 0.1024\tAccuracy: 96.93%\n",
            "[63] Train Loss: 0.0767\tTest Loss: 0.1015\tAccuracy: 97.09%\n",
            "[64] Train Loss: 0.0750\tTest Loss: 0.1005\tAccuracy: 97.06%\n",
            "[65] Train Loss: 0.0744\tTest Loss: 0.1018\tAccuracy: 97.03%\n",
            "[66] Train Loss: 0.0730\tTest Loss: 0.1053\tAccuracy: 96.86%\n",
            "[67] Train Loss: 0.0715\tTest Loss: 0.0999\tAccuracy: 97.03%\n",
            "[68] Train Loss: 0.0702\tTest Loss: 0.0963\tAccuracy: 97.21%\n",
            "[69] Train Loss: 0.0689\tTest Loss: 0.0960\tAccuracy: 97.19%\n",
            "[70] Train Loss: 0.0681\tTest Loss: 0.1256\tAccuracy: 95.88%\n",
            "[71] Train Loss: 0.0669\tTest Loss: 0.0963\tAccuracy: 97.20%\n",
            "[72] Train Loss: 0.0660\tTest Loss: 0.0974\tAccuracy: 97.04%\n",
            "[73] Train Loss: 0.0646\tTest Loss: 0.0983\tAccuracy: 97.16%\n",
            "[74] Train Loss: 0.0637\tTest Loss: 0.0913\tAccuracy: 97.26%\n",
            "[75] Train Loss: 0.0627\tTest Loss: 0.0903\tAccuracy: 97.26%\n",
            "[76] Train Loss: 0.0615\tTest Loss: 0.0887\tAccuracy: 97.29%\n",
            "[77] Train Loss: 0.0605\tTest Loss: 0.0888\tAccuracy: 97.31%\n",
            "[78] Train Loss: 0.0593\tTest Loss: 0.0882\tAccuracy: 97.46%\n",
            "[79] Train Loss: 0.0585\tTest Loss: 0.0873\tAccuracy: 97.29%\n",
            "[80] Train Loss: 0.0575\tTest Loss: 0.0874\tAccuracy: 97.36%\n",
            "[81] Train Loss: 0.0565\tTest Loss: 0.0947\tAccuracy: 97.10%\n",
            "[82] Train Loss: 0.0556\tTest Loss: 0.0911\tAccuracy: 97.36%\n",
            "[83] Train Loss: 0.0550\tTest Loss: 0.0901\tAccuracy: 97.29%\n",
            "[84] Train Loss: 0.0542\tTest Loss: 0.0853\tAccuracy: 97.47%\n",
            "[85] Train Loss: 0.0529\tTest Loss: 0.0850\tAccuracy: 97.62%\n",
            "[86] Train Loss: 0.0525\tTest Loss: 0.0853\tAccuracy: 97.54%\n",
            "[87] Train Loss: 0.0516\tTest Loss: 0.0896\tAccuracy: 97.25%\n",
            "[88] Train Loss: 0.0505\tTest Loss: 0.0835\tAccuracy: 97.52%\n",
            "[89] Train Loss: 0.0497\tTest Loss: 0.0827\tAccuracy: 97.60%\n",
            "[90] Train Loss: 0.0490\tTest Loss: 0.0873\tAccuracy: 97.51%\n",
            "[91] Train Loss: 0.0484\tTest Loss: 0.0847\tAccuracy: 97.57%\n",
            "[92] Train Loss: 0.0475\tTest Loss: 0.0824\tAccuracy: 97.54%\n",
            "[93] Train Loss: 0.0466\tTest Loss: 0.0827\tAccuracy: 97.59%\n",
            "[94] Train Loss: 0.0464\tTest Loss: 0.0832\tAccuracy: 97.53%\n",
            "[95] Train Loss: 0.0458\tTest Loss: 0.0896\tAccuracy: 97.29%\n",
            "[96] Train Loss: 0.0448\tTest Loss: 0.0806\tAccuracy: 97.50%\n",
            "[97] Train Loss: 0.0444\tTest Loss: 0.0800\tAccuracy: 97.66%\n",
            "[98] Train Loss: 0.0436\tTest Loss: 0.0833\tAccuracy: 97.47%\n",
            "[99] Train Loss: 0.0428\tTest Loss: 0.0824\tAccuracy: 97.60%\n",
            "[100] Train Loss: 0.0425\tTest Loss: 0.0808\tAccuracy: 97.48%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 활성화함수의 경우, ReLU를 보완한 LeakyReLU를 사용하였다.\n",
        "2. 학습률을 0.5에서 0.05로 바꾸었다.\n",
        "3. Net 모델에서는 굳이 layer를 하나 더 쌓을 필요 없다고 생각하였다. 왜냐하면, 이미 과제3에서 정확도가 97.97%가 나왔기 때문에 layer의 보완보다는 hyperparameter의 보완이 유용할 수 있다고 판단하였다.\n",
        "4. BUT, 정확도의 경우 약 0.5%정도 감소한 결과가 나왔다... 하지만, test loss가 작게 나와서 의미가 없지 않다고 생각한다."
      ],
      "metadata": {
        "id": "tYjmCLGDXww-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X5MsbHP0YchJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}