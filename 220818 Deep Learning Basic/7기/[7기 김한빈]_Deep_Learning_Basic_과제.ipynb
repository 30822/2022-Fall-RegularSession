{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0818]_Deep_Learning_Basic_과제.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","source":["import numpy as np\n","from sympy import *"],"metadata":{"id":"c8RS_HmSSl77","executionInfo":{"status":"ok","timestamp":1661090518381,"user_tz":-540,"elapsed":423,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz","executionInfo":{"status":"ok","timestamp":1661090518381,"user_tz":-540,"elapsed":21,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"source":["def relu(x):\n","\n","  return np.maximum(x,0)"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","\n","  return 1. * (x > 0)"],"metadata":{"id":"Esm4jmTVijro","executionInfo":{"status":"ok","timestamp":1661090518382,"user_tz":-540,"elapsed":20,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["ex_list = [-1.3, -2.3, 0, 2, 3]"],"metadata":{"id":"o8zHPKZ4U_h1","executionInfo":{"status":"ok","timestamp":1661090518382,"user_tz":-540,"elapsed":19,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["relu(ex_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9jmsHEBVHRh","executionInfo":{"status":"ok","timestamp":1661090518382,"user_tz":-540,"elapsed":18,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"a0b850c9-a2ef-4660-e67d-fea14ad6f2bf"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 2., 3.])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","source":["from IPython import get_ipython\n","get_ipython().magic('reset -sf')"],"metadata":{"id":"VqXptklJaF5W","executionInfo":{"status":"ok","timestamp":1661090518383,"user_tz":-540,"elapsed":17,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import sklearn.datasets\n","\n","mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"],"metadata":{"id":"A1h7g1V1aJMc","executionInfo":{"status":"ok","timestamp":1661090579761,"user_tz":-540,"elapsed":61394,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# data preprocessing\n","\n","num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","  y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","  y_test[y_test_index[idx], idx] = 1    "],"metadata":{"id":"ENoHQ2JFatrH","executionInfo":{"status":"ok","timestamp":1661090579762,"user_tz":-540,"elapsed":26,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["x_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F694FN63ax9r","executionInfo":{"status":"ok","timestamp":1661090579762,"user_tz":-540,"elapsed":24,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"dea056b8-a84b-4c51-e63d-dfdfddab5624"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784, 60000)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden_size_1 = 128 # hidden unit size\n","hidden_size_2 = 64\n","\n","# two-layer neural network\n","\n","params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n","\n","          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n","          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n","          \n","          \"W3\": np.random.randn(num_class, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_2)\n","          }\n","# Xavier initialization: https://reniew.github.io/13/"],"metadata":{"id":"9L6daoh_QxO8","executionInfo":{"status":"ok","timestamp":1661090579763,"user_tz":-540,"elapsed":17,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def d_sigmoid(x):\n","  # derivative of sigmoid\n","  exp = np.exp(-x)\n","  return (exp)/((1+exp)**2)\n","\n","def softmax(x):\n","  exp = np.exp(x)\n","  return exp/np.sum(exp, axis=0)"],"metadata":{"id":"KCTXiSU3Qyqd","executionInfo":{"status":"ok","timestamp":1661090579763,"user_tz":-540,"elapsed":16,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def compute_loss(y_true, y_pred):\n","  # loss calculation\n","\n","  num_sample = y_true.shape[1]\n","  Li = -1 * np.sum(y_true * np.log(y_pred))\n","  \n","  return Li/num_sample"],"metadata":{"id":"CDFcWEkhQz53","executionInfo":{"status":"ok","timestamp":1661090579763,"user_tz":-540,"elapsed":15,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def foward_pass(x, params):\n","  \n","  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params[\"A1\"] = sigmoid(params[\"S1\"])\n","\n","  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","  params[\"A2\"] = sigmoid(params[\"S2\"])\n","  \n","  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","  params[\"A3\"] = softmax(params[\"S3\"])\n","\n","  return params"],"metadata":{"id":"YvVCUZKNQ1oV","executionInfo":{"status":"ok","timestamp":1661090579764,"user_tz":-540,"elapsed":16,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def foward_pass_test(x, params):\n","\n","  params_test = {}\n","  \n","  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n","  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","  params_test[\"A2\"] = sigmoid(params_test[\"S2\"])\n","  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n","\n","  return params_test"],"metadata":{"id":"dUbAjGqGQ2J7","executionInfo":{"status":"ok","timestamp":1661090579764,"user_tz":-540,"elapsed":16,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def compute_accuracy(y_true, y_pred):\n","  y_true_idx = np.argmax(y_true, axis = 0)\n","  y_pred_idx = np.argmax(y_pred, axis = 0)\n","  num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","  accuracy = num_correct / y_true.shape[1] * 100\n","\n","  return accuracy"],"metadata":{"id":"R6fm4MzAQ3cF","executionInfo":{"status":"ok","timestamp":1661090579764,"user_tz":-540,"elapsed":15,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def backward_pass(x, y_true, params):\n","  dS3 = params[\"A3\"] - y_true\n","  # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n","  # dS2 is softmax + CE loss derivative\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] =  np.sum(dS3, axis=1, keepdims=True)/x.shape[1] #*(1/x.shape[1])\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"metadata":{"id":"yoMWVMr6Q7Kl","executionInfo":{"status":"ok","timestamp":1661090580306,"user_tz":-540,"elapsed":557,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.5\n","\n","for i in range(epochs):\n","\n","  if i == 0:\n","    params = foward_pass(x_train, params)\n","    \n","  grads = backward_pass(x_train, y_train, params)\n","\n","  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","  params = foward_pass(x_train, params)\n","  train_loss = compute_loss(y_train, params[\"A3\"])\n","  train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","  params_test = foward_pass_test(x_test, params)\n","  test_loss = compute_loss(y_test, params_test[\"A3\"])\n","  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EL-NQ_dftTu","executionInfo":{"status":"ok","timestamp":1661093353253,"user_tz":-540,"elapsed":206162,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"8fdc4e48-e01d-4ab3-c446-f58dcf57700f"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = 2.280842, training acuracy = 14.21%, test loss = 2.280371, training acuracy = 14.82%\n","Epoch 2: training loss = 2.278588, training acuracy = 14.75%, test loss = 2.278057, training acuracy = 15.44%\n","Epoch 3: training loss = 2.2763, training acuracy = 15.45%, test loss = 2.275706, training acuracy = 16.15%\n","Epoch 4: training loss = 2.273972, training acuracy = 16.29%, test loss = 2.273316, training acuracy = 17.03%\n","Epoch 5: training loss = 2.2716, training acuracy = 17.2%, test loss = 2.27088, training acuracy = 17.97%\n","Epoch 6: training loss = 2.26918, training acuracy = 18.25%, test loss = 2.268395, training acuracy = 18.92%\n","Epoch 7: training loss = 2.266706, training acuracy = 19.19%, test loss = 2.265855, training acuracy = 19.97%\n","Epoch 8: training loss = 2.264174, training acuracy = 20.31%, test loss = 2.263255, training acuracy = 20.99%\n","Epoch 9: training loss = 2.261578, training acuracy = 21.42%, test loss = 2.260591, training acuracy = 22.11%\n","Epoch 10: training loss = 2.258914, training acuracy = 22.5%, test loss = 2.257855, training acuracy = 23.25%\n","Epoch 11: training loss = 2.256176, training acuracy = 23.56%, test loss = 2.255044, training acuracy = 24.23%\n","Epoch 12: training loss = 2.253358, training acuracy = 24.67%, test loss = 2.252151, training acuracy = 25.19%\n","Epoch 13: training loss = 2.250454, training acuracy = 25.7%, test loss = 2.24917, training acuracy = 26.21%\n","Epoch 14: training loss = 2.247459, training acuracy = 26.68%, test loss = 2.246094, training acuracy = 27.15%\n","Epoch 15: training loss = 2.244364, training acuracy = 27.68%, test loss = 2.242918, training acuracy = 28.38%\n","Epoch 16: training loss = 2.241165, training acuracy = 28.69%, test loss = 2.239634, training acuracy = 29.39%\n","Epoch 17: training loss = 2.237854, training acuracy = 29.56%, test loss = 2.236234, training acuracy = 30.38%\n","Epoch 18: training loss = 2.234423, training acuracy = 30.48%, test loss = 2.232713, training acuracy = 31.31%\n","Epoch 19: training loss = 2.230865, training acuracy = 31.25%, test loss = 2.22906, training acuracy = 32.18%\n","Epoch 20: training loss = 2.227172, training acuracy = 32.08%, test loss = 2.22527, training acuracy = 32.94%\n","Epoch 21: training loss = 2.223335, training acuracy = 32.84%, test loss = 2.221332, training acuracy = 33.76%\n","Epoch 22: training loss = 2.219346, training acuracy = 33.59%, test loss = 2.217238, training acuracy = 34.59%\n","Epoch 23: training loss = 2.215196, training acuracy = 34.35%, test loss = 2.212979, training acuracy = 35.22%\n","Epoch 24: training loss = 2.210874, training acuracy = 35.11%, test loss = 2.208544, training acuracy = 35.88%\n","Epoch 25: training loss = 2.206372, training acuracy = 35.85%, test loss = 2.203924, training acuracy = 36.67%\n","Epoch 26: training loss = 2.201679, training acuracy = 36.52%, test loss = 2.199109, training acuracy = 37.25%\n","Epoch 27: training loss = 2.196785, training acuracy = 37.16%, test loss = 2.194086, training acuracy = 37.97%\n","Epoch 28: training loss = 2.191677, training acuracy = 37.76%, test loss = 2.188846, training acuracy = 38.45%\n","Epoch 29: training loss = 2.186345, training acuracy = 38.35%, test loss = 2.183376, training acuracy = 39.07%\n","Epoch 30: training loss = 2.180777, training acuracy = 38.93%, test loss = 2.177664, training acuracy = 39.68%\n","Epoch 31: training loss = 2.174961, training acuracy = 39.51%, test loss = 2.171699, training acuracy = 40.28%\n","Epoch 32: training loss = 2.168885, training acuracy = 40.06%, test loss = 2.165466, training acuracy = 40.79%\n","Epoch 33: training loss = 2.162535, training acuracy = 40.6%, test loss = 2.158954, training acuracy = 41.14%\n","Epoch 34: training loss = 2.1559, training acuracy = 41.07%, test loss = 2.15215, training acuracy = 41.46%\n","Epoch 35: training loss = 2.148966, training acuracy = 41.5%, test loss = 2.145041, training acuracy = 41.93%\n","Epoch 36: training loss = 2.141721, training acuracy = 41.91%, test loss = 2.137613, training acuracy = 42.38%\n","Epoch 37: training loss = 2.134151, training acuracy = 42.29%, test loss = 2.129854, training acuracy = 42.82%\n","Epoch 38: training loss = 2.126246, training acuracy = 42.67%, test loss = 2.121751, training acuracy = 43.0%\n","Epoch 39: training loss = 2.117991, training acuracy = 43.06%, test loss = 2.113292, training acuracy = 43.29%\n","Epoch 40: training loss = 2.109377, training acuracy = 43.44%, test loss = 2.104465, training acuracy = 43.63%\n","Epoch 41: training loss = 2.100391, training acuracy = 43.89%, test loss = 2.09526, training acuracy = 44.01%\n","Epoch 42: training loss = 2.091025, training acuracy = 44.25%, test loss = 2.085667, training acuracy = 44.42%\n","Epoch 43: training loss = 2.081269, training acuracy = 44.59%, test loss = 2.075676, training acuracy = 44.75%\n","Epoch 44: training loss = 2.071115, training acuracy = 44.92%, test loss = 2.06528, training acuracy = 44.92%\n","Epoch 45: training loss = 2.060558, training acuracy = 45.28%, test loss = 2.054474, training acuracy = 45.2%\n","Epoch 46: training loss = 2.049593, training acuracy = 45.62%, test loss = 2.043252, training acuracy = 45.67%\n","Epoch 47: training loss = 2.038217, training acuracy = 45.98%, test loss = 2.031613, training acuracy = 45.95%\n","Epoch 48: training loss = 2.026429, training acuracy = 46.31%, test loss = 2.019556, training acuracy = 46.13%\n","Epoch 49: training loss = 2.014231, training acuracy = 46.7%, test loss = 2.007084, training acuracy = 46.46%\n","Epoch 50: training loss = 2.001627, training acuracy = 47.09%, test loss = 1.994199, training acuracy = 46.73%\n","Epoch 51: training loss = 1.988622, training acuracy = 47.43%, test loss = 1.98091, training acuracy = 47.04%\n","Epoch 52: training loss = 1.975226, training acuracy = 47.82%, test loss = 1.967224, training acuracy = 47.35%\n","Epoch 53: training loss = 1.961449, training acuracy = 48.13%, test loss = 1.953154, training acuracy = 47.85%\n","Epoch 54: training loss = 1.947304, training acuracy = 48.49%, test loss = 1.938714, training acuracy = 48.28%\n","Epoch 55: training loss = 1.932805, training acuracy = 48.82%, test loss = 1.923919, training acuracy = 48.7%\n","Epoch 56: training loss = 1.917972, training acuracy = 49.2%, test loss = 1.908787, training acuracy = 49.03%\n","Epoch 57: training loss = 1.902821, training acuracy = 49.56%, test loss = 1.893339, training acuracy = 49.44%\n","Epoch 58: training loss = 1.887375, training acuracy = 50.02%, test loss = 1.877595, training acuracy = 49.95%\n","Epoch 59: training loss = 1.871655, training acuracy = 50.4%, test loss = 1.861579, training acuracy = 50.44%\n","Epoch 60: training loss = 1.855684, training acuracy = 50.83%, test loss = 1.845315, training acuracy = 50.91%\n","Epoch 61: training loss = 1.839485, training acuracy = 51.27%, test loss = 1.828827, training acuracy = 51.37%\n","Epoch 62: training loss = 1.823085, training acuracy = 51.66%, test loss = 1.812142, training acuracy = 51.82%\n","Epoch 63: training loss = 1.806506, training acuracy = 52.14%, test loss = 1.795284, training acuracy = 52.34%\n","Epoch 64: training loss = 1.789776, training acuracy = 52.56%, test loss = 1.778279, training acuracy = 52.81%\n","Epoch 65: training loss = 1.772917, training acuracy = 52.98%, test loss = 1.761154, training acuracy = 53.34%\n","Epoch 66: training loss = 1.755957, training acuracy = 53.39%, test loss = 1.743933, training acuracy = 53.79%\n","Epoch 67: training loss = 1.738918, training acuracy = 53.8%, test loss = 1.726643, training acuracy = 54.26%\n","Epoch 68: training loss = 1.721825, training acuracy = 54.23%, test loss = 1.709308, training acuracy = 54.6%\n","Epoch 69: training loss = 1.704703, training acuracy = 54.65%, test loss = 1.691952, training acuracy = 55.01%\n","Epoch 70: training loss = 1.687573, training acuracy = 55.08%, test loss = 1.6746, training acuracy = 55.66%\n","Epoch 71: training loss = 1.67046, training acuracy = 55.58%, test loss = 1.657273, training acuracy = 56.16%\n","Epoch 72: training loss = 1.653383, training acuracy = 56.0%, test loss = 1.639994, training acuracy = 56.51%\n","Epoch 73: training loss = 1.636364, training acuracy = 56.47%, test loss = 1.622784, training acuracy = 56.91%\n","Epoch 74: training loss = 1.619423, training acuracy = 56.89%, test loss = 1.605663, training acuracy = 57.41%\n","Epoch 75: training loss = 1.602578, training acuracy = 57.31%, test loss = 1.588649, training acuracy = 57.94%\n","Epoch 76: training loss = 1.585848, training acuracy = 57.75%, test loss = 1.571761, training acuracy = 58.45%\n","Epoch 77: training loss = 1.569247, training acuracy = 58.2%, test loss = 1.555015, training acuracy = 58.94%\n","Epoch 78: training loss = 1.552793, training acuracy = 58.61%, test loss = 1.538426, training acuracy = 59.45%\n","Epoch 79: training loss = 1.536497, training acuracy = 59.07%, test loss = 1.522008, training acuracy = 59.87%\n","Epoch 80: training loss = 1.520373, training acuracy = 59.56%, test loss = 1.505772, training acuracy = 60.33%\n","Epoch 81: training loss = 1.504432, training acuracy = 60.0%, test loss = 1.489729, training acuracy = 60.77%\n","Epoch 82: training loss = 1.488682, training acuracy = 60.42%, test loss = 1.473888, training acuracy = 61.25%\n","Epoch 83: training loss = 1.473133, training acuracy = 60.88%, test loss = 1.458257, training acuracy = 61.52%\n","Epoch 84: training loss = 1.45779, training acuracy = 61.33%, test loss = 1.442843, training acuracy = 61.93%\n","Epoch 85: training loss = 1.442659, training acuracy = 61.7%, test loss = 1.427649, training acuracy = 62.28%\n","Epoch 86: training loss = 1.427744, training acuracy = 62.17%, test loss = 1.412681, training acuracy = 62.63%\n","Epoch 87: training loss = 1.413048, training acuracy = 62.56%, test loss = 1.39794, training acuracy = 63.02%\n","Epoch 88: training loss = 1.398573, training acuracy = 62.99%, test loss = 1.383428, training acuracy = 63.44%\n","Epoch 89: training loss = 1.38432, training acuracy = 63.4%, test loss = 1.369144, training acuracy = 63.85%\n","Epoch 90: training loss = 1.37029, training acuracy = 63.77%, test loss = 1.35509, training acuracy = 64.26%\n","Epoch 91: training loss = 1.356481, training acuracy = 64.15%, test loss = 1.341264, training acuracy = 64.79%\n","Epoch 92: training loss = 1.342894, training acuracy = 64.53%, test loss = 1.327664, training acuracy = 65.14%\n","Epoch 93: training loss = 1.329525, training acuracy = 64.91%, test loss = 1.314288, training acuracy = 65.55%\n","Epoch 94: training loss = 1.316373, training acuracy = 65.3%, test loss = 1.301134, training acuracy = 66.09%\n","Epoch 95: training loss = 1.303437, training acuracy = 65.69%, test loss = 1.288199, training acuracy = 66.39%\n","Epoch 96: training loss = 1.290712, training acuracy = 66.05%, test loss = 1.27548, training acuracy = 66.64%\n","Epoch 97: training loss = 1.278197, training acuracy = 66.46%, test loss = 1.262973, training acuracy = 67.02%\n","Epoch 98: training loss = 1.265889, training acuracy = 66.81%, test loss = 1.250676, training acuracy = 67.44%\n","Epoch 99: training loss = 1.253785, training acuracy = 67.1%, test loss = 1.238585, training acuracy = 67.91%\n","Epoch 100: training loss = 1.241882, training acuracy = 67.42%, test loss = 1.226696, training acuracy = 68.23%\n"]}]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","source":["from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm"],"metadata":{"id":"X7gT8zZrccqr","executionInfo":{"status":"ok","timestamp":1661090602159,"user_tz":-540,"elapsed":1087,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# 이미지를 텐서로 변경\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"6PI2vub8PyK9","executionInfo":{"status":"ok","timestamp":1661090602166,"user_tz":-540,"elapsed":78,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["trainset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = True,\n","    download  = True,\n","    transform = transform\n",")\n","testset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = False,\n","    download  = True,\n","    transform = transform\n",")"],"metadata":{"id":"b_AedUXiPzEL","executionInfo":{"status":"ok","timestamp":1661090602170,"user_tz":-540,"elapsed":76,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 512\n","# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n","# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n","train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"b-ug5VmEc5w8","executionInfo":{"status":"ok","timestamp":1661090602171,"user_tz":-540,"elapsed":74,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk","executionInfo":{"status":"ok","timestamp":1661090602172,"user_tz":-540,"elapsed":69,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"source":["# Assignment 3 구현은 여기서 ()\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"execution_count":22,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vaPCGQy4cCCN","executionInfo":{"status":"ok","timestamp":1661090602172,"user_tz":-540,"elapsed":68,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"4c7e4414-aa4e-40d4-da0f-06dd6a6c4a9f"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"uSney41ucozU","executionInfo":{"status":"ok","timestamp":1661090602173,"user_tz":-540,"elapsed":57,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"id":"pUJuSB_HcwVt","executionInfo":{"status":"ok","timestamp":1661090602174,"user_tz":-540,"elapsed":55,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"aX-SDX14czH0","executionInfo":{"status":"ok","timestamp":1661090602174,"user_tz":-540,"elapsed":53,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in(range(1, EPOCHS + 1)):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VbfLa9_pc045","executionInfo":{"status":"ok","timestamp":1661091314293,"user_tz":-540,"elapsed":712170,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"7e28810b-99d7-4a3b-eae4-93d75712dfd2"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.7900\tTest Loss: 0.3325\tAccuracy: 89.35%\n","[2] Train Loss: 0.2408\tTest Loss: 0.2499\tAccuracy: 91.88%\n","[3] Train Loss: 0.1679\tTest Loss: 0.1756\tAccuracy: 94.37%\n","[4] Train Loss: 0.1288\tTest Loss: 0.1684\tAccuracy: 94.19%\n","[5] Train Loss: 0.1066\tTest Loss: 0.1443\tAccuracy: 95.51%\n","[6] Train Loss: 0.0970\tTest Loss: 0.2304\tAccuracy: 92.82%\n","[7] Train Loss: 0.0970\tTest Loss: 0.0970\tAccuracy: 97.03%\n","[8] Train Loss: 0.0674\tTest Loss: 0.0807\tAccuracy: 97.47%\n","[9] Train Loss: 0.0591\tTest Loss: 0.0781\tAccuracy: 97.57%\n","[10] Train Loss: 0.0537\tTest Loss: 0.1360\tAccuracy: 95.80%\n","[11] Train Loss: 0.0473\tTest Loss: 0.1103\tAccuracy: 96.40%\n","[12] Train Loss: 0.2672\tTest Loss: 0.1043\tAccuracy: 96.85%\n","[13] Train Loss: 0.0778\tTest Loss: 0.1370\tAccuracy: 95.62%\n","[14] Train Loss: 0.0615\tTest Loss: 0.0999\tAccuracy: 97.05%\n","[15] Train Loss: 0.0524\tTest Loss: 0.0874\tAccuracy: 97.44%\n","[16] Train Loss: 0.0452\tTest Loss: 0.0960\tAccuracy: 96.99%\n","[17] Train Loss: 0.0398\tTest Loss: 0.1042\tAccuracy: 97.08%\n","[18] Train Loss: 0.0352\tTest Loss: 0.0998\tAccuracy: 96.98%\n","[19] Train Loss: 0.0323\tTest Loss: 0.1370\tAccuracy: 96.00%\n","[20] Train Loss: 0.0298\tTest Loss: 0.1092\tAccuracy: 96.75%\n","[21] Train Loss: 0.0271\tTest Loss: 0.0973\tAccuracy: 97.08%\n","[22] Train Loss: 0.0235\tTest Loss: 0.1378\tAccuracy: 96.49%\n","[23] Train Loss: 0.0213\tTest Loss: 0.0825\tAccuracy: 97.52%\n","[24] Train Loss: 0.0183\tTest Loss: 0.0867\tAccuracy: 97.37%\n","[25] Train Loss: 0.0162\tTest Loss: 0.0839\tAccuracy: 97.63%\n","[26] Train Loss: 0.0149\tTest Loss: 0.0853\tAccuracy: 97.59%\n","[27] Train Loss: 0.0137\tTest Loss: 0.0843\tAccuracy: 97.66%\n","[28] Train Loss: 0.0118\tTest Loss: 0.0849\tAccuracy: 97.61%\n","[29] Train Loss: 0.0102\tTest Loss: 0.0848\tAccuracy: 97.76%\n","[30] Train Loss: 0.0094\tTest Loss: 0.0777\tAccuracy: 98.00%\n","[31] Train Loss: 0.0084\tTest Loss: 0.0775\tAccuracy: 98.06%\n","[32] Train Loss: 0.0076\tTest Loss: 0.1491\tAccuracy: 96.18%\n","[33] Train Loss: 2.7710\tTest Loss: 0.5096\tAccuracy: 84.15%\n","[34] Train Loss: 0.3090\tTest Loss: 0.2792\tAccuracy: 91.78%\n","[35] Train Loss: 0.3321\tTest Loss: 0.3022\tAccuracy: 91.12%\n","[36] Train Loss: 0.2152\tTest Loss: 0.2302\tAccuracy: 92.98%\n","[37] Train Loss: 0.1597\tTest Loss: 0.1828\tAccuracy: 94.60%\n","[38] Train Loss: 0.1568\tTest Loss: 0.2937\tAccuracy: 90.89%\n","[39] Train Loss: 0.1298\tTest Loss: 0.2633\tAccuracy: 92.23%\n","[40] Train Loss: 0.1280\tTest Loss: 0.1990\tAccuracy: 94.25%\n","[41] Train Loss: 0.1113\tTest Loss: 0.1883\tAccuracy: 94.36%\n","[42] Train Loss: 0.1002\tTest Loss: 0.3109\tAccuracy: 90.74%\n","[43] Train Loss: 0.1289\tTest Loss: 0.1973\tAccuracy: 94.28%\n","[44] Train Loss: 0.0902\tTest Loss: 0.1564\tAccuracy: 95.21%\n","[45] Train Loss: 0.0840\tTest Loss: 0.2685\tAccuracy: 92.11%\n","[46] Train Loss: 0.0798\tTest Loss: 0.1526\tAccuracy: 95.60%\n","[47] Train Loss: 0.0746\tTest Loss: 0.1166\tAccuracy: 96.62%\n","[48] Train Loss: 0.0710\tTest Loss: 0.1187\tAccuracy: 96.41%\n","[49] Train Loss: 0.0680\tTest Loss: 0.1221\tAccuracy: 96.44%\n","[50] Train Loss: 0.0643\tTest Loss: 0.2162\tAccuracy: 94.12%\n","[51] Train Loss: 0.0674\tTest Loss: 0.1130\tAccuracy: 96.64%\n","[52] Train Loss: 0.0585\tTest Loss: 0.1292\tAccuracy: 96.47%\n","[53] Train Loss: 0.0564\tTest Loss: 0.1512\tAccuracy: 95.82%\n","[54] Train Loss: 0.0556\tTest Loss: 0.1855\tAccuracy: 95.05%\n","[55] Train Loss: 0.0546\tTest Loss: 0.1262\tAccuracy: 96.72%\n","[56] Train Loss: 0.0484\tTest Loss: 0.1506\tAccuracy: 95.86%\n","[57] Train Loss: 0.0495\tTest Loss: 0.1431\tAccuracy: 95.98%\n","[58] Train Loss: 0.0472\tTest Loss: 0.1151\tAccuracy: 96.81%\n","[59] Train Loss: 0.0435\tTest Loss: 0.1664\tAccuracy: 95.54%\n","[60] Train Loss: 0.0444\tTest Loss: 0.1097\tAccuracy: 96.97%\n","[61] Train Loss: 0.0399\tTest Loss: 0.1407\tAccuracy: 96.48%\n","[62] Train Loss: 0.0400\tTest Loss: 0.1646\tAccuracy: 95.80%\n","[63] Train Loss: 0.0369\tTest Loss: 0.1213\tAccuracy: 97.05%\n","[64] Train Loss: 0.0367\tTest Loss: 0.3315\tAccuracy: 92.51%\n","[65] Train Loss: 0.5149\tTest Loss: 0.2427\tAccuracy: 93.44%\n","[66] Train Loss: 0.1622\tTest Loss: 0.1797\tAccuracy: 94.91%\n","[67] Train Loss: 0.1273\tTest Loss: 0.3037\tAccuracy: 91.49%\n","[68] Train Loss: 0.1111\tTest Loss: 0.1491\tAccuracy: 95.88%\n","[69] Train Loss: 0.0999\tTest Loss: 0.1927\tAccuracy: 94.68%\n","[70] Train Loss: 0.0923\tTest Loss: 0.1594\tAccuracy: 95.38%\n","[71] Train Loss: 0.0857\tTest Loss: 0.1728\tAccuracy: 94.98%\n","[72] Train Loss: 0.0829\tTest Loss: 0.3164\tAccuracy: 91.31%\n","[73] Train Loss: 0.1347\tTest Loss: 0.1531\tAccuracy: 95.66%\n","[74] Train Loss: 0.0770\tTest Loss: 0.1483\tAccuracy: 95.96%\n","[75] Train Loss: 0.0710\tTest Loss: 0.1361\tAccuracy: 96.27%\n","[76] Train Loss: 0.0696\tTest Loss: 0.1451\tAccuracy: 96.03%\n","[77] Train Loss: 0.0647\tTest Loss: 0.1479\tAccuracy: 96.00%\n","[78] Train Loss: 0.0640\tTest Loss: 0.1916\tAccuracy: 94.73%\n","[79] Train Loss: 0.0616\tTest Loss: 0.2195\tAccuracy: 94.46%\n","[80] Train Loss: 0.0595\tTest Loss: 0.1543\tAccuracy: 95.84%\n","[81] Train Loss: 0.0561\tTest Loss: 0.1418\tAccuracy: 96.29%\n","[82] Train Loss: 0.0533\tTest Loss: 0.1599\tAccuracy: 95.52%\n","[83] Train Loss: 0.0525\tTest Loss: 0.1829\tAccuracy: 95.35%\n","[84] Train Loss: 0.0513\tTest Loss: 0.2065\tAccuracy: 94.68%\n","[85] Train Loss: 0.0489\tTest Loss: 0.1577\tAccuracy: 96.01%\n","[86] Train Loss: 0.0461\tTest Loss: 0.1320\tAccuracy: 96.59%\n","[87] Train Loss: 0.0439\tTest Loss: 0.1559\tAccuracy: 95.97%\n","[88] Train Loss: 0.0430\tTest Loss: 0.1459\tAccuracy: 95.88%\n","[89] Train Loss: 0.0418\tTest Loss: 0.1993\tAccuracy: 94.76%\n","[90] Train Loss: 0.0402\tTest Loss: 0.1879\tAccuracy: 95.14%\n","[91] Train Loss: 0.0395\tTest Loss: 0.1394\tAccuracy: 96.34%\n","[92] Train Loss: 0.0368\tTest Loss: 0.4180\tAccuracy: 90.45%\n","[93] Train Loss: 0.0575\tTest Loss: 0.1342\tAccuracy: 96.71%\n","[94] Train Loss: 0.0342\tTest Loss: 0.2038\tAccuracy: 95.12%\n","[95] Train Loss: 0.0324\tTest Loss: 0.1597\tAccuracy: 96.09%\n","[96] Train Loss: 0.0311\tTest Loss: 0.1359\tAccuracy: 96.67%\n","[97] Train Loss: 0.0658\tTest Loss: 0.1698\tAccuracy: 96.20%\n","[98] Train Loss: 0.0328\tTest Loss: 0.1424\tAccuracy: 96.69%\n","[99] Train Loss: 0.0283\tTest Loss: 0.1438\tAccuracy: 96.64%\n","[100] Train Loss: 0.0273\tTest Loss: 0.1379\tAccuracy: 96.76%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","metadata":{"id":"k6b82DZG6W3j","executionInfo":{"status":"ok","timestamp":1661091314293,"user_tz":-540,"elapsed":18,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"source":["# Assignment 4 구현은 여기서 ()"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"ok","timestamp":1661092187547,"user_tz":-540,"elapsed":3,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"id":"d6FghqrKqczU"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.leakyrelu = nn.LeakyReLU(0.1)\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.leakyrelu(out)\n","        out = self.layer2(out)\n","        out = self.leakyrelu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"execution_count":48,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661092188005,"user_tz":-540,"elapsed":3,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"fd39c4c6-2977-44db-ed18-63cb1d9324bf","id":"lr0JMC9-qczU"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (leakyrelu): LeakyReLU(negative_slope=0.1)\n",")"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.5)"],"metadata":{"id":"spa9OTqtjzhf","executionInfo":{"status":"ok","timestamp":1661092406629,"user_tz":-540,"elapsed":525,"user":{"displayName":"김한빈","userId":"12210570214395338003"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in(range(1, EPOCHS + 1)):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JWa8xGwnkEym","executionInfo":{"status":"ok","timestamp":1661093147108,"user_tz":-540,"elapsed":739964,"user":{"displayName":"김한빈","userId":"12210570214395338003"}},"outputId":"e92fd5b3-1d0c-4d3e-8c15-ae3cb1f11216"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 1975.9628\tTest Loss: 439.9503\tAccuracy: 87.37%\n","[2] Train Loss: 232.4814\tTest Loss: 179.4079\tAccuracy: 92.67%\n","[3] Train Loss: 140.7368\tTest Loss: 165.9926\tAccuracy: 92.36%\n","[4] Train Loss: 99.4893\tTest Loss: 120.2303\tAccuracy: 93.39%\n","[5] Train Loss: 64.4371\tTest Loss: 98.0465\tAccuracy: 93.29%\n","[6] Train Loss: 77.3351\tTest Loss: 131.5321\tAccuracy: 92.10%\n","[7] Train Loss: 66.5808\tTest Loss: 91.3434\tAccuracy: 94.12%\n","[8] Train Loss: 42.6393\tTest Loss: 91.6166\tAccuracy: 93.03%\n","[9] Train Loss: 47.8854\tTest Loss: 139.5611\tAccuracy: 88.22%\n","[10] Train Loss: 36.8201\tTest Loss: 104.1044\tAccuracy: 90.04%\n","[11] Train Loss: 39.9807\tTest Loss: 103.6024\tAccuracy: 93.18%\n","[12] Train Loss: 41.3582\tTest Loss: 63.3776\tAccuracy: 94.93%\n","[13] Train Loss: 47.7359\tTest Loss: 150.7091\tAccuracy: 93.06%\n","[14] Train Loss: 60.6608\tTest Loss: 87.8861\tAccuracy: 95.39%\n","[15] Train Loss: 53.8337\tTest Loss: 52.2955\tAccuracy: 94.88%\n","[16] Train Loss: 43.0411\tTest Loss: 76.4864\tAccuracy: 94.25%\n","[17] Train Loss: 48.4082\tTest Loss: 94.3136\tAccuracy: 93.93%\n","[18] Train Loss: 53.0304\tTest Loss: 74.7354\tAccuracy: 94.97%\n","[19] Train Loss: 37.7251\tTest Loss: 55.8363\tAccuracy: 95.16%\n","[20] Train Loss: 21.4088\tTest Loss: 46.7798\tAccuracy: 95.91%\n","[21] Train Loss: 23.9167\tTest Loss: 64.6151\tAccuracy: 95.04%\n","[22] Train Loss: 28.0566\tTest Loss: 73.4241\tAccuracy: 94.24%\n","[23] Train Loss: 44.5782\tTest Loss: 111.5182\tAccuracy: 92.86%\n","[24] Train Loss: 222.7032\tTest Loss: 2901.5659\tAccuracy: 84.56%\n","[25] Train Loss: 12289.0078\tTest Loss: 3082.6265\tAccuracy: 93.75%\n","[26] Train Loss: 1657.7203\tTest Loss: 1768.7164\tAccuracy: 94.32%\n","[27] Train Loss: 921.4221\tTest Loss: 1609.9611\tAccuracy: 92.86%\n","[28] Train Loss: 790.4865\tTest Loss: 1237.5608\tAccuracy: 94.82%\n","[29] Train Loss: 477.2784\tTest Loss: 947.0145\tAccuracy: 95.83%\n","[30] Train Loss: 408.0106\tTest Loss: 940.5146\tAccuracy: 95.61%\n","[31] Train Loss: 287.5843\tTest Loss: 799.1204\tAccuracy: 95.71%\n","[32] Train Loss: 257.6494\tTest Loss: 727.3575\tAccuracy: 95.77%\n","[33] Train Loss: 204.4671\tTest Loss: 838.8088\tAccuracy: 93.77%\n","[34] Train Loss: 221.0935\tTest Loss: 856.0319\tAccuracy: 94.60%\n","[35] Train Loss: 234.0933\tTest Loss: 733.9760\tAccuracy: 96.12%\n","[36] Train Loss: 217.0871\tTest Loss: 714.3983\tAccuracy: 96.61%\n","[37] Train Loss: 184.9001\tTest Loss: 626.4790\tAccuracy: 96.49%\n","[38] Train Loss: 215.7128\tTest Loss: 865.5822\tAccuracy: 95.51%\n","[39] Train Loss: 215.9344\tTest Loss: 661.5939\tAccuracy: 95.84%\n","[40] Train Loss: 238.1075\tTest Loss: 698.4245\tAccuracy: 96.02%\n","[41] Train Loss: 158.8902\tTest Loss: 883.1896\tAccuracy: 94.45%\n","[42] Train Loss: 178.4322\tTest Loss: 845.8445\tAccuracy: 95.28%\n","[43] Train Loss: 153.2975\tTest Loss: 645.4685\tAccuracy: 95.96%\n","[44] Train Loss: 148.3825\tTest Loss: 622.6122\tAccuracy: 96.74%\n","[45] Train Loss: 200.0803\tTest Loss: 637.0516\tAccuracy: 96.22%\n","[46] Train Loss: 135.8684\tTest Loss: 627.4482\tAccuracy: 95.96%\n","[47] Train Loss: 167.8996\tTest Loss: 779.2025\tAccuracy: 96.16%\n","[48] Train Loss: 179.4884\tTest Loss: 751.5698\tAccuracy: 95.90%\n","[49] Train Loss: 236.4879\tTest Loss: 1003.3417\tAccuracy: 95.55%\n","[50] Train Loss: 252.8918\tTest Loss: 745.8046\tAccuracy: 96.44%\n","[51] Train Loss: 362.4292\tTest Loss: 1172.3239\tAccuracy: 94.49%\n","[52] Train Loss: 348.9423\tTest Loss: 1158.2612\tAccuracy: 94.22%\n","[53] Train Loss: 330.9953\tTest Loss: 1630.5223\tAccuracy: 93.44%\n","[54] Train Loss: 490.4635\tTest Loss: 1395.4174\tAccuracy: 94.79%\n","[55] Train Loss: 472.0304\tTest Loss: 1544.2982\tAccuracy: 94.70%\n","[56] Train Loss: 417.0042\tTest Loss: 1113.4263\tAccuracy: 95.95%\n","[57] Train Loss: 359.5753\tTest Loss: 1285.4778\tAccuracy: 95.72%\n","[58] Train Loss: 403.7777\tTest Loss: 1337.5948\tAccuracy: 95.77%\n","[59] Train Loss: 429.8075\tTest Loss: 1439.5283\tAccuracy: 95.92%\n","[60] Train Loss: 475.6963\tTest Loss: 1620.1791\tAccuracy: 95.67%\n","[61] Train Loss: 600.1517\tTest Loss: 1503.2008\tAccuracy: 95.50%\n","[62] Train Loss: 463.4264\tTest Loss: 2328.4624\tAccuracy: 93.26%\n","[63] Train Loss: 495.4898\tTest Loss: 1528.8546\tAccuracy: 95.91%\n","[64] Train Loss: 474.9565\tTest Loss: 1522.0826\tAccuracy: 95.66%\n","[65] Train Loss: 341.1724\tTest Loss: 1679.6520\tAccuracy: 95.48%\n","[66] Train Loss: 428.5058\tTest Loss: 1773.8519\tAccuracy: 95.91%\n","[67] Train Loss: 433.5793\tTest Loss: 1630.5325\tAccuracy: 96.25%\n","[68] Train Loss: 416.1360\tTest Loss: 2054.8564\tAccuracy: 95.92%\n","[69] Train Loss: 949.7603\tTest Loss: 2411.1201\tAccuracy: 96.02%\n","[70] Train Loss: 781.0867\tTest Loss: 2642.2058\tAccuracy: 95.28%\n","[71] Train Loss: 853.9210\tTest Loss: 2137.2144\tAccuracy: 96.25%\n","[72] Train Loss: 571.4791\tTest Loss: 2489.9780\tAccuracy: 95.89%\n","[73] Train Loss: 415.9288\tTest Loss: 2206.4768\tAccuracy: 96.56%\n","[74] Train Loss: 330.9520\tTest Loss: 2453.2812\tAccuracy: 96.16%\n","[75] Train Loss: 488.5076\tTest Loss: 3623.4292\tAccuracy: 92.29%\n","[76] Train Loss: 515.6988\tTest Loss: 2197.9773\tAccuracy: 96.75%\n","[77] Train Loss: 336.8007\tTest Loss: 2084.1606\tAccuracy: 96.77%\n","[78] Train Loss: 488.0450\tTest Loss: 2777.5940\tAccuracy: 95.84%\n","[79] Train Loss: 444.7728\tTest Loss: 2169.4514\tAccuracy: 96.90%\n","[80] Train Loss: 593.6781\tTest Loss: 2830.6416\tAccuracy: 96.20%\n","[81] Train Loss: 763.1929\tTest Loss: 4063.9258\tAccuracy: 95.18%\n","[82] Train Loss: 952.5899\tTest Loss: 4439.0820\tAccuracy: 95.63%\n","[83] Train Loss: 810.7341\tTest Loss: 3234.3970\tAccuracy: 96.89%\n","[84] Train Loss: 949.3138\tTest Loss: 4143.3354\tAccuracy: 96.12%\n","[85] Train Loss: 621.2988\tTest Loss: 5002.4150\tAccuracy: 94.65%\n","[86] Train Loss: 752.2471\tTest Loss: 3936.6418\tAccuracy: 96.41%\n","[87] Train Loss: 733.1392\tTest Loss: 3944.4077\tAccuracy: 96.69%\n","[88] Train Loss: 551.0446\tTest Loss: 3906.6265\tAccuracy: 96.63%\n","[89] Train Loss: 826.0052\tTest Loss: 3919.0879\tAccuracy: 96.71%\n","[90] Train Loss: 473.4341\tTest Loss: 3530.2668\tAccuracy: 97.03%\n","[91] Train Loss: 520.5757\tTest Loss: 5490.4409\tAccuracy: 95.84%\n","[92] Train Loss: 636.0838\tTest Loss: 3893.9622\tAccuracy: 97.04%\n","[93] Train Loss: 618.6421\tTest Loss: 4730.6143\tAccuracy: 96.82%\n","[94] Train Loss: 667.7913\tTest Loss: 5801.0049\tAccuracy: 96.24%\n","[95] Train Loss: 732.6741\tTest Loss: 4624.1069\tAccuracy: 96.79%\n","[96] Train Loss: 697.6214\tTest Loss: 5921.6968\tAccuracy: 96.55%\n","[97] Train Loss: 1067.3459\tTest Loss: 5476.3442\tAccuracy: 96.78%\n","[98] Train Loss: 838.1914\tTest Loss: 5419.4937\tAccuracy: 97.06%\n","[99] Train Loss: 918.5174\tTest Loss: 6173.3525\tAccuracy: 96.53%\n","[100] Train Loss: 790.4056\tTest Loss: 6464.5381\tAccuracy: 96.81%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","source":["1. 위의 기존 모델에서 learning rate이 커서 오히려 정확도가 떨어지는 문제가 있었기 때문에 0.25로 줄여보았습니다. 그러나 오히려 정확도가 낮아졌기 때문에 다시 0.5로 학습률을 조정했습니다. \n","2. optimizer로는 Adam을 사용하였습니다. \n","3. Relu 대신 leakyrelu를 사용해보았습니다. \n","4. elu나 gelu등은 시간관계상 사용해보지 못했습니다. \n","5. leakyrelu의 기울기 값을 0.01로 둬보았지만 성능이 좋지않아 0.1로 설정했습니다.\n","6. 결과적으로 큰 차이는 보이지 않았습니다."],"metadata":{"id":"FTjzMe3PzTVc"}},{"cell_type":"code","source":[""],"metadata":{"id":"UX96TXB916k7"},"execution_count":null,"outputs":[]}]}