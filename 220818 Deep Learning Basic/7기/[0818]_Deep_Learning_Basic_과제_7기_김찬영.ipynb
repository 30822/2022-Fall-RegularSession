{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rjuQY9f2mdS"
      },
      "source": [
        "## 과제 1\n",
        "ReLu activation function과 derivative function을 구현해보세요\n",
        "- Hint : np.maximum 함수 사용하면 편리합니다\n",
        "- 다른 방법 사용하셔도 무방합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "puH0YVGI2uLz"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "\n",
        "    return np.maximun(x,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Esm4jmTVijro"
      },
      "outputs": [],
      "source": [
        "def d_relu(x):\n",
        "    return np.maximum(x/abs(x),0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8Hi0Rc2-yJ"
      },
      "source": [
        "## 과제 2\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n",
        "- Hint : 코드 파일의 예시는 Two layer MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "outputs": [],
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "    dS3 = params[\"A3\"] - y_true\n",
        "    grads = {}\n",
        "\n",
        "    grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "    grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "    dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "    grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "    grads[\"db2\"] = (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "    dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "    grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "    grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twf-R8s-34zT"
      },
      "source": [
        "## 과제 3\n",
        "Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n",
        "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
        "\n",
        "hyperparameter는 다음과 같이 설정\n",
        "\n",
        "- epochs : 100\n",
        "- hiddensize : 128, 64 (two layer)\n",
        "- learning_rate : 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gmo1glCpQoGN"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "n4hnLTlTQugm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "\n",
        "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4MggeQ5FQv1V"
      },
      "outputs": [],
      "source": [
        "# data preprocessing\n",
        "\n",
        "num_train = 60000\n",
        "num_class = 10\n",
        "\n",
        "x_train = np.float32(mnist.data[:num_train]).T\n",
        "y_train_index = np.int32(mnist.target[:num_train]).T\n",
        "x_test = np.float32(mnist.data[num_train:]).T\n",
        "y_test_index = np.int32(mnist.target[num_train:]).T\n",
        "\n",
        "# Normalization\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "x_size = x_train.shape[0]\n",
        "\n",
        "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
        "for idx in range(y_train_index.shape[0]):\n",
        "  y_train[y_train_index[idx], idx] = 1\n",
        "\n",
        "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
        "for idx in range(y_test_index.shape[0]):\n",
        "  y_test[y_test_index[idx], idx] = 1    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9L6daoh_QxO8"
      },
      "outputs": [],
      "source": [
        "#parameter initialization\n",
        "\n",
        "hidden_size_1 = 128\n",
        "hidden_size_2 = 64 \n",
        "\n",
        "# two-layer neural network\n",
        "\n",
        "params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n",
        "          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n",
        "          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n",
        "          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n",
        "          \"W3\": np.random.randn(num_class, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n",
        "          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_2)\n",
        "          }\n",
        "# Xavier initialization: https://reniew.github.io/13/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KCTXiSU3Qyqd"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def d_sigmoid(x):\n",
        "  # derivative of sigmoid\n",
        "  exp = np.exp(-x)\n",
        "  return (exp)/((1+exp)**2)\n",
        "\n",
        "def softmax(x):\n",
        "  exp = np.exp(x)\n",
        "  return exp/np.sum(exp, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CDFcWEkhQz53"
      },
      "outputs": [],
      "source": [
        "def compute_loss(y_true, y_pred):\n",
        "  # loss calculation\n",
        "\n",
        "  num_sample = y_true.shape[1]\n",
        "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
        "  \n",
        "  return Li/num_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YvVCUZKNQ1oV"
      },
      "outputs": [],
      "source": [
        "def foward_pass(x, params):\n",
        "  \n",
        "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params[\"A1\"] = sigmoid(params[\"S1\"])\n",
        "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
        "  params[\"A2\"] = sigmoid(params[\"S2\"])\n",
        "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
        "  params[\"A3\"] = softmax(params[\"S3\"])\n",
        "\n",
        "  return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dUbAjGqGQ2J7"
      },
      "outputs": [],
      "source": [
        "def foward_pass_test(x, params):\n",
        "\n",
        "  params_test = {}\n",
        "  \n",
        "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
        "  params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n",
        "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
        "  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
        "  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
        "  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n",
        "\n",
        "  return params_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R6fm4MzAQ3cF"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(y_true, y_pred):\n",
        "  y_true_idx = np.argmax(y_true, axis = 0)\n",
        "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
        "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
        "\n",
        "  accuracy = num_correct / y_true.shape[1] * 100\n",
        "\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fusEy49j3uhs"
      },
      "outputs": [],
      "source": [
        "def backward_pass(x, y_true, params):\n",
        "    dS3 = params[\"A3\"] - y_true\n",
        "    grads = {}\n",
        "\n",
        "    grads[\"dW3\"] =  np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
        "    grads[\"db3\"] =  (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    dA2 = np.dot(params[\"W3\"].T, dS3)\n",
        "    dS2 = dA2 * d_sigmoid(params[\"S2\"])\n",
        "\n",
        "    grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
        "    grads[\"db2\"] = (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    dA1 = np.dot(params[\"W2\"].T, dS2)\n",
        "    dS1 = dA1 * d_sigmoid(params[\"S1\"])\n",
        "\n",
        "    grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
        "    grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "spjWjDrvQ8yN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: training loss = 2.293128, training acuracy = 11.24%, test loss = 2.30182, training acuracy = 10.61%\n",
            "Epoch 2: training loss = 2.290366, training acuracy = 11.24%, test loss = 2.301669, training acuracy = 11.11%\n",
            "Epoch 3: training loss = 2.287844, training acuracy = 11.25%, test loss = 2.301507, training acuracy = 11.77%\n",
            "Epoch 4: training loss = 2.285312, training acuracy = 11.4%, test loss = 2.301344, training acuracy = 12.33%\n",
            "Epoch 5: training loss = 2.282763, training acuracy = 11.97%, test loss = 2.30118, training acuracy = 13.04%\n",
            "Epoch 6: training loss = 2.280192, training acuracy = 12.91%, test loss = 2.301014, training acuracy = 13.77%\n",
            "Epoch 7: training loss = 2.277596, training acuracy = 13.96%, test loss = 2.300846, training acuracy = 14.7%\n",
            "Epoch 8: training loss = 2.274968, training acuracy = 15.07%, test loss = 2.300677, training acuracy = 15.7%\n",
            "Epoch 9: training loss = 2.272305, training acuracy = 16.22%, test loss = 2.300506, training acuracy = 17.19%\n",
            "Epoch 10: training loss = 2.269602, training acuracy = 17.25%, test loss = 2.300331, training acuracy = 18.45%\n",
            "Epoch 11: training loss = 2.266853, training acuracy = 18.32%, test loss = 2.300155, training acuracy = 20.12%\n",
            "Epoch 12: training loss = 2.264053, training acuracy = 19.42%, test loss = 2.299974, training acuracy = 22.04%\n",
            "Epoch 13: training loss = 2.261198, training acuracy = 20.48%, test loss = 2.299791, training acuracy = 23.78%\n",
            "Epoch 14: training loss = 2.258282, training acuracy = 21.52%, test loss = 2.299603, training acuracy = 25.61%\n",
            "Epoch 15: training loss = 2.255299, training acuracy = 22.53%, test loss = 2.299412, training acuracy = 27.19%\n",
            "Epoch 16: training loss = 2.252244, training acuracy = 23.56%, test loss = 2.299216, training acuracy = 29.0%\n",
            "Epoch 17: training loss = 2.24911, training acuracy = 24.64%, test loss = 2.299015, training acuracy = 30.79%\n",
            "Epoch 18: training loss = 2.245892, training acuracy = 25.76%, test loss = 2.298808, training acuracy = 32.31%\n",
            "Epoch 19: training loss = 2.242583, training acuracy = 26.87%, test loss = 2.298596, training acuracy = 33.56%\n",
            "Epoch 20: training loss = 2.239177, training acuracy = 28.05%, test loss = 2.298378, training acuracy = 35.13%\n",
            "Epoch 21: training loss = 2.235667, training acuracy = 29.22%, test loss = 2.298153, training acuracy = 36.32%\n",
            "Epoch 22: training loss = 2.232045, training acuracy = 30.37%, test loss = 2.297921, training acuracy = 37.48%\n",
            "Epoch 23: training loss = 2.228304, training acuracy = 31.53%, test loss = 2.297682, training acuracy = 38.68%\n",
            "Epoch 24: training loss = 2.224437, training acuracy = 32.79%, test loss = 2.297434, training acuracy = 39.8%\n",
            "Epoch 25: training loss = 2.220434, training acuracy = 34.0%, test loss = 2.297178, training acuracy = 40.78%\n",
            "Epoch 26: training loss = 2.216289, training acuracy = 35.24%, test loss = 2.296913, training acuracy = 41.58%\n",
            "Epoch 27: training loss = 2.211991, training acuracy = 36.54%, test loss = 2.296637, training acuracy = 42.42%\n",
            "Epoch 28: training loss = 2.207532, training acuracy = 37.67%, test loss = 2.296351, training acuracy = 43.29%\n",
            "Epoch 29: training loss = 2.202902, training acuracy = 38.79%, test loss = 2.296054, training acuracy = 43.83%\n",
            "Epoch 30: training loss = 2.198092, training acuracy = 39.99%, test loss = 2.295745, training acuracy = 44.37%\n",
            "Epoch 31: training loss = 2.193091, training acuracy = 41.1%, test loss = 2.295424, training acuracy = 44.89%\n",
            "Epoch 32: training loss = 2.187888, training acuracy = 42.13%, test loss = 2.295089, training acuracy = 45.26%\n",
            "Epoch 33: training loss = 2.182472, training acuracy = 43.17%, test loss = 2.294739, training acuracy = 45.6%\n",
            "Epoch 34: training loss = 2.176832, training acuracy = 44.17%, test loss = 2.294374, training acuracy = 46.01%\n",
            "Epoch 35: training loss = 2.170956, training acuracy = 45.08%, test loss = 2.293993, training acuracy = 46.31%\n",
            "Epoch 36: training loss = 2.164831, training acuracy = 45.96%, test loss = 2.293595, training acuracy = 46.67%\n",
            "Epoch 37: training loss = 2.158447, training acuracy = 46.76%, test loss = 2.293178, training acuracy = 47.06%\n",
            "Epoch 38: training loss = 2.151789, training acuracy = 47.63%, test loss = 2.292742, training acuracy = 47.53%\n",
            "Epoch 39: training loss = 2.144845, training acuracy = 48.45%, test loss = 2.292286, training acuracy = 47.95%\n",
            "Epoch 40: training loss = 2.137602, training acuracy = 49.19%, test loss = 2.291807, training acuracy = 48.32%\n",
            "Epoch 41: training loss = 2.130046, training acuracy = 49.88%, test loss = 2.291306, training acuracy = 48.58%\n",
            "Epoch 42: training loss = 2.122166, training acuracy = 50.49%, test loss = 2.29078, training acuracy = 49.13%\n",
            "Epoch 43: training loss = 2.113947, training acuracy = 51.13%, test loss = 2.290229, training acuracy = 49.64%\n",
            "Epoch 44: training loss = 2.105376, training acuracy = 51.74%, test loss = 2.28965, training acuracy = 50.27%\n",
            "Epoch 45: training loss = 2.096443, training acuracy = 52.33%, test loss = 2.289043, training acuracy = 50.8%\n",
            "Epoch 46: training loss = 2.087134, training acuracy = 52.85%, test loss = 2.288406, training acuracy = 51.19%\n",
            "Epoch 47: training loss = 2.077438, training acuracy = 53.35%, test loss = 2.287737, training acuracy = 51.81%\n",
            "Epoch 48: training loss = 2.067344, training acuracy = 53.83%, test loss = 2.287034, training acuracy = 52.32%\n",
            "Epoch 49: training loss = 2.056844, training acuracy = 54.31%, test loss = 2.286297, training acuracy = 52.95%\n",
            "Epoch 50: training loss = 2.045928, training acuracy = 54.75%, test loss = 2.285524, training acuracy = 53.33%\n",
            "Epoch 51: training loss = 2.03459, training acuracy = 55.15%, test loss = 2.284712, training acuracy = 53.94%\n",
            "Epoch 52: training loss = 2.022824, training acuracy = 55.58%, test loss = 2.283861, training acuracy = 54.61%\n",
            "Epoch 53: training loss = 2.010626, training acuracy = 56.05%, test loss = 2.282969, training acuracy = 55.19%\n",
            "Epoch 54: training loss = 1.997994, training acuracy = 56.42%, test loss = 2.282034, training acuracy = 55.69%\n",
            "Epoch 55: training loss = 1.984927, training acuracy = 56.79%, test loss = 2.281056, training acuracy = 56.34%\n",
            "Epoch 56: training loss = 1.971429, training acuracy = 57.14%, test loss = 2.280032, training acuracy = 56.85%\n",
            "Epoch 57: training loss = 1.957503, training acuracy = 57.55%, test loss = 2.278961, training acuracy = 57.37%\n",
            "Epoch 58: training loss = 1.943156, training acuracy = 57.91%, test loss = 2.277843, training acuracy = 57.91%\n",
            "Epoch 59: training loss = 1.928396, training acuracy = 58.31%, test loss = 2.276676, training acuracy = 58.33%\n",
            "Epoch 60: training loss = 1.913235, training acuracy = 58.67%, test loss = 2.275459, training acuracy = 58.83%\n",
            "Epoch 61: training loss = 1.897686, training acuracy = 59.01%, test loss = 2.274193, training acuracy = 59.31%\n",
            "Epoch 62: training loss = 1.881765, training acuracy = 59.36%, test loss = 2.272875, training acuracy = 59.74%\n",
            "Epoch 63: training loss = 1.86549, training acuracy = 59.69%, test loss = 2.271507, training acuracy = 60.09%\n",
            "Epoch 64: training loss = 1.84888, training acuracy = 60.04%, test loss = 2.270088, training acuracy = 60.55%\n",
            "Epoch 65: training loss = 1.831958, training acuracy = 60.35%, test loss = 2.268617, training acuracy = 60.75%\n",
            "Epoch 66: training loss = 1.814748, training acuracy = 60.64%, test loss = 2.267096, training acuracy = 61.04%\n",
            "Epoch 67: training loss = 1.797273, training acuracy = 61.0%, test loss = 2.265525, training acuracy = 61.34%\n",
            "Epoch 68: training loss = 1.779561, training acuracy = 61.4%, test loss = 2.263904, training acuracy = 61.66%\n",
            "Epoch 69: training loss = 1.761639, training acuracy = 61.73%, test loss = 2.262235, training acuracy = 61.92%\n",
            "Epoch 70: training loss = 1.743536, training acuracy = 62.03%, test loss = 2.260518, training acuracy = 62.12%\n",
            "Epoch 71: training loss = 1.72528, training acuracy = 62.34%, test loss = 2.258755, training acuracy = 62.51%\n",
            "Epoch 72: training loss = 1.706902, training acuracy = 62.67%, test loss = 2.256947, training acuracy = 62.75%\n",
            "Epoch 73: training loss = 1.688431, training acuracy = 62.98%, test loss = 2.255096, training acuracy = 63.14%\n",
            "Epoch 74: training loss = 1.669897, training acuracy = 63.3%, test loss = 2.253204, training acuracy = 63.37%\n",
            "Epoch 75: training loss = 1.651329, training acuracy = 63.61%, test loss = 2.251272, training acuracy = 63.78%\n",
            "Epoch 76: training loss = 1.632757, training acuracy = 63.92%, test loss = 2.249303, training acuracy = 64.1%\n",
            "Epoch 77: training loss = 1.61421, training acuracy = 64.24%, test loss = 2.247299, training acuracy = 64.35%\n",
            "Epoch 78: training loss = 1.595714, training acuracy = 64.54%, test loss = 2.245261, training acuracy = 64.55%\n",
            "Epoch 79: training loss = 1.577297, training acuracy = 64.86%, test loss = 2.243193, training acuracy = 64.81%\n",
            "Epoch 80: training loss = 1.558984, training acuracy = 65.11%, test loss = 2.241097, training acuracy = 64.97%\n",
            "Epoch 81: training loss = 1.540798, training acuracy = 65.45%, test loss = 2.238974, training acuracy = 65.14%\n",
            "Epoch 82: training loss = 1.522763, training acuracy = 65.7%, test loss = 2.236828, training acuracy = 65.39%\n",
            "Epoch 83: training loss = 1.504898, training acuracy = 65.91%, test loss = 2.23466, training acuracy = 65.42%\n",
            "Epoch 84: training loss = 1.487225, training acuracy = 66.23%, test loss = 2.232474, training acuracy = 65.6%\n",
            "Epoch 85: training loss = 1.46976, training acuracy = 66.5%, test loss = 2.230271, training acuracy = 65.81%\n",
            "Epoch 86: training loss = 1.452518, training acuracy = 66.73%, test loss = 2.228054, training acuracy = 66.0%\n",
            "Epoch 87: training loss = 1.435516, training acuracy = 66.98%, test loss = 2.225826, training acuracy = 66.18%\n",
            "Epoch 88: training loss = 1.418764, training acuracy = 67.24%, test loss = 2.223588, training acuracy = 66.25%\n",
            "Epoch 89: training loss = 1.402274, training acuracy = 67.5%, test loss = 2.221342, training acuracy = 66.39%\n",
            "Epoch 90: training loss = 1.386057, training acuracy = 67.77%, test loss = 2.219092, training acuracy = 66.57%\n",
            "Epoch 91: training loss = 1.370119, training acuracy = 68.03%, test loss = 2.216838, training acuracy = 66.67%\n",
            "Epoch 92: training loss = 1.354467, training acuracy = 68.29%, test loss = 2.214583, training acuracy = 66.87%\n",
            "Epoch 93: training loss = 1.339106, training acuracy = 68.56%, test loss = 2.212328, training acuracy = 66.91%\n",
            "Epoch 94: training loss = 1.324042, training acuracy = 68.8%, test loss = 2.210076, training acuracy = 66.93%\n",
            "Epoch 95: training loss = 1.309275, training acuracy = 69.01%, test loss = 2.207829, training acuracy = 67.06%\n",
            "Epoch 96: training loss = 1.294808, training acuracy = 69.21%, test loss = 2.205586, training acuracy = 67.26%\n",
            "Epoch 97: training loss = 1.280642, training acuracy = 69.41%, test loss = 2.203352, training acuracy = 67.41%\n",
            "Epoch 98: training loss = 1.266776, training acuracy = 69.64%, test loss = 2.201125, training acuracy = 67.43%\n",
            "Epoch 99: training loss = 1.253208, training acuracy = 69.86%, test loss = 2.198909, training acuracy = 67.56%\n",
            "Epoch 100: training loss = 1.239938, training acuracy = 70.06%, test loss = 2.196703, training acuracy = 67.66%\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "learning_rate = 0.5\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  if i == 0:\n",
        "    params = foward_pass(x_train, params)\n",
        "    \n",
        "  grads = backward_pass(x_train, y_train, params)\n",
        "\n",
        "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
        "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
        "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
        "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
        "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
        "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
        "\n",
        "  params = foward_pass(x_train, params)\n",
        "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
        "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
        "\n",
        "  params_test = foward_pass_test(x_test, params)\n",
        "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
        "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
        "\n",
        "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n",
        "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaqqRzF73oBu"
      },
      "source": [
        "## 과제 4\n",
        "과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
        "\n",
        "- Hint : Activation function, hyperparameter setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "aYMJi8AoPwhz"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms, datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "6PI2vub8PyK9"
      },
      "outputs": [],
      "source": [
        "# 이미지를 텐서로 변경\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "b_AedUXiPzEL"
      },
      "outputs": [],
      "source": [
        "trainset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = True,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")\n",
        "testset = datasets.MNIST(\n",
        "    root      = './.data/', \n",
        "    train     = False,\n",
        "    download  = True,\n",
        "    transform = transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "r-L0KlKDP1Xt"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 512\n",
        "# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n",
        "# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader =  torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykzOb0e_M35L"
      },
      "source": [
        "### MLP 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "0Bq1HKFOP4Js"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(784,128)\n",
        "        self.layer2 = nn.Linear(128,64)\n",
        "        self.layer3 = nn.Linear(64,10)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        out = self.layer1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "3L-Aqf7en2_R"
      },
      "outputs": [],
      "source": [
        "model = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "ydaHoi-HP9t9"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    batch_losses = []\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        \n",
        "        output = model(data)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        batch_losses.append(loss)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "    \n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    \n",
        "    return avg_loss\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    batch_losses = []\n",
        "    correct = 0 \n",
        "\n",
        "    with torch.no_grad(): \n",
        "        for data, target in test_loader:\n",
        "            # 예측값 생성\n",
        "            output = model(data)\n",
        "\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            batch_losses.append(loss)\n",
        "\n",
        "\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "\n",
        "            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    ######## 배치 당 평균 loss 계산 ############\n",
        "    avg_loss =  sum(batch_losses) / len(batch_losses)\n",
        "\n",
        "    #정확도 계산\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-R5EDITNLYm"
      },
      "source": [
        "### 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "fVfEwSzWQEUM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] Train Loss: 0.0215\tTest Loss: 0.1235\tAccuracy: 97.58%\n",
            "[2] Train Loss: 0.0194\tTest Loss: 0.1214\tAccuracy: 97.48%\n",
            "[3] Train Loss: 0.0255\tTest Loss: 0.1291\tAccuracy: 97.60%\n",
            "[4] Train Loss: 0.0353\tTest Loss: 0.1147\tAccuracy: 97.52%\n",
            "[5] Train Loss: 0.0264\tTest Loss: 0.1271\tAccuracy: 97.70%\n",
            "[6] Train Loss: 0.0247\tTest Loss: 0.1384\tAccuracy: 97.50%\n",
            "[7] Train Loss: 0.0173\tTest Loss: 0.1270\tAccuracy: 97.83%\n",
            "[8] Train Loss: 0.0161\tTest Loss: 0.1683\tAccuracy: 97.30%\n",
            "[9] Train Loss: 0.0196\tTest Loss: 0.1261\tAccuracy: 97.68%\n",
            "[10] Train Loss: 0.0282\tTest Loss: 0.1303\tAccuracy: 97.58%\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train(model, train_loader, optimizer)\n",
        "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
        "    \n",
        "    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n",
        "          epoch, train_loss, test_loss, test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMIBQq7onH"
      },
      "source": [
        "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux7mPf6E78d4"
      },
      "source": [
        "activation function을 relu로 변경하여 gradient vanishing 문제를 해결함\n",
        "또한 optimizer를 Adam으로 설정하여 global local minimum에 빠지는 오류를 최소화함"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[0818] Deep Learning Basic_과제.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('spo2_project')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "582540dd4bc2c84b7fda8c629676a5f89b2fe97ef485f933b9740f8fa2ae0f5c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
