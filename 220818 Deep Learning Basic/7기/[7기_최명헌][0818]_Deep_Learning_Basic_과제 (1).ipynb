{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0818]_Deep_Learning_Basic_과제.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import sklearn.datasets"],"metadata":{"id":"N5hI9pAlZ75k","executionInfo":{"status":"ok","timestamp":1661069108797,"user_tz":-540,"elapsed":593,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def sigmoid(x):\n","    return 1/(1+np.exp(-x))\n","\n","def d_sigmoid(x):\n","    # derivative of sigmoid\n","    exp = np.exp(-x)\n","    return (exp)/((1+exp)**2)\n","\n","def softmax(x):\n","    exp = np.exp(x)\n","    return exp/np.sum(exp, axis=0)\n","\n","def compute_loss(y_true, y_pred):\n","    # loss calculation\n","\n","    num_sample = y_true.shape[1]\n","    Li = -1 * np.sum(y_true * np.log(y_pred))\n","\n","    return Li/num_sample\n","\n","def foward_pass(x, params, activation):\n","    if activation == 'relu':\n","        params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","        params[\"A1\"] = relu(params[\"S1\"])\n","        params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","        params[\"A2\"] = relu(params[\"S2\"])\n","        params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","        params[\"A3\"] = softmax(params[\"S3\"])\n","    elif activation == 'sigmoid':\n","        params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","        params[\"A1\"] = sigmoid(params[\"S1\"])\n","        params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n","        params[\"A2\"] = sigmoid(params[\"S2\"])\n","        params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n","        params[\"A3\"] = softmax(params[\"S3\"])\n","    return params\n","\n","def foward_pass_test(x, params, activation):\n","\n","    params_test = {}\n","    if activation == 'relu':\n","        params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","        params_test[\"A1\"] = relu(params_test[\"S1\"])\n","        params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","        params_test[\"A2\"] = relu(params_test[\"S2\"])\n","        params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","        params_test[\"A3\"] = softmax(params_test[\"S3\"])\n","    if activation == 'sigmoid':\n","        params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n","        params_test[\"A1\"] = sigmoid(params_test[\"S1\"])\n","        params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n","        params_test[\"A2\"] = sigmoid(params_test[\"S2\"])\n","        params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n","        params_test[\"A3\"] = softmax(params_test[\"S3\"])\n","    return params_test\n","\n","def compute_accuracy(y_true, y_pred):\n","    y_true_idx = np.argmax(y_true, axis = 0)\n","    y_pred_idx = np.argmax(y_pred, axis = 0)\n","    num_correct = np.sum(y_true_idx==y_pred_idx)\n","\n","    accuracy = num_correct / y_true.shape[1] * 100\n","\n","    return accuracy"],"metadata":{"id":"Z7YV5Bh6hc5r","executionInfo":{"status":"ok","timestamp":1661070653008,"user_tz":-540,"elapsed":372,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz","executionInfo":{"status":"ok","timestamp":1661070656204,"user_tz":-540,"elapsed":326,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"source":["def relu(x):\n","    return np.maximum(x, 0)"],"execution_count":32,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","    return np.where(x>0, 1, 0)"],"metadata":{"id":"Esm4jmTVijro","executionInfo":{"status":"ok","timestamp":1661070656616,"user_tz":-540,"elapsed":5,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs","executionInfo":{"status":"ok","timestamp":1661070657428,"user_tz":-540,"elapsed":2,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"source":["def backward_pass(x, y_true, params, activation):\n","    dS3 = params[\"A3\"] - y_true\n","    \n","    grads = {}\n","    if activation == 'relu':\n","        grads['dW3'] = np.dot(dS3, params['A2'].T) / x.shape[1]\n","        grads['db3'] = (1 / x.shape[1]) * np.sum(dS3, axis = 1, keepdims = True) / x.shape[1]\n","\n","        dA2 = np.dot(params['W3'].T, dS3)\n","        dS2 = dA2 * d_relu(params['S2'])\n","\n","        grads['dW2'] = np.dot(dS2, params['A1'].T) / x.shape[1]\n","        grads['db2'] = (1 / x.shape[1]) * np.sum(dS2, axis = 1, keepdims = True) / x.shape[1]\n","\n","        dA1 = np.dot(params['W2'].T, dS2)\n","        dS1 = dA1 * d_relu(params['S1'])\n","\n","        grads['dW1'] = np.dot(dS1, x.T) / x.shape[1]\n","        grads['db1'] = np.sum(dS1, axis = 1, keepdims = True) / x.shape[1]\n","    elif activation == 'sigmoid':\n","        grads['dW3'] = np.dot(dS3, params['A2'].T) / x.shape[1]\n","        grads['db3'] = (1 / x.shape[1]) * np.sum(dS3, axis = 1, keepdims = True) / x.shape[1]\n","\n","        dA2 = np.dot(params['W3'].T, dS3)\n","        dS2 = dA2 * d_sigmoid(params['S2'])\n","\n","        grads['dW2'] = np.dot(dS2, params['A1'].T) / x.shape[1]\n","        grads['db2'] = (1 / x.shape[1]) * np.sum(dS2, axis = 1, keepdims = True) / x.shape[1]\n","\n","        dA1 = np.dot(params['W2'].T, dS2)\n","        dS1 = dA1 * d_sigmoid(params['S1'])\n","\n","        grads['dW1'] = np.dot(dS1, x.T) / x.shape[1]\n","        grads['db1'] = np.sum(dS1, axis = 1, keepdims = True) / x.shape[1]\n","    return grads"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk","executionInfo":{"status":"ok","timestamp":1661070721790,"user_tz":-540,"elapsed":62490,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"source":["mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"],"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# data preprocessing\n","\n","num_train = 60000\n","num_class = 10\n","\n","x_train = np.float32(mnist.data[:num_train]).T\n","y_train_index = np.int32(mnist.target[:num_train]).T\n","x_test = np.float32(mnist.data[num_train:]).T\n","y_test_index = np.int32(mnist.target[num_train:]).T\n","\n","# Normalization\n","\n","x_train /= 255\n","x_test /= 255\n","x_size = x_train.shape[0]\n","\n","y_train = np.zeros((num_class, y_train_index.shape[0]))\n","for idx in range(y_train_index.shape[0]):\n","    y_train[y_train_index[idx], idx] = 1\n","\n","y_test = np.zeros((num_class, y_test_index.shape[0]))\n","for idx in range(y_test_index.shape[0]):\n","    y_test[y_test_index[idx], idx] = 1    "],"metadata":{"id":"oPmAf2Zxb2IF","executionInfo":{"status":"ok","timestamp":1661070722336,"user_tz":-540,"elapsed":592,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden_size_1 = 128\n","hidden_size_2 = 64 # hidden unit size\n","\n","# three-layer neural network\n","\n","params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n","          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n","          \"W3\": np.random.randn(num_class, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_2)\n","          \n","          }\n","# Xavier initialization: https://reniew.github.io/13/"],"metadata":{"id":"I0fGLs4wb11C","executionInfo":{"status":"ok","timestamp":1661070815209,"user_tz":-540,"elapsed":405,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","learning_rate = 0.5\n","activation = 'sigmoid'\n","for i in range(epochs):\n","\n","    if i == 0:\n","        params = foward_pass(x_train, params, activation)\n","\n","    grads = backward_pass(x_train, y_train, params, activation)\n","\n","    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","    params = foward_pass(x_train, params, activation)\n","    train_loss = compute_loss(y_train, params[\"A3\"])\n","    train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","    params_test = foward_pass_test(x_test, params, activation)\n","    test_loss = compute_loss(y_test, params_test[\"A3\"])\n","    test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","    .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VhZPSgHjc_Gt","executionInfo":{"status":"ok","timestamp":1661071033831,"user_tz":-540,"elapsed":217057,"user":{"displayName":"colab minds","userId":"00363918935231101942"}},"outputId":"0e793b83-3b9e-46e7-9a50-5b9919f709d7"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = 2.299048, training acuracy = 12.5%, test loss = 2.299367, training acuracy = 12.94%\n","Epoch 2: training loss = 2.295244, training acuracy = 11.24%, test loss = 2.29485, training acuracy = 11.35%\n","Epoch 3: training loss = 2.292805, training acuracy = 11.24%, test loss = 2.292325, training acuracy = 11.35%\n","Epoch 4: training loss = 2.29036, training acuracy = 11.24%, test loss = 2.289819, training acuracy = 11.35%\n","Epoch 5: training loss = 2.287899, training acuracy = 11.24%, test loss = 2.287299, training acuracy = 11.35%\n","Epoch 6: training loss = 2.285418, training acuracy = 11.26%, test loss = 2.284759, training acuracy = 11.35%\n","Epoch 7: training loss = 2.282912, training acuracy = 11.33%, test loss = 2.282193, training acuracy = 11.45%\n","Epoch 8: training loss = 2.280376, training acuracy = 11.56%, test loss = 2.279596, training acuracy = 11.77%\n","Epoch 9: training loss = 2.277806, training acuracy = 11.93%, test loss = 2.276965, training acuracy = 12.32%\n","Epoch 10: training loss = 2.275197, training acuracy = 12.59%, test loss = 2.274294, training acuracy = 13.09%\n","Epoch 11: training loss = 2.272545, training acuracy = 13.46%, test loss = 2.271578, training acuracy = 13.86%\n","Epoch 12: training loss = 2.269843, training acuracy = 14.69%, test loss = 2.268813, training acuracy = 15.28%\n","Epoch 13: training loss = 2.267088, training acuracy = 16.06%, test loss = 2.265992, training acuracy = 16.67%\n","Epoch 14: training loss = 2.264274, training acuracy = 17.55%, test loss = 2.263111, training acuracy = 18.3%\n","Epoch 15: training loss = 2.261395, training acuracy = 19.04%, test loss = 2.260164, training acuracy = 19.87%\n","Epoch 16: training loss = 2.258446, training acuracy = 20.39%, test loss = 2.257145, training acuracy = 21.36%\n","Epoch 17: training loss = 2.255422, training acuracy = 21.96%, test loss = 2.254049, training acuracy = 22.95%\n","Epoch 18: training loss = 2.252316, training acuracy = 23.38%, test loss = 2.25087, training acuracy = 24.34%\n","Epoch 19: training loss = 2.249121, training acuracy = 24.74%, test loss = 2.2476, training acuracy = 25.62%\n","Epoch 20: training loss = 2.245832, training acuracy = 26.11%, test loss = 2.244234, training acuracy = 27.05%\n","Epoch 21: training loss = 2.242442, training acuracy = 27.41%, test loss = 2.240764, training acuracy = 28.31%\n","Epoch 22: training loss = 2.238943, training acuracy = 28.69%, test loss = 2.237184, training acuracy = 29.54%\n","Epoch 23: training loss = 2.235329, training acuracy = 29.99%, test loss = 2.233485, training acuracy = 30.82%\n","Epoch 24: training loss = 2.231592, training acuracy = 31.3%, test loss = 2.22966, training acuracy = 32.02%\n","Epoch 25: training loss = 2.227723, training acuracy = 32.52%, test loss = 2.225702, training acuracy = 33.35%\n","Epoch 26: training loss = 2.223714, training acuracy = 33.82%, test loss = 2.2216, training acuracy = 34.64%\n","Epoch 27: training loss = 2.219556, training acuracy = 35.12%, test loss = 2.217346, training acuracy = 36.0%\n","Epoch 28: training loss = 2.215241, training acuracy = 36.37%, test loss = 2.212932, training acuracy = 37.43%\n","Epoch 29: training loss = 2.210758, training acuracy = 37.47%, test loss = 2.208346, training acuracy = 38.65%\n","Epoch 30: training loss = 2.206098, training acuracy = 38.66%, test loss = 2.203579, training acuracy = 39.68%\n","Epoch 31: training loss = 2.20125, training acuracy = 39.81%, test loss = 2.198621, training acuracy = 40.88%\n","Epoch 32: training loss = 2.196203, training acuracy = 40.93%, test loss = 2.193459, training acuracy = 41.96%\n","Epoch 33: training loss = 2.190945, training acuracy = 41.92%, test loss = 2.188082, training acuracy = 42.94%\n","Epoch 34: training loss = 2.185465, training acuracy = 42.79%, test loss = 2.182479, training acuracy = 43.9%\n","Epoch 35: training loss = 2.17975, training acuracy = 43.71%, test loss = 2.176637, training acuracy = 44.82%\n","Epoch 36: training loss = 2.173788, training acuracy = 44.54%, test loss = 2.170542, training acuracy = 45.5%\n","Epoch 37: training loss = 2.167565, training acuracy = 45.32%, test loss = 2.164181, training acuracy = 46.34%\n","Epoch 38: training loss = 2.161067, training acuracy = 46.04%, test loss = 2.15754, training acuracy = 47.14%\n","Epoch 39: training loss = 2.154281, training acuracy = 46.75%, test loss = 2.150605, training acuracy = 47.77%\n","Epoch 40: training loss = 2.147191, training acuracy = 47.43%, test loss = 2.143361, training acuracy = 48.42%\n","Epoch 41: training loss = 2.139783, training acuracy = 48.06%, test loss = 2.135793, training acuracy = 48.93%\n","Epoch 42: training loss = 2.132042, training acuracy = 48.7%, test loss = 2.127886, training acuracy = 49.51%\n","Epoch 43: training loss = 2.123953, training acuracy = 49.27%, test loss = 2.119625, training acuracy = 49.93%\n","Epoch 44: training loss = 2.1155, training acuracy = 49.77%, test loss = 2.110995, training acuracy = 50.47%\n","Epoch 45: training loss = 2.106669, training acuracy = 50.27%, test loss = 2.101979, training acuracy = 50.79%\n","Epoch 46: training loss = 2.097445, training acuracy = 50.67%, test loss = 2.092565, training acuracy = 51.19%\n","Epoch 47: training loss = 2.087813, training acuracy = 51.04%, test loss = 2.082736, training acuracy = 51.59%\n","Epoch 48: training loss = 2.077759, training acuracy = 51.4%, test loss = 2.07248, training acuracy = 51.9%\n","Epoch 49: training loss = 2.067271, training acuracy = 51.72%, test loss = 2.061783, training acuracy = 52.24%\n","Epoch 50: training loss = 2.056337, training acuracy = 52.05%, test loss = 2.050635, training acuracy = 52.48%\n","Epoch 51: training loss = 2.044947, training acuracy = 52.35%, test loss = 2.039024, training acuracy = 52.75%\n","Epoch 52: training loss = 2.033092, training acuracy = 52.62%, test loss = 2.026944, training acuracy = 53.13%\n","Epoch 53: training loss = 2.020764, training acuracy = 52.89%, test loss = 2.014386, training acuracy = 53.47%\n","Epoch 54: training loss = 2.007961, training acuracy = 53.18%, test loss = 2.001348, training acuracy = 53.69%\n","Epoch 55: training loss = 1.99468, training acuracy = 53.42%, test loss = 1.987828, training acuracy = 53.77%\n","Epoch 56: training loss = 1.980922, training acuracy = 53.68%, test loss = 1.973828, training acuracy = 53.97%\n","Epoch 57: training loss = 1.966692, training acuracy = 53.94%, test loss = 1.959355, training acuracy = 54.16%\n","Epoch 58: training loss = 1.951998, training acuracy = 54.17%, test loss = 1.944415, training acuracy = 54.41%\n","Epoch 59: training loss = 1.936852, training acuracy = 54.42%, test loss = 1.929024, training acuracy = 54.66%\n","Epoch 60: training loss = 1.92127, training acuracy = 54.66%, test loss = 1.913196, training acuracy = 54.86%\n","Epoch 61: training loss = 1.905272, training acuracy = 54.94%, test loss = 1.896954, training acuracy = 55.13%\n","Epoch 62: training loss = 1.88888, training acuracy = 55.24%, test loss = 1.880322, training acuracy = 55.3%\n","Epoch 63: training loss = 1.872124, training acuracy = 55.49%, test loss = 1.863329, training acuracy = 55.52%\n","Epoch 64: training loss = 1.855033, training acuracy = 55.73%, test loss = 1.846007, training acuracy = 55.84%\n","Epoch 65: training loss = 1.837644, training acuracy = 56.04%, test loss = 1.828392, training acuracy = 56.18%\n","Epoch 66: training loss = 1.819992, training acuracy = 56.3%, test loss = 1.810523, training acuracy = 56.5%\n","Epoch 67: training loss = 1.802119, training acuracy = 56.56%, test loss = 1.792441, training acuracy = 56.83%\n","Epoch 68: training loss = 1.784064, training acuracy = 56.84%, test loss = 1.774187, training acuracy = 57.04%\n","Epoch 69: training loss = 1.765872, training acuracy = 57.14%, test loss = 1.755806, training acuracy = 57.38%\n","Epoch 70: training loss = 1.747583, training acuracy = 57.48%, test loss = 1.737341, training acuracy = 57.75%\n","Epoch 71: training loss = 1.729241, training acuracy = 57.78%, test loss = 1.718834, training acuracy = 58.07%\n","Epoch 72: training loss = 1.710888, training acuracy = 58.15%, test loss = 1.700328, training acuracy = 58.45%\n","Epoch 73: training loss = 1.692561, training acuracy = 58.5%, test loss = 1.681862, training acuracy = 58.92%\n","Epoch 74: training loss = 1.674299, training acuracy = 58.86%, test loss = 1.663473, training acuracy = 59.31%\n","Epoch 75: training loss = 1.656135, training acuracy = 59.24%, test loss = 1.645196, training acuracy = 59.73%\n","Epoch 76: training loss = 1.638102, training acuracy = 59.65%, test loss = 1.627062, training acuracy = 60.1%\n","Epoch 77: training loss = 1.620225, training acuracy = 60.05%, test loss = 1.609099, training acuracy = 60.54%\n","Epoch 78: training loss = 1.602531, training acuracy = 60.38%, test loss = 1.591329, training acuracy = 60.95%\n","Epoch 79: training loss = 1.585039, training acuracy = 60.74%, test loss = 1.573774, training acuracy = 61.36%\n","Epoch 80: training loss = 1.567766, training acuracy = 61.15%, test loss = 1.556449, training acuracy = 61.72%\n","Epoch 81: training loss = 1.550725, training acuracy = 61.52%, test loss = 1.539368, training acuracy = 62.25%\n","Epoch 82: training loss = 1.533928, training acuracy = 61.95%, test loss = 1.52254, training acuracy = 62.72%\n","Epoch 83: training loss = 1.517381, training acuracy = 62.3%, test loss = 1.505972, training acuracy = 63.11%\n","Epoch 84: training loss = 1.501089, training acuracy = 62.66%, test loss = 1.489667, training acuracy = 63.6%\n","Epoch 85: training loss = 1.485054, training acuracy = 63.02%, test loss = 1.473628, training acuracy = 63.97%\n","Epoch 86: training loss = 1.469277, training acuracy = 63.42%, test loss = 1.457853, training acuracy = 64.34%\n","Epoch 87: training loss = 1.453755, training acuracy = 63.77%, test loss = 1.442341, training acuracy = 64.76%\n","Epoch 88: training loss = 1.438486, training acuracy = 64.1%, test loss = 1.427087, training acuracy = 65.08%\n","Epoch 89: training loss = 1.423465, training acuracy = 64.44%, test loss = 1.412086, training acuracy = 65.46%\n","Epoch 90: training loss = 1.408687, training acuracy = 64.74%, test loss = 1.397333, training acuracy = 65.82%\n","Epoch 91: training loss = 1.394146, training acuracy = 65.12%, test loss = 1.382821, training acuracy = 66.15%\n","Epoch 92: training loss = 1.379837, training acuracy = 65.47%, test loss = 1.368543, training acuracy = 66.49%\n","Epoch 93: training loss = 1.365752, training acuracy = 65.79%, test loss = 1.354492, training acuracy = 66.82%\n","Epoch 94: training loss = 1.351884, training acuracy = 66.14%, test loss = 1.340661, training acuracy = 67.06%\n","Epoch 95: training loss = 1.338228, training acuracy = 66.48%, test loss = 1.327043, training acuracy = 67.33%\n","Epoch 96: training loss = 1.324777, training acuracy = 66.84%, test loss = 1.313631, training acuracy = 67.62%\n","Epoch 97: training loss = 1.311524, training acuracy = 67.12%, test loss = 1.300419, training acuracy = 67.95%\n","Epoch 98: training loss = 1.298464, training acuracy = 67.39%, test loss = 1.287399, training acuracy = 68.23%\n","Epoch 99: training loss = 1.285592, training acuracy = 67.72%, test loss = 1.274567, training acuracy = 68.48%\n","Epoch 100: training loss = 1.272902, training acuracy = 68.07%, test loss = 1.261917, training acuracy = 68.73%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","source":["#parameter initialization\n","\n","hidden_size_1 = 128\n","hidden_size_2 = 64 # hidden unit size\n","\n","# three-layer neural network\n","\n","params = {\"W1\": np.random.randn(hidden_size_1, x_size) * np.sqrt(1/ x_size),\n","          \"b1\": np.zeros((hidden_size_1, 1)) * np.sqrt(1/ x_size),\n","          \"W2\": np.random.randn(hidden_size_2, hidden_size_1) * np.sqrt(1/ hidden_size_1),\n","          \"b2\": np.zeros((hidden_size_2, 1)) * np.sqrt(1/ hidden_size_1),\n","          \"W3\": np.random.randn(num_class, hidden_size_2) * np.sqrt(1/ hidden_size_2),\n","          \"b3\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size_2)\n","          \n","          }\n","# Xavier initialization: https://reniew.github.io/13/"],"metadata":{"id":"W60K6wyGjVxL","executionInfo":{"status":"ok","timestamp":1661071834535,"user_tz":-540,"elapsed":329,"user":{"displayName":"colab minds","userId":"00363918935231101942"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6b82DZG6W3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661072639504,"user_tz":-540,"elapsed":792945,"user":{"displayName":"colab minds","userId":"00363918935231101942"}},"outputId":"4a404755-9ae3-4ac4-80af-9eb250288ccc"},"source":["# Assignment 4 구현은 여기서 ()\n","epochs = 500\n","learning_rate = 0.1\n","activation = 'relu'\n","for i in range(epochs):\n","\n","    if i == 0:\n","        params = foward_pass(x_train, params, activation)\n","\n","    grads = backward_pass(x_train, y_train, params, activation)\n","\n","    params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n","    params[\"b1\"] -= learning_rate * grads[\"db1\"]\n","    params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n","    params[\"b2\"] -= learning_rate * grads[\"db2\"]\n","    params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n","    params[\"b3\"] -= learning_rate * grads[\"db3\"]\n","\n","    params = foward_pass(x_train, params, activation)\n","    train_loss = compute_loss(y_train, params[\"A3\"])\n","    train_acc = compute_accuracy(y_train, params[\"A3\"])\n","\n","    params_test = foward_pass_test(x_test, params, activation)\n","    test_loss = compute_loss(y_test, params_test[\"A3\"])\n","    test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n","\n","    print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, training acuracy = {}%\"\n","    .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: training loss = 2.289795, training acuracy = 16.4%, test loss = 2.287241, training acuracy = 17.03%\n","Epoch 2: training loss = 2.264904, training acuracy = 21.51%, test loss = 2.261363, training acuracy = 22.86%\n","Epoch 3: training loss = 2.242305, training acuracy = 26.53%, test loss = 2.237864, training acuracy = 28.29%\n","Epoch 4: training loss = 2.22084, training acuracy = 31.47%, test loss = 2.21558, training acuracy = 33.32%\n","Epoch 5: training loss = 2.199529, training acuracy = 36.17%, test loss = 2.193455, training acuracy = 38.18%\n","Epoch 6: training loss = 2.177743, training acuracy = 40.18%, test loss = 2.170841, training acuracy = 42.08%\n","Epoch 7: training loss = 2.155159, training acuracy = 43.48%, test loss = 2.14741, training acuracy = 45.5%\n","Epoch 8: training loss = 2.131483, training acuracy = 46.07%, test loss = 2.122914, training acuracy = 47.95%\n","Epoch 9: training loss = 2.106488, training acuracy = 48.17%, test loss = 2.097118, training acuracy = 49.93%\n","Epoch 10: training loss = 2.079968, training acuracy = 49.86%, test loss = 2.069736, training acuracy = 51.58%\n","Epoch 11: training loss = 2.051647, training acuracy = 51.3%, test loss = 2.040525, training acuracy = 53.17%\n","Epoch 12: training loss = 2.021395, training acuracy = 52.52%, test loss = 2.009329, training acuracy = 54.34%\n","Epoch 13: training loss = 1.989208, training acuracy = 53.61%, test loss = 1.976223, training acuracy = 55.45%\n","Epoch 14: training loss = 1.955169, training acuracy = 54.47%, test loss = 1.941277, training acuracy = 56.25%\n","Epoch 15: training loss = 1.919574, training acuracy = 55.35%, test loss = 1.904777, training acuracy = 56.96%\n","Epoch 16: training loss = 1.882689, training acuracy = 56.24%, test loss = 1.866999, training acuracy = 57.74%\n","Epoch 17: training loss = 1.844605, training acuracy = 57.06%, test loss = 1.827994, training acuracy = 58.42%\n","Epoch 18: training loss = 1.805395, training acuracy = 57.92%, test loss = 1.787887, training acuracy = 59.33%\n","Epoch 19: training loss = 1.765148, training acuracy = 58.93%, test loss = 1.746779, training acuracy = 59.93%\n","Epoch 20: training loss = 1.724011, training acuracy = 59.82%, test loss = 1.704821, training acuracy = 60.65%\n","Epoch 21: training loss = 1.682147, training acuracy = 60.73%, test loss = 1.662171, training acuracy = 61.51%\n","Epoch 22: training loss = 1.639771, training acuracy = 61.7%, test loss = 1.619043, training acuracy = 62.38%\n","Epoch 23: training loss = 1.597131, training acuracy = 62.75%, test loss = 1.575691, training acuracy = 63.43%\n","Epoch 24: training loss = 1.554469, training acuracy = 63.75%, test loss = 1.532382, training acuracy = 64.68%\n","Epoch 25: training loss = 1.512003, training acuracy = 64.82%, test loss = 1.489351, training acuracy = 65.73%\n","Epoch 26: training loss = 1.469981, training acuracy = 65.9%, test loss = 1.446826, training acuracy = 66.75%\n","Epoch 27: training loss = 1.428611, training acuracy = 67.05%, test loss = 1.404988, training acuracy = 67.95%\n","Epoch 28: training loss = 1.388063, training acuracy = 68.24%, test loss = 1.364005, training acuracy = 69.01%\n","Epoch 29: training loss = 1.348508, training acuracy = 69.28%, test loss = 1.324073, training acuracy = 70.03%\n","Epoch 30: training loss = 1.310081, training acuracy = 70.35%, test loss = 1.2853, training acuracy = 70.97%\n","Epoch 31: training loss = 1.272895, training acuracy = 71.36%, test loss = 1.247799, training acuracy = 72.24%\n","Epoch 32: training loss = 1.237026, training acuracy = 72.35%, test loss = 1.21165, training acuracy = 73.18%\n","Epoch 33: training loss = 1.20252, training acuracy = 73.35%, test loss = 1.176896, training acuracy = 74.08%\n","Epoch 34: training loss = 1.169406, training acuracy = 74.19%, test loss = 1.143576, training acuracy = 74.99%\n","Epoch 35: training loss = 1.137699, training acuracy = 75.01%, test loss = 1.111696, training acuracy = 75.94%\n","Epoch 36: training loss = 1.107391, training acuracy = 75.71%, test loss = 1.081244, training acuracy = 76.73%\n","Epoch 37: training loss = 1.078453, training acuracy = 76.39%, test loss = 1.052193, training acuracy = 77.29%\n","Epoch 38: training loss = 1.050858, training acuracy = 76.99%, test loss = 1.024506, training acuracy = 77.88%\n","Epoch 39: training loss = 1.024575, training acuracy = 77.51%, test loss = 0.99815, training acuracy = 78.4%\n","Epoch 40: training loss = 0.999557, training acuracy = 77.99%, test loss = 0.973074, training acuracy = 78.78%\n","Epoch 41: training loss = 0.975756, training acuracy = 78.41%, test loss = 0.949227, training acuracy = 79.32%\n","Epoch 42: training loss = 0.95312, training acuracy = 78.82%, test loss = 0.926562, training acuracy = 79.69%\n","Epoch 43: training loss = 0.931591, training acuracy = 79.22%, test loss = 0.90502, training acuracy = 79.92%\n","Epoch 44: training loss = 0.911117, training acuracy = 79.55%, test loss = 0.884548, training acuracy = 80.16%\n","Epoch 45: training loss = 0.891643, training acuracy = 79.91%, test loss = 0.865097, training acuracy = 80.51%\n","Epoch 46: training loss = 0.873123, training acuracy = 80.19%, test loss = 0.846614, training acuracy = 80.88%\n","Epoch 47: training loss = 0.855511, training acuracy = 80.48%, test loss = 0.829057, training acuracy = 81.21%\n","Epoch 48: training loss = 0.838753, training acuracy = 80.73%, test loss = 0.812362, training acuracy = 81.52%\n","Epoch 49: training loss = 0.822803, training acuracy = 80.99%, test loss = 0.796478, training acuracy = 81.71%\n","Epoch 50: training loss = 0.807615, training acuracy = 81.23%, test loss = 0.781355, training acuracy = 82.02%\n","Epoch 51: training loss = 0.793143, training acuracy = 81.44%, test loss = 0.766947, training acuracy = 82.24%\n","Epoch 52: training loss = 0.779344, training acuracy = 81.68%, test loss = 0.753212, training acuracy = 82.41%\n","Epoch 53: training loss = 0.766181, training acuracy = 81.84%, test loss = 0.740116, training acuracy = 82.64%\n","Epoch 54: training loss = 0.753617, training acuracy = 82.02%, test loss = 0.727615, training acuracy = 82.82%\n","Epoch 55: training loss = 0.741617, training acuracy = 82.25%, test loss = 0.715673, training acuracy = 82.97%\n","Epoch 56: training loss = 0.730144, training acuracy = 82.4%, test loss = 0.704263, training acuracy = 83.16%\n","Epoch 57: training loss = 0.719165, training acuracy = 82.61%, test loss = 0.693356, training acuracy = 83.4%\n","Epoch 58: training loss = 0.708651, training acuracy = 82.79%, test loss = 0.682913, training acuracy = 83.5%\n","Epoch 59: training loss = 0.698576, training acuracy = 82.93%, test loss = 0.672911, training acuracy = 83.62%\n","Epoch 60: training loss = 0.688914, training acuracy = 83.07%, test loss = 0.663322, training acuracy = 83.86%\n","Epoch 61: training loss = 0.679639, training acuracy = 83.26%, test loss = 0.654121, training acuracy = 83.99%\n","Epoch 62: training loss = 0.67073, training acuracy = 83.38%, test loss = 0.64529, training acuracy = 84.16%\n","Epoch 63: training loss = 0.662168, training acuracy = 83.52%, test loss = 0.636806, training acuracy = 84.26%\n","Epoch 64: training loss = 0.653935, training acuracy = 83.67%, test loss = 0.628649, training acuracy = 84.35%\n","Epoch 65: training loss = 0.646012, training acuracy = 83.79%, test loss = 0.6208, training acuracy = 84.5%\n","Epoch 66: training loss = 0.638381, training acuracy = 83.91%, test loss = 0.61324, training acuracy = 84.69%\n","Epoch 67: training loss = 0.631026, training acuracy = 84.1%, test loss = 0.605957, training acuracy = 84.82%\n","Epoch 68: training loss = 0.623933, training acuracy = 84.24%, test loss = 0.598936, training acuracy = 84.95%\n","Epoch 69: training loss = 0.617087, training acuracy = 84.36%, test loss = 0.592159, training acuracy = 85.07%\n","Epoch 70: training loss = 0.610477, training acuracy = 84.47%, test loss = 0.585616, training acuracy = 85.18%\n","Epoch 71: training loss = 0.604088, training acuracy = 84.6%, test loss = 0.579293, training acuracy = 85.25%\n","Epoch 72: training loss = 0.597909, training acuracy = 84.72%, test loss = 0.573182, training acuracy = 85.41%\n","Epoch 73: training loss = 0.591932, training acuracy = 84.82%, test loss = 0.56727, training acuracy = 85.51%\n","Epoch 74: training loss = 0.586146, training acuracy = 84.92%, test loss = 0.561549, training acuracy = 85.61%\n","Epoch 75: training loss = 0.58054, training acuracy = 85.01%, test loss = 0.556007, training acuracy = 85.75%\n","Epoch 76: training loss = 0.575106, training acuracy = 85.12%, test loss = 0.550637, training acuracy = 85.87%\n","Epoch 77: training loss = 0.569836, training acuracy = 85.24%, test loss = 0.545431, training acuracy = 85.99%\n","Epoch 78: training loss = 0.564724, training acuracy = 85.34%, test loss = 0.540383, training acuracy = 86.13%\n","Epoch 79: training loss = 0.55976, training acuracy = 85.43%, test loss = 0.535486, training acuracy = 86.2%\n","Epoch 80: training loss = 0.554941, training acuracy = 85.54%, test loss = 0.53073, training acuracy = 86.38%\n","Epoch 81: training loss = 0.550258, training acuracy = 85.63%, test loss = 0.526113, training acuracy = 86.44%\n","Epoch 82: training loss = 0.545706, training acuracy = 85.72%, test loss = 0.521625, training acuracy = 86.52%\n","Epoch 83: training loss = 0.541279, training acuracy = 85.82%, test loss = 0.517259, training acuracy = 86.58%\n","Epoch 84: training loss = 0.536973, training acuracy = 85.9%, test loss = 0.513013, training acuracy = 86.67%\n","Epoch 85: training loss = 0.532783, training acuracy = 85.98%, test loss = 0.508881, training acuracy = 86.74%\n","Epoch 86: training loss = 0.528704, training acuracy = 86.07%, test loss = 0.504862, training acuracy = 86.79%\n","Epoch 87: training loss = 0.52473, training acuracy = 86.16%, test loss = 0.500946, training acuracy = 86.89%\n","Epoch 88: training loss = 0.520859, training acuracy = 86.23%, test loss = 0.497132, training acuracy = 86.99%\n","Epoch 89: training loss = 0.517086, training acuracy = 86.31%, test loss = 0.493417, training acuracy = 87.11%\n","Epoch 90: training loss = 0.513407, training acuracy = 86.41%, test loss = 0.489793, training acuracy = 87.21%\n","Epoch 91: training loss = 0.50982, training acuracy = 86.5%, test loss = 0.486262, training acuracy = 87.27%\n","Epoch 92: training loss = 0.50632, training acuracy = 86.58%, test loss = 0.48282, training acuracy = 87.34%\n","Epoch 93: training loss = 0.502905, training acuracy = 86.68%, test loss = 0.479459, training acuracy = 87.46%\n","Epoch 94: training loss = 0.499571, training acuracy = 86.76%, test loss = 0.476181, training acuracy = 87.56%\n","Epoch 95: training loss = 0.496316, training acuracy = 86.82%, test loss = 0.472983, training acuracy = 87.62%\n","Epoch 96: training loss = 0.493137, training acuracy = 86.89%, test loss = 0.46986, training acuracy = 87.68%\n","Epoch 97: training loss = 0.490032, training acuracy = 86.96%, test loss = 0.46681, training acuracy = 87.79%\n","Epoch 98: training loss = 0.486998, training acuracy = 87.02%, test loss = 0.463831, training acuracy = 87.89%\n","Epoch 99: training loss = 0.484032, training acuracy = 87.08%, test loss = 0.46092, training acuracy = 87.93%\n","Epoch 100: training loss = 0.481132, training acuracy = 87.17%, test loss = 0.458078, training acuracy = 88.01%\n","Epoch 101: training loss = 0.478295, training acuracy = 87.23%, test loss = 0.455298, training acuracy = 88.06%\n","Epoch 102: training loss = 0.475521, training acuracy = 87.3%, test loss = 0.452581, training acuracy = 88.12%\n","Epoch 103: training loss = 0.472807, training acuracy = 87.38%, test loss = 0.449923, training acuracy = 88.2%\n","Epoch 104: training loss = 0.470152, training acuracy = 87.44%, test loss = 0.447327, training acuracy = 88.29%\n","Epoch 105: training loss = 0.467554, training acuracy = 87.51%, test loss = 0.444786, training acuracy = 88.3%\n","Epoch 106: training loss = 0.46501, training acuracy = 87.57%, test loss = 0.442302, training acuracy = 88.34%\n","Epoch 107: training loss = 0.462519, training acuracy = 87.62%, test loss = 0.43987, training acuracy = 88.35%\n","Epoch 108: training loss = 0.460079, training acuracy = 87.66%, test loss = 0.43749, training acuracy = 88.41%\n","Epoch 109: training loss = 0.457688, training acuracy = 87.71%, test loss = 0.435159, training acuracy = 88.52%\n","Epoch 110: training loss = 0.455347, training acuracy = 87.76%, test loss = 0.432879, training acuracy = 88.55%\n","Epoch 111: training loss = 0.453053, training acuracy = 87.8%, test loss = 0.430646, training acuracy = 88.61%\n","Epoch 112: training loss = 0.450806, training acuracy = 87.85%, test loss = 0.428458, training acuracy = 88.67%\n","Epoch 113: training loss = 0.448603, training acuracy = 87.9%, test loss = 0.426315, training acuracy = 88.75%\n","Epoch 114: training loss = 0.446443, training acuracy = 87.98%, test loss = 0.424216, training acuracy = 88.8%\n","Epoch 115: training loss = 0.444324, training acuracy = 88.02%, test loss = 0.422158, training acuracy = 88.82%\n","Epoch 116: training loss = 0.442246, training acuracy = 88.05%, test loss = 0.420141, training acuracy = 88.87%\n","Epoch 117: training loss = 0.440209, training acuracy = 88.08%, test loss = 0.418164, training acuracy = 88.91%\n","Epoch 118: training loss = 0.43821, training acuracy = 88.12%, test loss = 0.416225, training acuracy = 88.94%\n","Epoch 119: training loss = 0.436249, training acuracy = 88.17%, test loss = 0.414324, training acuracy = 89.02%\n","Epoch 120: training loss = 0.434324, training acuracy = 88.21%, test loss = 0.41246, training acuracy = 89.09%\n","Epoch 121: training loss = 0.432436, training acuracy = 88.26%, test loss = 0.410631, training acuracy = 89.12%\n","Epoch 122: training loss = 0.430582, training acuracy = 88.31%, test loss = 0.408838, training acuracy = 89.16%\n","Epoch 123: training loss = 0.428763, training acuracy = 88.35%, test loss = 0.407078, training acuracy = 89.16%\n","Epoch 124: training loss = 0.426976, training acuracy = 88.39%, test loss = 0.40535, training acuracy = 89.17%\n","Epoch 125: training loss = 0.425222, training acuracy = 88.42%, test loss = 0.403656, training acuracy = 89.21%\n","Epoch 126: training loss = 0.4235, training acuracy = 88.46%, test loss = 0.401991, training acuracy = 89.24%\n","Epoch 127: training loss = 0.421807, training acuracy = 88.49%, test loss = 0.400358, training acuracy = 89.31%\n","Epoch 128: training loss = 0.420145, training acuracy = 88.54%, test loss = 0.398754, training acuracy = 89.33%\n","Epoch 129: training loss = 0.418511, training acuracy = 88.58%, test loss = 0.397179, training acuracy = 89.39%\n","Epoch 130: training loss = 0.416906, training acuracy = 88.61%, test loss = 0.395632, training acuracy = 89.41%\n","Epoch 131: training loss = 0.415328, training acuracy = 88.64%, test loss = 0.394112, training acuracy = 89.41%\n","Epoch 132: training loss = 0.413777, training acuracy = 88.68%, test loss = 0.39262, training acuracy = 89.48%\n","Epoch 133: training loss = 0.412252, training acuracy = 88.73%, test loss = 0.391153, training acuracy = 89.53%\n","Epoch 134: training loss = 0.410752, training acuracy = 88.76%, test loss = 0.389711, training acuracy = 89.59%\n","Epoch 135: training loss = 0.409277, training acuracy = 88.8%, test loss = 0.388294, training acuracy = 89.6%\n","Epoch 136: training loss = 0.407827, training acuracy = 88.83%, test loss = 0.386901, training acuracy = 89.61%\n","Epoch 137: training loss = 0.4064, training acuracy = 88.86%, test loss = 0.385532, training acuracy = 89.66%\n","Epoch 138: training loss = 0.404996, training acuracy = 88.89%, test loss = 0.384185, training acuracy = 89.69%\n","Epoch 139: training loss = 0.403615, training acuracy = 88.92%, test loss = 0.382862, training acuracy = 89.69%\n","Epoch 140: training loss = 0.402256, training acuracy = 88.95%, test loss = 0.381561, training acuracy = 89.7%\n","Epoch 141: training loss = 0.400919, training acuracy = 88.98%, test loss = 0.380281, training acuracy = 89.73%\n","Epoch 142: training loss = 0.399603, training acuracy = 89.02%, test loss = 0.379022, training acuracy = 89.71%\n","Epoch 143: training loss = 0.398308, training acuracy = 89.06%, test loss = 0.377783, training acuracy = 89.73%\n","Epoch 144: training loss = 0.397033, training acuracy = 89.08%, test loss = 0.376565, training acuracy = 89.74%\n","Epoch 145: training loss = 0.395777, training acuracy = 89.11%, test loss = 0.375367, training acuracy = 89.79%\n","Epoch 146: training loss = 0.39454, training acuracy = 89.13%, test loss = 0.374186, training acuracy = 89.8%\n","Epoch 147: training loss = 0.393322, training acuracy = 89.15%, test loss = 0.373026, training acuracy = 89.83%\n","Epoch 148: training loss = 0.392122, training acuracy = 89.16%, test loss = 0.371884, training acuracy = 89.83%\n","Epoch 149: training loss = 0.39094, training acuracy = 89.19%, test loss = 0.370759, training acuracy = 89.88%\n","Epoch 150: training loss = 0.389775, training acuracy = 89.21%, test loss = 0.369652, training acuracy = 89.9%\n","Epoch 151: training loss = 0.388628, training acuracy = 89.25%, test loss = 0.36856, training acuracy = 89.92%\n","Epoch 152: training loss = 0.387496, training acuracy = 89.26%, test loss = 0.367485, training acuracy = 89.92%\n","Epoch 153: training loss = 0.386381, training acuracy = 89.29%, test loss = 0.366426, training acuracy = 89.97%\n","Epoch 154: training loss = 0.385282, training acuracy = 89.32%, test loss = 0.365383, training acuracy = 90.02%\n","Epoch 155: training loss = 0.384198, training acuracy = 89.34%, test loss = 0.364354, training acuracy = 90.03%\n","Epoch 156: training loss = 0.383129, training acuracy = 89.37%, test loss = 0.363341, training acuracy = 90.06%\n","Epoch 157: training loss = 0.382075, training acuracy = 89.41%, test loss = 0.362343, training acuracy = 90.11%\n","Epoch 158: training loss = 0.381035, training acuracy = 89.44%, test loss = 0.361359, training acuracy = 90.12%\n","Epoch 159: training loss = 0.38001, training acuracy = 89.46%, test loss = 0.360388, training acuracy = 90.12%\n","Epoch 160: training loss = 0.378999, training acuracy = 89.49%, test loss = 0.359431, training acuracy = 90.15%\n","Epoch 161: training loss = 0.378002, training acuracy = 89.5%, test loss = 0.358488, training acuracy = 90.18%\n","Epoch 162: training loss = 0.377018, training acuracy = 89.53%, test loss = 0.357557, training acuracy = 90.2%\n","Epoch 163: training loss = 0.376047, training acuracy = 89.56%, test loss = 0.35664, training acuracy = 90.22%\n","Epoch 164: training loss = 0.375089, training acuracy = 89.59%, test loss = 0.355736, training acuracy = 90.24%\n","Epoch 165: training loss = 0.374143, training acuracy = 89.6%, test loss = 0.354843, training acuracy = 90.26%\n","Epoch 166: training loss = 0.37321, training acuracy = 89.6%, test loss = 0.353965, training acuracy = 90.27%\n","Epoch 167: training loss = 0.372288, training acuracy = 89.64%, test loss = 0.353097, training acuracy = 90.3%\n","Epoch 168: training loss = 0.371378, training acuracy = 89.66%, test loss = 0.352241, training acuracy = 90.33%\n","Epoch 169: training loss = 0.370478, training acuracy = 89.69%, test loss = 0.351395, training acuracy = 90.35%\n","Epoch 170: training loss = 0.36959, training acuracy = 89.72%, test loss = 0.350559, training acuracy = 90.36%\n","Epoch 171: training loss = 0.368713, training acuracy = 89.74%, test loss = 0.349735, training acuracy = 90.36%\n","Epoch 172: training loss = 0.367845, training acuracy = 89.77%, test loss = 0.348921, training acuracy = 90.38%\n","Epoch 173: training loss = 0.366988, training acuracy = 89.78%, test loss = 0.348115, training acuracy = 90.39%\n","Epoch 174: training loss = 0.36614, training acuracy = 89.8%, test loss = 0.34732, training acuracy = 90.4%\n","Epoch 175: training loss = 0.365303, training acuracy = 89.82%, test loss = 0.346535, training acuracy = 90.41%\n","Epoch 176: training loss = 0.364475, training acuracy = 89.83%, test loss = 0.345759, training acuracy = 90.41%\n","Epoch 177: training loss = 0.363657, training acuracy = 89.85%, test loss = 0.344991, training acuracy = 90.42%\n","Epoch 178: training loss = 0.362848, training acuracy = 89.87%, test loss = 0.344234, training acuracy = 90.41%\n","Epoch 179: training loss = 0.362049, training acuracy = 89.89%, test loss = 0.343484, training acuracy = 90.45%\n","Epoch 180: training loss = 0.361259, training acuracy = 89.92%, test loss = 0.342743, training acuracy = 90.46%\n","Epoch 181: training loss = 0.360477, training acuracy = 89.95%, test loss = 0.342011, training acuracy = 90.51%\n","Epoch 182: training loss = 0.359704, training acuracy = 89.98%, test loss = 0.341287, training acuracy = 90.55%\n","Epoch 183: training loss = 0.35894, training acuracy = 89.98%, test loss = 0.340573, training acuracy = 90.56%\n","Epoch 184: training loss = 0.358184, training acuracy = 90.01%, test loss = 0.339867, training acuracy = 90.55%\n","Epoch 185: training loss = 0.357437, training acuracy = 90.02%, test loss = 0.339169, training acuracy = 90.57%\n","Epoch 186: training loss = 0.356696, training acuracy = 90.04%, test loss = 0.338479, training acuracy = 90.58%\n","Epoch 187: training loss = 0.355964, training acuracy = 90.05%, test loss = 0.337796, training acuracy = 90.59%\n","Epoch 188: training loss = 0.355239, training acuracy = 90.08%, test loss = 0.337121, training acuracy = 90.63%\n","Epoch 189: training loss = 0.354522, training acuracy = 90.1%, test loss = 0.336454, training acuracy = 90.64%\n","Epoch 190: training loss = 0.353813, training acuracy = 90.12%, test loss = 0.335794, training acuracy = 90.65%\n","Epoch 191: training loss = 0.353111, training acuracy = 90.14%, test loss = 0.335142, training acuracy = 90.66%\n","Epoch 192: training loss = 0.352415, training acuracy = 90.14%, test loss = 0.334495, training acuracy = 90.67%\n","Epoch 193: training loss = 0.351727, training acuracy = 90.16%, test loss = 0.333855, training acuracy = 90.71%\n","Epoch 194: training loss = 0.351046, training acuracy = 90.17%, test loss = 0.333221, training acuracy = 90.72%\n","Epoch 195: training loss = 0.350372, training acuracy = 90.18%, test loss = 0.332594, training acuracy = 90.73%\n","Epoch 196: training loss = 0.349705, training acuracy = 90.2%, test loss = 0.331974, training acuracy = 90.75%\n","Epoch 197: training loss = 0.349044, training acuracy = 90.21%, test loss = 0.331361, training acuracy = 90.75%\n","Epoch 198: training loss = 0.348389, training acuracy = 90.23%, test loss = 0.330754, training acuracy = 90.75%\n","Epoch 199: training loss = 0.347741, training acuracy = 90.24%, test loss = 0.330152, training acuracy = 90.78%\n","Epoch 200: training loss = 0.347099, training acuracy = 90.26%, test loss = 0.329557, training acuracy = 90.78%\n","Epoch 201: training loss = 0.346463, training acuracy = 90.29%, test loss = 0.328969, training acuracy = 90.78%\n","Epoch 202: training loss = 0.345833, training acuracy = 90.3%, test loss = 0.328386, training acuracy = 90.81%\n","Epoch 203: training loss = 0.345209, training acuracy = 90.3%, test loss = 0.327809, training acuracy = 90.84%\n","Epoch 204: training loss = 0.34459, training acuracy = 90.32%, test loss = 0.327238, training acuracy = 90.88%\n","Epoch 205: training loss = 0.343978, training acuracy = 90.34%, test loss = 0.32667, training acuracy = 90.9%\n","Epoch 206: training loss = 0.343371, training acuracy = 90.35%, test loss = 0.326108, training acuracy = 90.9%\n","Epoch 207: training loss = 0.342769, training acuracy = 90.36%, test loss = 0.325553, training acuracy = 90.92%\n","Epoch 208: training loss = 0.342172, training acuracy = 90.37%, test loss = 0.325002, training acuracy = 90.93%\n","Epoch 209: training loss = 0.341581, training acuracy = 90.38%, test loss = 0.324456, training acuracy = 90.95%\n","Epoch 210: training loss = 0.340994, training acuracy = 90.41%, test loss = 0.323915, training acuracy = 90.96%\n","Epoch 211: training loss = 0.340413, training acuracy = 90.42%, test loss = 0.32338, training acuracy = 90.96%\n","Epoch 212: training loss = 0.339837, training acuracy = 90.44%, test loss = 0.32285, training acuracy = 90.96%\n","Epoch 213: training loss = 0.339266, training acuracy = 90.45%, test loss = 0.322324, training acuracy = 90.97%\n","Epoch 214: training loss = 0.3387, training acuracy = 90.47%, test loss = 0.321803, training acuracy = 90.98%\n","Epoch 215: training loss = 0.338138, training acuracy = 90.48%, test loss = 0.321287, training acuracy = 91.0%\n","Epoch 216: training loss = 0.337582, training acuracy = 90.49%, test loss = 0.320776, training acuracy = 91.01%\n","Epoch 217: training loss = 0.33703, training acuracy = 90.5%, test loss = 0.320268, training acuracy = 91.04%\n","Epoch 218: training loss = 0.336482, training acuracy = 90.51%, test loss = 0.319763, training acuracy = 91.02%\n","Epoch 219: training loss = 0.335939, training acuracy = 90.52%, test loss = 0.319264, training acuracy = 91.04%\n","Epoch 220: training loss = 0.335399, training acuracy = 90.54%, test loss = 0.318768, training acuracy = 91.05%\n","Epoch 221: training loss = 0.334864, training acuracy = 90.56%, test loss = 0.318277, training acuracy = 91.05%\n","Epoch 222: training loss = 0.334334, training acuracy = 90.56%, test loss = 0.317788, training acuracy = 91.05%\n","Epoch 223: training loss = 0.333807, training acuracy = 90.58%, test loss = 0.317305, training acuracy = 91.05%\n","Epoch 224: training loss = 0.333284, training acuracy = 90.59%, test loss = 0.316823, training acuracy = 91.07%\n","Epoch 225: training loss = 0.332765, training acuracy = 90.62%, test loss = 0.316346, training acuracy = 91.08%\n","Epoch 226: training loss = 0.332251, training acuracy = 90.62%, test loss = 0.315873, training acuracy = 91.1%\n","Epoch 227: training loss = 0.33174, training acuracy = 90.64%, test loss = 0.315404, training acuracy = 91.12%\n","Epoch 228: training loss = 0.331233, training acuracy = 90.64%, test loss = 0.314938, training acuracy = 91.14%\n","Epoch 229: training loss = 0.330729, training acuracy = 90.65%, test loss = 0.314477, training acuracy = 91.15%\n","Epoch 230: training loss = 0.33023, training acuracy = 90.66%, test loss = 0.314019, training acuracy = 91.15%\n","Epoch 231: training loss = 0.329733, training acuracy = 90.68%, test loss = 0.313565, training acuracy = 91.17%\n","Epoch 232: training loss = 0.329241, training acuracy = 90.7%, test loss = 0.313114, training acuracy = 91.2%\n","Epoch 233: training loss = 0.328752, training acuracy = 90.72%, test loss = 0.312666, training acuracy = 91.2%\n","Epoch 234: training loss = 0.328267, training acuracy = 90.73%, test loss = 0.312222, training acuracy = 91.2%\n","Epoch 235: training loss = 0.327786, training acuracy = 90.74%, test loss = 0.311783, training acuracy = 91.2%\n","Epoch 236: training loss = 0.327307, training acuracy = 90.75%, test loss = 0.311346, training acuracy = 91.21%\n","Epoch 237: training loss = 0.326832, training acuracy = 90.76%, test loss = 0.310913, training acuracy = 91.24%\n","Epoch 238: training loss = 0.32636, training acuracy = 90.76%, test loss = 0.310481, training acuracy = 91.27%\n","Epoch 239: training loss = 0.325891, training acuracy = 90.78%, test loss = 0.310052, training acuracy = 91.28%\n","Epoch 240: training loss = 0.325425, training acuracy = 90.78%, test loss = 0.309627, training acuracy = 91.3%\n","Epoch 241: training loss = 0.324962, training acuracy = 90.79%, test loss = 0.309204, training acuracy = 91.3%\n","Epoch 242: training loss = 0.324503, training acuracy = 90.8%, test loss = 0.308784, training acuracy = 91.31%\n","Epoch 243: training loss = 0.324046, training acuracy = 90.82%, test loss = 0.308367, training acuracy = 91.32%\n","Epoch 244: training loss = 0.323592, training acuracy = 90.83%, test loss = 0.307954, training acuracy = 91.35%\n","Epoch 245: training loss = 0.323142, training acuracy = 90.84%, test loss = 0.307544, training acuracy = 91.37%\n","Epoch 246: training loss = 0.322694, training acuracy = 90.85%, test loss = 0.307135, training acuracy = 91.38%\n","Epoch 247: training loss = 0.322249, training acuracy = 90.87%, test loss = 0.306729, training acuracy = 91.39%\n","Epoch 248: training loss = 0.321808, training acuracy = 90.88%, test loss = 0.306326, training acuracy = 91.4%\n","Epoch 249: training loss = 0.321369, training acuracy = 90.89%, test loss = 0.305926, training acuracy = 91.39%\n","Epoch 250: training loss = 0.320933, training acuracy = 90.91%, test loss = 0.305529, training acuracy = 91.42%\n","Epoch 251: training loss = 0.320499, training acuracy = 90.93%, test loss = 0.305135, training acuracy = 91.45%\n","Epoch 252: training loss = 0.320069, training acuracy = 90.94%, test loss = 0.304744, training acuracy = 91.47%\n","Epoch 253: training loss = 0.319641, training acuracy = 90.95%, test loss = 0.304354, training acuracy = 91.46%\n","Epoch 254: training loss = 0.319216, training acuracy = 90.96%, test loss = 0.303968, training acuracy = 91.47%\n","Epoch 255: training loss = 0.318793, training acuracy = 90.97%, test loss = 0.303585, training acuracy = 91.48%\n","Epoch 256: training loss = 0.318373, training acuracy = 90.98%, test loss = 0.303204, training acuracy = 91.48%\n","Epoch 257: training loss = 0.317956, training acuracy = 90.99%, test loss = 0.302826, training acuracy = 91.49%\n","Epoch 258: training loss = 0.317541, training acuracy = 91.0%, test loss = 0.30245, training acuracy = 91.5%\n","Epoch 259: training loss = 0.317128, training acuracy = 91.01%, test loss = 0.302076, training acuracy = 91.5%\n","Epoch 260: training loss = 0.316718, training acuracy = 91.02%, test loss = 0.301704, training acuracy = 91.52%\n","Epoch 261: training loss = 0.31631, training acuracy = 91.02%, test loss = 0.301334, training acuracy = 91.54%\n","Epoch 262: training loss = 0.315904, training acuracy = 91.04%, test loss = 0.300965, training acuracy = 91.54%\n","Epoch 263: training loss = 0.3155, training acuracy = 91.05%, test loss = 0.300599, training acuracy = 91.54%\n","Epoch 264: training loss = 0.315099, training acuracy = 91.06%, test loss = 0.300236, training acuracy = 91.54%\n","Epoch 265: training loss = 0.3147, training acuracy = 91.07%, test loss = 0.299876, training acuracy = 91.55%\n","Epoch 266: training loss = 0.314303, training acuracy = 91.08%, test loss = 0.299517, training acuracy = 91.55%\n","Epoch 267: training loss = 0.313908, training acuracy = 91.09%, test loss = 0.299159, training acuracy = 91.56%\n","Epoch 268: training loss = 0.313516, training acuracy = 91.1%, test loss = 0.298805, training acuracy = 91.56%\n","Epoch 269: training loss = 0.313125, training acuracy = 91.12%, test loss = 0.298452, training acuracy = 91.59%\n","Epoch 270: training loss = 0.312736, training acuracy = 91.13%, test loss = 0.298102, training acuracy = 91.63%\n","Epoch 271: training loss = 0.31235, training acuracy = 91.14%, test loss = 0.297752, training acuracy = 91.64%\n","Epoch 272: training loss = 0.311965, training acuracy = 91.14%, test loss = 0.297405, training acuracy = 91.65%\n","Epoch 273: training loss = 0.311582, training acuracy = 91.16%, test loss = 0.297061, training acuracy = 91.66%\n","Epoch 274: training loss = 0.311201, training acuracy = 91.17%, test loss = 0.296717, training acuracy = 91.66%\n","Epoch 275: training loss = 0.310822, training acuracy = 91.19%, test loss = 0.296375, training acuracy = 91.66%\n","Epoch 276: training loss = 0.310445, training acuracy = 91.2%, test loss = 0.296036, training acuracy = 91.67%\n","Epoch 277: training loss = 0.310069, training acuracy = 91.22%, test loss = 0.295698, training acuracy = 91.67%\n","Epoch 278: training loss = 0.309696, training acuracy = 91.23%, test loss = 0.295362, training acuracy = 91.69%\n","Epoch 279: training loss = 0.309324, training acuracy = 91.24%, test loss = 0.295027, training acuracy = 91.69%\n","Epoch 280: training loss = 0.308955, training acuracy = 91.25%, test loss = 0.294694, training acuracy = 91.71%\n","Epoch 281: training loss = 0.308587, training acuracy = 91.26%, test loss = 0.294363, training acuracy = 91.72%\n","Epoch 282: training loss = 0.308221, training acuracy = 91.27%, test loss = 0.294033, training acuracy = 91.73%\n","Epoch 283: training loss = 0.307856, training acuracy = 91.28%, test loss = 0.293705, training acuracy = 91.75%\n","Epoch 284: training loss = 0.307494, training acuracy = 91.28%, test loss = 0.293379, training acuracy = 91.76%\n","Epoch 285: training loss = 0.307133, training acuracy = 91.29%, test loss = 0.293054, training acuracy = 91.77%\n","Epoch 286: training loss = 0.306773, training acuracy = 91.29%, test loss = 0.292731, training acuracy = 91.79%\n","Epoch 287: training loss = 0.306416, training acuracy = 91.3%, test loss = 0.292409, training acuracy = 91.82%\n","Epoch 288: training loss = 0.306059, training acuracy = 91.32%, test loss = 0.292088, training acuracy = 91.82%\n","Epoch 289: training loss = 0.305705, training acuracy = 91.33%, test loss = 0.291768, training acuracy = 91.84%\n","Epoch 290: training loss = 0.305352, training acuracy = 91.34%, test loss = 0.291451, training acuracy = 91.84%\n","Epoch 291: training loss = 0.305, training acuracy = 91.35%, test loss = 0.291134, training acuracy = 91.85%\n","Epoch 292: training loss = 0.30465, training acuracy = 91.36%, test loss = 0.290819, training acuracy = 91.85%\n","Epoch 293: training loss = 0.304301, training acuracy = 91.36%, test loss = 0.290506, training acuracy = 91.87%\n","Epoch 294: training loss = 0.303954, training acuracy = 91.37%, test loss = 0.290194, training acuracy = 91.88%\n","Epoch 295: training loss = 0.303609, training acuracy = 91.38%, test loss = 0.289884, training acuracy = 91.88%\n","Epoch 296: training loss = 0.303265, training acuracy = 91.4%, test loss = 0.289574, training acuracy = 91.88%\n","Epoch 297: training loss = 0.302923, training acuracy = 91.41%, test loss = 0.289267, training acuracy = 91.9%\n","Epoch 298: training loss = 0.302582, training acuracy = 91.42%, test loss = 0.288961, training acuracy = 91.9%\n","Epoch 299: training loss = 0.302242, training acuracy = 91.42%, test loss = 0.288656, training acuracy = 91.91%\n","Epoch 300: training loss = 0.301905, training acuracy = 91.44%, test loss = 0.288353, training acuracy = 91.91%\n","Epoch 301: training loss = 0.301569, training acuracy = 91.45%, test loss = 0.288052, training acuracy = 91.91%\n","Epoch 302: training loss = 0.301234, training acuracy = 91.45%, test loss = 0.287751, training acuracy = 91.91%\n","Epoch 303: training loss = 0.300901, training acuracy = 91.46%, test loss = 0.287451, training acuracy = 91.92%\n","Epoch 304: training loss = 0.300569, training acuracy = 91.48%, test loss = 0.287154, training acuracy = 91.93%\n","Epoch 305: training loss = 0.300239, training acuracy = 91.48%, test loss = 0.286856, training acuracy = 91.94%\n","Epoch 306: training loss = 0.29991, training acuracy = 91.49%, test loss = 0.28656, training acuracy = 91.94%\n","Epoch 307: training loss = 0.299582, training acuracy = 91.5%, test loss = 0.286267, training acuracy = 91.94%\n","Epoch 308: training loss = 0.299255, training acuracy = 91.51%, test loss = 0.285974, training acuracy = 91.94%\n","Epoch 309: training loss = 0.29893, training acuracy = 91.52%, test loss = 0.285682, training acuracy = 91.94%\n","Epoch 310: training loss = 0.298606, training acuracy = 91.53%, test loss = 0.285392, training acuracy = 91.96%\n","Epoch 311: training loss = 0.298283, training acuracy = 91.54%, test loss = 0.285104, training acuracy = 91.97%\n","Epoch 312: training loss = 0.297962, training acuracy = 91.56%, test loss = 0.284816, training acuracy = 91.95%\n","Epoch 313: training loss = 0.297641, training acuracy = 91.57%, test loss = 0.284531, training acuracy = 91.96%\n","Epoch 314: training loss = 0.297322, training acuracy = 91.58%, test loss = 0.284246, training acuracy = 91.97%\n","Epoch 315: training loss = 0.297004, training acuracy = 91.59%, test loss = 0.283963, training acuracy = 91.97%\n","Epoch 316: training loss = 0.296688, training acuracy = 91.6%, test loss = 0.283681, training acuracy = 91.95%\n","Epoch 317: training loss = 0.296373, training acuracy = 91.61%, test loss = 0.2834, training acuracy = 91.94%\n","Epoch 318: training loss = 0.296059, training acuracy = 91.62%, test loss = 0.283119, training acuracy = 91.95%\n","Epoch 319: training loss = 0.295747, training acuracy = 91.62%, test loss = 0.282841, training acuracy = 91.98%\n","Epoch 320: training loss = 0.295436, training acuracy = 91.63%, test loss = 0.282564, training acuracy = 91.98%\n","Epoch 321: training loss = 0.295126, training acuracy = 91.64%, test loss = 0.282288, training acuracy = 91.97%\n","Epoch 322: training loss = 0.294817, training acuracy = 91.65%, test loss = 0.282013, training acuracy = 91.99%\n","Epoch 323: training loss = 0.294509, training acuracy = 91.66%, test loss = 0.281738, training acuracy = 91.99%\n","Epoch 324: training loss = 0.294203, training acuracy = 91.66%, test loss = 0.281465, training acuracy = 91.99%\n","Epoch 325: training loss = 0.293898, training acuracy = 91.66%, test loss = 0.281193, training acuracy = 92.0%\n","Epoch 326: training loss = 0.293594, training acuracy = 91.67%, test loss = 0.280921, training acuracy = 91.99%\n","Epoch 327: training loss = 0.293291, training acuracy = 91.68%, test loss = 0.280651, training acuracy = 92.0%\n","Epoch 328: training loss = 0.292989, training acuracy = 91.69%, test loss = 0.280383, training acuracy = 92.0%\n","Epoch 329: training loss = 0.292688, training acuracy = 91.69%, test loss = 0.280115, training acuracy = 92.01%\n","Epoch 330: training loss = 0.292388, training acuracy = 91.71%, test loss = 0.279848, training acuracy = 92.02%\n","Epoch 331: training loss = 0.29209, training acuracy = 91.72%, test loss = 0.279582, training acuracy = 92.02%\n","Epoch 332: training loss = 0.291792, training acuracy = 91.73%, test loss = 0.279317, training acuracy = 92.04%\n","Epoch 333: training loss = 0.291495, training acuracy = 91.74%, test loss = 0.279052, training acuracy = 92.05%\n","Epoch 334: training loss = 0.2912, training acuracy = 91.76%, test loss = 0.278789, training acuracy = 92.05%\n","Epoch 335: training loss = 0.290905, training acuracy = 91.76%, test loss = 0.278526, training acuracy = 92.06%\n","Epoch 336: training loss = 0.290612, training acuracy = 91.77%, test loss = 0.278265, training acuracy = 92.07%\n","Epoch 337: training loss = 0.290319, training acuracy = 91.78%, test loss = 0.278005, training acuracy = 92.08%\n","Epoch 338: training loss = 0.290027, training acuracy = 91.78%, test loss = 0.277746, training acuracy = 92.08%\n","Epoch 339: training loss = 0.289737, training acuracy = 91.79%, test loss = 0.277488, training acuracy = 92.09%\n","Epoch 340: training loss = 0.289447, training acuracy = 91.8%, test loss = 0.277231, training acuracy = 92.09%\n","Epoch 341: training loss = 0.289158, training acuracy = 91.81%, test loss = 0.276974, training acuracy = 92.12%\n","Epoch 342: training loss = 0.28887, training acuracy = 91.82%, test loss = 0.276719, training acuracy = 92.13%\n","Epoch 343: training loss = 0.288583, training acuracy = 91.83%, test loss = 0.276465, training acuracy = 92.14%\n","Epoch 344: training loss = 0.288296, training acuracy = 91.85%, test loss = 0.276211, training acuracy = 92.15%\n","Epoch 345: training loss = 0.288011, training acuracy = 91.86%, test loss = 0.275958, training acuracy = 92.14%\n","Epoch 346: training loss = 0.287726, training acuracy = 91.86%, test loss = 0.275707, training acuracy = 92.14%\n","Epoch 347: training loss = 0.287443, training acuracy = 91.88%, test loss = 0.275457, training acuracy = 92.15%\n","Epoch 348: training loss = 0.28716, training acuracy = 91.89%, test loss = 0.275207, training acuracy = 92.16%\n","Epoch 349: training loss = 0.286878, training acuracy = 91.89%, test loss = 0.274958, training acuracy = 92.16%\n","Epoch 350: training loss = 0.286597, training acuracy = 91.9%, test loss = 0.274711, training acuracy = 92.18%\n","Epoch 351: training loss = 0.286317, training acuracy = 91.9%, test loss = 0.274465, training acuracy = 92.18%\n","Epoch 352: training loss = 0.286038, training acuracy = 91.91%, test loss = 0.274219, training acuracy = 92.2%\n","Epoch 353: training loss = 0.28576, training acuracy = 91.92%, test loss = 0.273975, training acuracy = 92.21%\n","Epoch 354: training loss = 0.285482, training acuracy = 91.93%, test loss = 0.273731, training acuracy = 92.21%\n","Epoch 355: training loss = 0.285206, training acuracy = 91.94%, test loss = 0.273488, training acuracy = 92.23%\n","Epoch 356: training loss = 0.28493, training acuracy = 91.95%, test loss = 0.273246, training acuracy = 92.25%\n","Epoch 357: training loss = 0.284655, training acuracy = 91.96%, test loss = 0.273004, training acuracy = 92.26%\n","Epoch 358: training loss = 0.284381, training acuracy = 91.96%, test loss = 0.272764, training acuracy = 92.26%\n","Epoch 359: training loss = 0.284108, training acuracy = 91.97%, test loss = 0.272524, training acuracy = 92.27%\n","Epoch 360: training loss = 0.283835, training acuracy = 91.98%, test loss = 0.272285, training acuracy = 92.28%\n","Epoch 361: training loss = 0.283564, training acuracy = 91.99%, test loss = 0.272046, training acuracy = 92.29%\n","Epoch 362: training loss = 0.283293, training acuracy = 92.0%, test loss = 0.271808, training acuracy = 92.28%\n","Epoch 363: training loss = 0.283022, training acuracy = 92.0%, test loss = 0.271569, training acuracy = 92.28%\n","Epoch 364: training loss = 0.282752, training acuracy = 92.01%, test loss = 0.271333, training acuracy = 92.31%\n","Epoch 365: training loss = 0.282483, training acuracy = 92.01%, test loss = 0.271096, training acuracy = 92.31%\n","Epoch 366: training loss = 0.282215, training acuracy = 92.02%, test loss = 0.27086, training acuracy = 92.31%\n","Epoch 367: training loss = 0.281948, training acuracy = 92.01%, test loss = 0.270624, training acuracy = 92.31%\n","Epoch 368: training loss = 0.281681, training acuracy = 92.02%, test loss = 0.270389, training acuracy = 92.31%\n","Epoch 369: training loss = 0.281415, training acuracy = 92.04%, test loss = 0.270155, training acuracy = 92.31%\n","Epoch 370: training loss = 0.28115, training acuracy = 92.04%, test loss = 0.269921, training acuracy = 92.31%\n","Epoch 371: training loss = 0.280885, training acuracy = 92.05%, test loss = 0.269687, training acuracy = 92.33%\n","Epoch 372: training loss = 0.280621, training acuracy = 92.06%, test loss = 0.269455, training acuracy = 92.33%\n","Epoch 373: training loss = 0.280358, training acuracy = 92.06%, test loss = 0.269224, training acuracy = 92.33%\n","Epoch 374: training loss = 0.280095, training acuracy = 92.06%, test loss = 0.268993, training acuracy = 92.34%\n","Epoch 375: training loss = 0.279833, training acuracy = 92.07%, test loss = 0.268762, training acuracy = 92.35%\n","Epoch 376: training loss = 0.279572, training acuracy = 92.08%, test loss = 0.268532, training acuracy = 92.34%\n","Epoch 377: training loss = 0.279312, training acuracy = 92.08%, test loss = 0.268302, training acuracy = 92.35%\n","Epoch 378: training loss = 0.279052, training acuracy = 92.09%, test loss = 0.268073, training acuracy = 92.37%\n","Epoch 379: training loss = 0.278793, training acuracy = 92.1%, test loss = 0.267846, training acuracy = 92.37%\n","Epoch 380: training loss = 0.278535, training acuracy = 92.11%, test loss = 0.267618, training acuracy = 92.37%\n","Epoch 381: training loss = 0.278278, training acuracy = 92.11%, test loss = 0.267391, training acuracy = 92.39%\n","Epoch 382: training loss = 0.278021, training acuracy = 92.12%, test loss = 0.267165, training acuracy = 92.39%\n","Epoch 383: training loss = 0.277765, training acuracy = 92.12%, test loss = 0.26694, training acuracy = 92.4%\n","Epoch 384: training loss = 0.277509, training acuracy = 92.12%, test loss = 0.266716, training acuracy = 92.4%\n","Epoch 385: training loss = 0.277255, training acuracy = 92.14%, test loss = 0.266492, training acuracy = 92.41%\n","Epoch 386: training loss = 0.277, training acuracy = 92.14%, test loss = 0.266268, training acuracy = 92.43%\n","Epoch 387: training loss = 0.276747, training acuracy = 92.14%, test loss = 0.266046, training acuracy = 92.45%\n","Epoch 388: training loss = 0.276494, training acuracy = 92.15%, test loss = 0.265823, training acuracy = 92.45%\n","Epoch 389: training loss = 0.276242, training acuracy = 92.16%, test loss = 0.265602, training acuracy = 92.45%\n","Epoch 390: training loss = 0.27599, training acuracy = 92.17%, test loss = 0.26538, training acuracy = 92.46%\n","Epoch 391: training loss = 0.275739, training acuracy = 92.18%, test loss = 0.265159, training acuracy = 92.46%\n","Epoch 392: training loss = 0.275489, training acuracy = 92.18%, test loss = 0.264939, training acuracy = 92.46%\n","Epoch 393: training loss = 0.275238, training acuracy = 92.18%, test loss = 0.264719, training acuracy = 92.46%\n","Epoch 394: training loss = 0.274989, training acuracy = 92.2%, test loss = 0.264499, training acuracy = 92.48%\n","Epoch 395: training loss = 0.27474, training acuracy = 92.2%, test loss = 0.26428, training acuracy = 92.49%\n","Epoch 396: training loss = 0.274492, training acuracy = 92.21%, test loss = 0.264061, training acuracy = 92.49%\n","Epoch 397: training loss = 0.274245, training acuracy = 92.21%, test loss = 0.263843, training acuracy = 92.5%\n","Epoch 398: training loss = 0.273999, training acuracy = 92.22%, test loss = 0.263626, training acuracy = 92.53%\n","Epoch 399: training loss = 0.273753, training acuracy = 92.23%, test loss = 0.263409, training acuracy = 92.54%\n","Epoch 400: training loss = 0.273508, training acuracy = 92.24%, test loss = 0.263192, training acuracy = 92.54%\n","Epoch 401: training loss = 0.273263, training acuracy = 92.25%, test loss = 0.262976, training acuracy = 92.54%\n","Epoch 402: training loss = 0.273019, training acuracy = 92.26%, test loss = 0.26276, training acuracy = 92.56%\n","Epoch 403: training loss = 0.272776, training acuracy = 92.28%, test loss = 0.262545, training acuracy = 92.57%\n","Epoch 404: training loss = 0.272533, training acuracy = 92.28%, test loss = 0.26233, training acuracy = 92.58%\n","Epoch 405: training loss = 0.272291, training acuracy = 92.29%, test loss = 0.262116, training acuracy = 92.58%\n","Epoch 406: training loss = 0.272049, training acuracy = 92.3%, test loss = 0.261903, training acuracy = 92.6%\n","Epoch 407: training loss = 0.271808, training acuracy = 92.3%, test loss = 0.261691, training acuracy = 92.6%\n","Epoch 408: training loss = 0.271567, training acuracy = 92.31%, test loss = 0.261479, training acuracy = 92.6%\n","Epoch 409: training loss = 0.271327, training acuracy = 92.32%, test loss = 0.261268, training acuracy = 92.6%\n","Epoch 410: training loss = 0.271088, training acuracy = 92.33%, test loss = 0.261057, training acuracy = 92.65%\n","Epoch 411: training loss = 0.270849, training acuracy = 92.33%, test loss = 0.260847, training acuracy = 92.65%\n","Epoch 412: training loss = 0.270611, training acuracy = 92.34%, test loss = 0.260637, training acuracy = 92.65%\n","Epoch 413: training loss = 0.270374, training acuracy = 92.35%, test loss = 0.260428, training acuracy = 92.65%\n","Epoch 414: training loss = 0.270137, training acuracy = 92.35%, test loss = 0.260219, training acuracy = 92.65%\n","Epoch 415: training loss = 0.269901, training acuracy = 92.36%, test loss = 0.260011, training acuracy = 92.65%\n","Epoch 416: training loss = 0.269665, training acuracy = 92.36%, test loss = 0.259802, training acuracy = 92.65%\n","Epoch 417: training loss = 0.26943, training acuracy = 92.37%, test loss = 0.259595, training acuracy = 92.66%\n","Epoch 418: training loss = 0.269196, training acuracy = 92.38%, test loss = 0.259389, training acuracy = 92.68%\n","Epoch 419: training loss = 0.268961, training acuracy = 92.38%, test loss = 0.259183, training acuracy = 92.69%\n","Epoch 420: training loss = 0.268728, training acuracy = 92.39%, test loss = 0.258978, training acuracy = 92.7%\n","Epoch 421: training loss = 0.268494, training acuracy = 92.4%, test loss = 0.258774, training acuracy = 92.71%\n","Epoch 422: training loss = 0.268262, training acuracy = 92.4%, test loss = 0.25857, training acuracy = 92.71%\n","Epoch 423: training loss = 0.268029, training acuracy = 92.4%, test loss = 0.258367, training acuracy = 92.72%\n","Epoch 424: training loss = 0.267798, training acuracy = 92.41%, test loss = 0.258164, training acuracy = 92.73%\n","Epoch 425: training loss = 0.267567, training acuracy = 92.41%, test loss = 0.257961, training acuracy = 92.73%\n","Epoch 426: training loss = 0.267336, training acuracy = 92.42%, test loss = 0.257759, training acuracy = 92.74%\n","Epoch 427: training loss = 0.267106, training acuracy = 92.44%, test loss = 0.257557, training acuracy = 92.74%\n","Epoch 428: training loss = 0.266877, training acuracy = 92.44%, test loss = 0.257356, training acuracy = 92.73%\n","Epoch 429: training loss = 0.266648, training acuracy = 92.44%, test loss = 0.257155, training acuracy = 92.75%\n","Epoch 430: training loss = 0.26642, training acuracy = 92.45%, test loss = 0.256955, training acuracy = 92.75%\n","Epoch 431: training loss = 0.266192, training acuracy = 92.46%, test loss = 0.256755, training acuracy = 92.75%\n","Epoch 432: training loss = 0.265964, training acuracy = 92.47%, test loss = 0.256555, training acuracy = 92.76%\n","Epoch 433: training loss = 0.265738, training acuracy = 92.48%, test loss = 0.256355, training acuracy = 92.76%\n","Epoch 434: training loss = 0.265511, training acuracy = 92.49%, test loss = 0.256156, training acuracy = 92.78%\n","Epoch 435: training loss = 0.265285, training acuracy = 92.5%, test loss = 0.255957, training acuracy = 92.78%\n","Epoch 436: training loss = 0.265059, training acuracy = 92.5%, test loss = 0.255759, training acuracy = 92.78%\n","Epoch 437: training loss = 0.264834, training acuracy = 92.51%, test loss = 0.255561, training acuracy = 92.79%\n","Epoch 438: training loss = 0.26461, training acuracy = 92.51%, test loss = 0.255363, training acuracy = 92.8%\n","Epoch 439: training loss = 0.264385, training acuracy = 92.52%, test loss = 0.255167, training acuracy = 92.8%\n","Epoch 440: training loss = 0.264162, training acuracy = 92.52%, test loss = 0.25497, training acuracy = 92.8%\n","Epoch 441: training loss = 0.263938, training acuracy = 92.53%, test loss = 0.254774, training acuracy = 92.8%\n","Epoch 442: training loss = 0.263715, training acuracy = 92.54%, test loss = 0.254579, training acuracy = 92.81%\n","Epoch 443: training loss = 0.263492, training acuracy = 92.55%, test loss = 0.254382, training acuracy = 92.81%\n","Epoch 444: training loss = 0.26327, training acuracy = 92.55%, test loss = 0.254187, training acuracy = 92.81%\n","Epoch 445: training loss = 0.263049, training acuracy = 92.55%, test loss = 0.253993, training acuracy = 92.84%\n","Epoch 446: training loss = 0.262828, training acuracy = 92.56%, test loss = 0.253799, training acuracy = 92.84%\n","Epoch 447: training loss = 0.262607, training acuracy = 92.56%, test loss = 0.253606, training acuracy = 92.85%\n","Epoch 448: training loss = 0.262386, training acuracy = 92.57%, test loss = 0.253412, training acuracy = 92.85%\n","Epoch 449: training loss = 0.262166, training acuracy = 92.58%, test loss = 0.253219, training acuracy = 92.85%\n","Epoch 450: training loss = 0.261947, training acuracy = 92.58%, test loss = 0.253025, training acuracy = 92.87%\n","Epoch 451: training loss = 0.261728, training acuracy = 92.59%, test loss = 0.252833, training acuracy = 92.86%\n","Epoch 452: training loss = 0.261509, training acuracy = 92.6%, test loss = 0.252641, training acuracy = 92.87%\n","Epoch 453: training loss = 0.261291, training acuracy = 92.61%, test loss = 0.252449, training acuracy = 92.87%\n","Epoch 454: training loss = 0.261073, training acuracy = 92.62%, test loss = 0.252258, training acuracy = 92.87%\n","Epoch 455: training loss = 0.260855, training acuracy = 92.62%, test loss = 0.252067, training acuracy = 92.87%\n","Epoch 456: training loss = 0.260638, training acuracy = 92.62%, test loss = 0.251876, training acuracy = 92.87%\n","Epoch 457: training loss = 0.260422, training acuracy = 92.63%, test loss = 0.251686, training acuracy = 92.87%\n","Epoch 458: training loss = 0.260205, training acuracy = 92.63%, test loss = 0.251497, training acuracy = 92.9%\n","Epoch 459: training loss = 0.259989, training acuracy = 92.64%, test loss = 0.251307, training acuracy = 92.9%\n","Epoch 460: training loss = 0.259774, training acuracy = 92.64%, test loss = 0.251119, training acuracy = 92.9%\n","Epoch 461: training loss = 0.259559, training acuracy = 92.65%, test loss = 0.250929, training acuracy = 92.91%\n","Epoch 462: training loss = 0.259344, training acuracy = 92.65%, test loss = 0.25074, training acuracy = 92.92%\n","Epoch 463: training loss = 0.25913, training acuracy = 92.65%, test loss = 0.250551, training acuracy = 92.92%\n","Epoch 464: training loss = 0.258916, training acuracy = 92.66%, test loss = 0.250364, training acuracy = 92.93%\n","Epoch 465: training loss = 0.258703, training acuracy = 92.66%, test loss = 0.250176, training acuracy = 92.93%\n","Epoch 466: training loss = 0.25849, training acuracy = 92.67%, test loss = 0.249989, training acuracy = 92.93%\n","Epoch 467: training loss = 0.258277, training acuracy = 92.68%, test loss = 0.249802, training acuracy = 92.94%\n","Epoch 468: training loss = 0.258065, training acuracy = 92.68%, test loss = 0.249616, training acuracy = 92.94%\n","Epoch 469: training loss = 0.257853, training acuracy = 92.68%, test loss = 0.249431, training acuracy = 92.95%\n","Epoch 470: training loss = 0.257641, training acuracy = 92.69%, test loss = 0.249246, training acuracy = 92.96%\n","Epoch 471: training loss = 0.25743, training acuracy = 92.7%, test loss = 0.24906, training acuracy = 92.98%\n","Epoch 472: training loss = 0.257219, training acuracy = 92.7%, test loss = 0.248876, training acuracy = 92.99%\n","Epoch 473: training loss = 0.257008, training acuracy = 92.71%, test loss = 0.248691, training acuracy = 92.99%\n","Epoch 474: training loss = 0.256798, training acuracy = 92.71%, test loss = 0.248506, training acuracy = 92.99%\n","Epoch 475: training loss = 0.256588, training acuracy = 92.72%, test loss = 0.248322, training acuracy = 92.99%\n","Epoch 476: training loss = 0.256379, training acuracy = 92.72%, test loss = 0.248137, training acuracy = 92.99%\n","Epoch 477: training loss = 0.25617, training acuracy = 92.73%, test loss = 0.247954, training acuracy = 93.0%\n","Epoch 478: training loss = 0.255961, training acuracy = 92.74%, test loss = 0.247769, training acuracy = 93.0%\n","Epoch 479: training loss = 0.255752, training acuracy = 92.74%, test loss = 0.247586, training acuracy = 93.0%\n","Epoch 480: training loss = 0.255544, training acuracy = 92.75%, test loss = 0.247404, training acuracy = 93.01%\n","Epoch 481: training loss = 0.255337, training acuracy = 92.76%, test loss = 0.247222, training acuracy = 93.02%\n","Epoch 482: training loss = 0.255129, training acuracy = 92.76%, test loss = 0.247041, training acuracy = 93.03%\n","Epoch 483: training loss = 0.254922, training acuracy = 92.76%, test loss = 0.246859, training acuracy = 93.03%\n","Epoch 484: training loss = 0.254715, training acuracy = 92.76%, test loss = 0.246677, training acuracy = 93.05%\n","Epoch 485: training loss = 0.254509, training acuracy = 92.76%, test loss = 0.246495, training acuracy = 93.06%\n","Epoch 486: training loss = 0.254302, training acuracy = 92.77%, test loss = 0.246313, training acuracy = 93.07%\n","Epoch 487: training loss = 0.254096, training acuracy = 92.78%, test loss = 0.246131, training acuracy = 93.07%\n","Epoch 488: training loss = 0.253891, training acuracy = 92.78%, test loss = 0.245949, training acuracy = 93.07%\n","Epoch 489: training loss = 0.253685, training acuracy = 92.79%, test loss = 0.245768, training acuracy = 93.08%\n","Epoch 490: training loss = 0.25348, training acuracy = 92.8%, test loss = 0.245587, training acuracy = 93.08%\n","Epoch 491: training loss = 0.253275, training acuracy = 92.8%, test loss = 0.245406, training acuracy = 93.09%\n","Epoch 492: training loss = 0.253071, training acuracy = 92.8%, test loss = 0.245227, training acuracy = 93.09%\n","Epoch 493: training loss = 0.252867, training acuracy = 92.81%, test loss = 0.245047, training acuracy = 93.09%\n","Epoch 494: training loss = 0.252663, training acuracy = 92.82%, test loss = 0.244868, training acuracy = 93.1%\n","Epoch 495: training loss = 0.25246, training acuracy = 92.82%, test loss = 0.244689, training acuracy = 93.11%\n","Epoch 496: training loss = 0.252258, training acuracy = 92.82%, test loss = 0.24451, training acuracy = 93.11%\n","Epoch 497: training loss = 0.252055, training acuracy = 92.83%, test loss = 0.244331, training acuracy = 93.13%\n","Epoch 498: training loss = 0.251854, training acuracy = 92.83%, test loss = 0.244152, training acuracy = 93.13%\n","Epoch 499: training loss = 0.251652, training acuracy = 92.84%, test loss = 0.243974, training acuracy = 93.13%\n","Epoch 500: training loss = 0.251451, training acuracy = 92.85%, test loss = 0.243796, training acuracy = 93.15%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"code","metadata":{"id":"ux7mPf6E78d4","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1661071427483,"user_tz":-540,"elapsed":10,"user":{"displayName":"colab minds","userId":"00363918935231101942"}},"outputId":"13df6f27-8b1f-4b1f-f489-357f2b6256a3"},"source":["'''\n","3 layers를 사용하기 때문에 sigmoid function의 단점인 vanishing gradient 현상이 더 드러났을 것이라고 생각 됨. \n","따라서 vanishing gradient 효과가 발생하지 않는 relu를 사용했을 때 더 좋은 성능이 나온 것으로 생각함.\n","lr을 0.5로하면 loss가 fluctuate하기 때문에 lr을 낮추고 epoch을 늘렸다.\n","'''"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n3 layers를 사용하기 때문에 sigmoid function의 단점인 vanishing gradient 현상이 더 드러났을 것이라고 생각 됨. \\n따라서 vanishing gradient 효과가 발생하지 않는 relu를 사용했을 때 더 좋은 성능이 나온 것으로 생각함.\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}]}]}