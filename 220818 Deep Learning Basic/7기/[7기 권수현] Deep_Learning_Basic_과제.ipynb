{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[7기 권수현] Deep_Learning_Basic_과제.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"],"metadata":{"id":"c2rdOhdqBulv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz"},"source":["def relu(x):\n","  relu_value = np.maximum(0, x)\n","  return relu_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","  if x > 0:\n","    d_relu_value = 1\n","  else:\n","    d_relu_value = 0\n","  return d_relu_value"],"metadata":{"id":"Esm4jmTVijro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs"},"source":["def backward_pass(x, y_true, params):\n","\n","  dS3 = params[\"A3\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] = np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] = (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_relu(params[\"S2\"])\n","\n","  grads[\"dW2\"] = np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk"},"source":["# Assignment 3 구현은 여기서 ()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"ldpu0GlMlzdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지를 텐서로 변경\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])"],"metadata":{"id":"04Eimlmel6lS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = True,\n","    download  = True,\n","    transform = transform\n",")\n","testset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = False,\n","    download  = True,\n","    transform = transform\n",")"],"metadata":{"id":"5MMhBwOFmB5r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 512\n","# train set과 test set 각각에 대하여 DataLoader를 생성합니다.\n","# shuffle=True 매개변수를 넣어 데이터를 섞어주세요.\n","train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader =  DataLoader(testset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"2mR_X9N4mLom"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"O1QucsaRmbIv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NA0Q5nqvmzK9","executionInfo":{"status":"ok","timestamp":1661192465933,"user_tz":-540,"elapsed":340,"user":{"displayName":"권수현(상경대학 응용통계학과)","userId":"09851944921108589009"}},"outputId":"60e1606e-1dea-4434-e0d9-cc720335d038"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["list(model.parameters()) # 행렬들을 직접 살펴볼 수 있음\n","                         # require_true 얘는 학습되는 애구나 알 수 있음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9hGq8XjZm0RK","executionInfo":{"status":"ok","timestamp":1661191415521,"user_tz":-540,"elapsed":19,"user":{"displayName":"권수현(상경대학 응용통계학과)","userId":"09851944921108589009"}},"outputId":"994d0462-2ae1-4791-f744-8ac64e9f016e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[ 0.0044,  0.0067, -0.0316,  ...,  0.0087,  0.0148,  0.0047],\n","         [ 0.0049, -0.0003,  0.0028,  ..., -0.0088, -0.0177, -0.0105],\n","         [-0.0218, -0.0031,  0.0189,  ..., -0.0156, -0.0233, -0.0086],\n","         ...,\n","         [-0.0130,  0.0155,  0.0198,  ...,  0.0018, -0.0025, -0.0299],\n","         [ 0.0130,  0.0128,  0.0343,  ...,  0.0174, -0.0022,  0.0129],\n","         [-0.0235, -0.0335, -0.0069,  ..., -0.0110,  0.0164,  0.0124]],\n","        requires_grad=True), Parameter containing:\n"," tensor([-2.9174e-02,  2.9836e-02,  2.5720e-02,  1.8705e-02, -3.3364e-02,\n","         -2.2398e-02,  1.3973e-02,  1.0171e-02, -2.5284e-02, -2.3829e-02,\n","          9.5126e-03, -2.4020e-02, -1.1689e-02,  2.1498e-02,  1.9857e-02,\n","         -3.5679e-02, -2.0950e-02,  2.9340e-03, -2.0010e-07,  3.5664e-02,\n","         -3.2953e-03,  2.1708e-02,  1.3812e-02, -8.0775e-03, -1.6894e-02,\n","          2.8457e-03, -2.4153e-02,  2.8532e-02, -2.7169e-02,  9.9044e-03,\n","          3.0376e-02,  2.3820e-02,  4.0598e-03, -1.5152e-02, -2.1856e-02,\n","          3.3631e-03,  1.3935e-02, -1.0955e-02, -3.3611e-02,  2.4033e-02,\n","         -3.4585e-02, -9.4690e-03,  1.2259e-02,  3.2604e-02, -9.3742e-03,\n","         -4.4962e-03,  2.1880e-02, -2.7090e-02, -1.5496e-02,  2.1251e-02,\n","         -2.6795e-03,  3.5686e-02, -2.8941e-02,  3.2089e-02, -7.1449e-03,\n","          2.9199e-04, -7.0569e-03,  4.7173e-03,  2.6304e-02, -5.4898e-03,\n","          4.1469e-03, -2.3229e-02, -3.1875e-02,  1.4393e-02,  1.4345e-03,\n","          2.3804e-02, -1.6261e-02, -2.9569e-02,  2.0901e-02,  3.3567e-02,\n","          3.5680e-02,  9.9413e-03,  2.6821e-02, -1.9668e-02,  2.0076e-02,\n","         -3.9958e-03, -2.6193e-02, -3.9817e-03, -5.5757e-03, -2.2344e-02,\n","         -1.2557e-02,  1.0495e-02,  1.6528e-02,  2.6180e-02,  1.7667e-02,\n","          1.2892e-02, -2.9998e-02, -1.2675e-02,  9.3380e-03,  1.0709e-02,\n","          1.4183e-02, -4.0330e-03,  1.7533e-02,  1.3941e-02, -3.5262e-02,\n","         -3.5391e-03, -3.4769e-02, -7.9505e-03, -3.3522e-02, -1.7190e-04,\n","         -2.9249e-03, -9.3141e-03,  3.3276e-02,  1.4071e-02,  6.0772e-03,\n","          1.5271e-02,  2.4452e-02,  2.3527e-02, -5.7458e-03, -3.0373e-02,\n","          9.0733e-03, -1.1751e-02, -3.5550e-02, -2.3261e-02, -2.7170e-02,\n","         -2.0359e-02, -6.1590e-03,  2.6975e-03,  5.8453e-03, -1.7378e-02,\n","         -1.6678e-02, -1.5544e-02, -2.9480e-02,  3.5442e-02,  2.6555e-02,\n","         -2.9014e-02, -1.7009e-02,  1.2421e-02], requires_grad=True), Parameter containing:\n"," tensor([[-0.0710, -0.0756,  0.0582,  ...,  0.0653, -0.0093,  0.0210],\n","         [-0.0665, -0.0132,  0.0012,  ...,  0.0357, -0.0780,  0.0407],\n","         [-0.0527, -0.0393,  0.0010,  ...,  0.0142, -0.0865,  0.0186],\n","         ...,\n","         [-0.0476, -0.0011,  0.0853,  ..., -0.0427,  0.0832, -0.0650],\n","         [-0.0585, -0.0541, -0.0727,  ...,  0.0040, -0.0707,  0.0597],\n","         [ 0.0871,  0.0569,  0.0237,  ...,  0.0653, -0.0067, -0.0335]],\n","        requires_grad=True), Parameter containing:\n"," tensor([ 0.0265, -0.0039,  0.0831, -0.0212,  0.0211, -0.0820,  0.0299, -0.0729,\n","          0.0858,  0.0864,  0.0245,  0.0012,  0.0675,  0.0174, -0.0550, -0.0680,\n","          0.0836, -0.0466, -0.0237,  0.0194, -0.0142,  0.0403,  0.0653, -0.0659,\n","          0.0399,  0.0412, -0.0309, -0.0646,  0.0022,  0.0083, -0.0881, -0.0273,\n","         -0.0702, -0.0468, -0.0417, -0.0562, -0.0309, -0.0446, -0.0603, -0.0678,\n","         -0.0532, -0.0396, -0.0048, -0.0397, -0.0338,  0.0265,  0.0298, -0.0063,\n","          0.0059,  0.0218,  0.0105, -0.0377, -0.0645, -0.0234,  0.0705, -0.0128,\n","          0.0094,  0.0359,  0.0040,  0.0758,  0.0054, -0.0853, -0.0452,  0.0740],\n","        requires_grad=True), Parameter containing:\n"," tensor([[ 1.1354e-01,  9.1717e-02, -3.0552e-02,  4.1665e-02,  1.0855e-01,\n","          -7.8438e-02, -7.0283e-02,  5.9403e-02,  8.4612e-02,  7.6969e-02,\n","           1.0153e-01,  1.1548e-01, -7.2536e-02,  1.1674e-01,  3.9521e-02,\n","          -3.8028e-02, -7.1436e-02,  1.1859e-01,  4.3368e-02,  8.1283e-02,\n","           4.1023e-02, -2.8035e-04, -7.0107e-02, -6.3981e-03,  7.2575e-02,\n","          -1.9812e-02, -2.5325e-02,  2.5168e-02,  2.0587e-03, -1.1562e-01,\n","          -1.9769e-02, -9.3574e-02, -6.9321e-02, -4.6544e-02,  8.9829e-02,\n","          -1.0028e-01, -7.6382e-02, -1.5537e-02,  4.1017e-03, -4.1950e-03,\n","          -5.8953e-02, -1.0523e-01, -9.1808e-02,  3.7185e-02, -9.0683e-02,\n","          -5.6628e-02,  9.9509e-02, -1.0362e-01, -1.1595e-01,  1.1878e-01,\n","          -4.2388e-02,  6.1804e-02, -1.2413e-01,  1.9654e-02,  1.3039e-03,\n","          -4.9793e-02, -1.1205e-01,  1.9303e-02,  6.2493e-02,  1.3045e-02,\n","           1.0811e-01,  4.5902e-02,  1.3951e-02,  9.1838e-02],\n","         [-1.2457e-01, -3.5886e-03,  1.1406e-02, -1.0420e-01,  1.0643e-01,\n","          -5.3871e-02, -3.3065e-02, -4.0131e-02,  5.3144e-03, -6.9442e-02,\n","          -5.4344e-02, -1.2340e-01, -1.0829e-02, -1.1520e-01,  1.9007e-02,\n","          -9.1975e-02, -1.0394e-01, -1.0210e-01, -3.9312e-02, -6.8865e-02,\n","          -8.4906e-02,  1.6281e-02,  9.9970e-04,  9.9972e-02,  4.7105e-02,\n","           5.5775e-03,  6.9992e-02, -7.1964e-02, -8.7780e-02, -1.1629e-01,\n","          -1.2179e-01,  8.3131e-02,  6.5841e-02,  3.1660e-02, -9.3248e-02,\n","           3.8474e-03, -4.8833e-02,  3.8780e-02,  4.3398e-02, -8.4934e-02,\n","          -8.4839e-02, -7.8784e-02, -5.7238e-02,  7.9406e-02, -9.8099e-02,\n","           6.4883e-02, -1.1283e-02,  3.0265e-02, -6.2203e-02,  3.2291e-03,\n","          -6.0820e-02, -2.6859e-02,  1.7792e-02, -6.6195e-02, -5.5257e-02,\n","          -1.0336e-01, -7.9331e-02,  3.8617e-02,  6.1417e-03,  9.2968e-02,\n","          -1.1941e-01,  3.5901e-02, -7.1746e-02,  2.9413e-02],\n","         [-1.1218e-01,  4.7143e-02,  9.3141e-02,  7.0980e-02,  4.2605e-02,\n","           1.1210e-01,  1.1146e-03,  1.1667e-01,  2.5563e-02,  4.2633e-02,\n","          -4.2001e-02, -1.1876e-01,  1.0998e-01, -4.4012e-02,  5.2502e-03,\n","          -6.8877e-02, -1.0088e-01,  7.2438e-02, -8.3829e-02,  5.5785e-02,\n","           4.7095e-02, -5.9111e-02,  8.5571e-02,  6.2441e-02,  2.4355e-02,\n","          -1.4625e-02,  1.0352e-01,  9.9915e-04,  6.2620e-02,  6.1266e-02,\n","           8.7606e-03,  5.9315e-02, -1.2306e-01,  1.3180e-02,  1.8880e-02,\n","           7.5212e-02, -9.7148e-02,  1.2032e-02, -1.2180e-01,  5.8183e-02,\n","          -1.6460e-02,  5.9099e-03,  1.1353e-01,  9.4360e-02,  9.3905e-02,\n","           3.6783e-02,  7.2810e-02,  8.9818e-02,  1.1720e-01, -2.1225e-03,\n","          -4.9198e-02, -1.0634e-01,  1.9748e-02, -5.3207e-02,  5.1537e-02,\n","           9.2944e-02,  5.5160e-02,  2.0363e-02,  4.4768e-02, -6.4849e-02,\n","          -9.8583e-02,  4.2011e-02,  4.0522e-02,  8.7113e-02],\n","         [-9.1076e-02,  9.8542e-02,  5.0453e-02, -9.5064e-02, -8.8521e-02,\n","          -1.1709e-01, -6.2870e-02,  5.6355e-02,  1.2036e-01, -2.6899e-02,\n","           1.2302e-01,  1.0171e-01,  1.0911e-01, -6.3784e-02,  4.0984e-02,\n","           1.1742e-01,  3.6678e-03, -2.9908e-02, -1.0383e-01, -1.2459e-01,\n","          -5.6013e-02, -1.0701e-01,  2.5792e-02, -4.9926e-02, -7.2602e-02,\n","           6.1519e-02, -9.0527e-02, -9.2558e-02, -4.6489e-02,  2.8097e-02,\n","          -1.1380e-01,  4.0993e-02,  9.0373e-02, -1.1188e-01, -6.5841e-02,\n","           5.6398e-02, -1.1497e-01,  6.2855e-02, -5.3138e-02, -3.9922e-02,\n","           7.4728e-02,  4.4055e-02, -3.5493e-02, -1.0211e-01, -5.2390e-02,\n","           1.0111e-01,  9.2140e-02,  7.0425e-02, -3.9588e-02,  1.1035e-02,\n","          -3.3676e-02,  1.0982e-01,  1.7732e-02, -8.9500e-02, -4.0550e-03,\n","          -3.3833e-02,  1.9489e-02, -3.3992e-02, -7.0652e-02, -7.3151e-02,\n","           1.0880e-01,  9.1419e-02, -7.0214e-03,  7.3740e-02],\n","         [-1.0660e-01, -2.7415e-02, -3.4957e-02,  6.7342e-02, -6.0703e-02,\n","          -1.2057e-02,  5.8595e-02,  3.5031e-02,  1.0899e-02, -7.4051e-02,\n","          -1.0949e-01, -8.4025e-02,  3.4699e-02, -4.5999e-02,  1.0573e-01,\n","          -2.0855e-02, -4.3111e-02,  2.3847e-02,  1.0762e-01,  1.1669e-01,\n","           7.9155e-02, -4.4790e-02, -5.4959e-02,  7.9595e-02,  5.9679e-02,\n","          -5.8555e-02, -9.2467e-02,  1.0628e-01, -3.3856e-02, -9.3354e-02,\n","           5.4731e-02,  1.9694e-02,  2.0028e-02, -9.2109e-02, -3.0685e-02,\n","           1.2811e-02,  1.0075e-01, -6.6478e-03,  7.4682e-02,  5.1467e-02,\n","           6.4090e-02,  9.6729e-02,  1.2048e-01,  1.1650e-01, -1.1644e-01,\n","          -1.2061e-01,  7.8433e-03,  1.2366e-01, -5.2521e-02, -8.0817e-02,\n","           8.9785e-02, -2.7885e-02,  1.9166e-02,  1.1846e-01, -1.7191e-03,\n","           1.3701e-02, -9.8971e-02,  9.8473e-02, -2.5814e-02,  5.9848e-02,\n","          -6.3636e-03,  9.3604e-02,  1.0050e-01, -8.1871e-03],\n","         [-6.1192e-02, -7.0236e-02,  6.0334e-02, -1.0902e-02,  3.1982e-02,\n","           3.4204e-02, -2.9578e-02,  5.0173e-02, -1.0792e-01, -8.2032e-02,\n","          -1.1042e-01, -6.9096e-03,  4.0917e-02,  4.8534e-02,  9.4175e-05,\n","          -2.2430e-02,  4.1407e-02, -5.7400e-03, -2.4928e-02,  1.1625e-01,\n","          -8.5657e-02,  1.0571e-01,  1.2415e-01, -7.0240e-02, -2.8834e-02,\n","          -5.9147e-02, -1.1536e-01, -1.4062e-02, -1.0708e-01, -1.1770e-02,\n","          -1.6504e-02, -1.1441e-01,  1.7110e-02, -6.3520e-02, -6.0310e-02,\n","           1.2348e-01, -1.9962e-02,  9.6423e-02,  1.6840e-02, -9.1234e-02,\n","          -3.1742e-02, -7.4818e-02, -9.5425e-02,  3.8741e-02,  1.2435e-01,\n","          -8.1128e-02,  5.8853e-02, -1.1206e-01, -9.5862e-02, -7.4257e-02,\n","           1.0611e-01, -3.5024e-02, -2.3183e-02,  6.9176e-02,  6.2137e-02,\n","           5.5727e-03,  8.4702e-02,  1.0299e-01, -1.5853e-02, -1.9539e-02,\n","          -5.1831e-02, -1.2724e-02,  7.3843e-02,  2.9244e-02],\n","         [ 1.0136e-01, -6.1637e-02,  1.0159e-01,  9.0725e-03,  1.1253e-01,\n","          -7.4770e-02,  8.6560e-02, -7.7495e-02, -9.3904e-03, -1.5669e-02,\n","          -9.6815e-02, -5.4960e-02, -1.1274e-01, -8.0315e-02,  1.2080e-01,\n","          -9.7744e-02,  5.9946e-02,  1.6555e-02, -7.1692e-02,  5.3753e-02,\n","           8.3474e-02,  1.7797e-02,  1.1911e-01, -9.0817e-02,  2.2325e-02,\n","           4.6683e-02,  2.1429e-02, -4.9746e-04, -1.7567e-02,  1.1528e-01,\n","          -1.9363e-02, -6.4014e-02,  3.9855e-02,  6.7109e-02, -1.0780e-01,\n","           1.2189e-01,  8.2338e-02, -3.1512e-02, -5.1855e-02,  3.2720e-02,\n","          -3.4026e-02,  3.3466e-02,  5.2830e-02,  1.2879e-02, -9.8679e-02,\n","          -1.1399e-01, -8.8634e-02, -5.2434e-02,  1.4762e-02,  1.6031e-02,\n","          -1.2378e-01,  5.1612e-04,  8.4633e-02, -1.6970e-02,  3.4516e-02,\n","          -4.3714e-02,  2.5589e-02, -2.3694e-02,  1.0729e-01, -3.8864e-02,\n","          -2.2810e-02,  1.1392e-01, -7.6356e-02,  5.5951e-02],\n","         [ 8.6380e-02,  4.8358e-02, -5.8848e-02,  2.9368e-02, -9.9384e-02,\n","           5.2557e-02, -1.1798e-01, -5.1251e-02, -2.5779e-02, -1.1564e-01,\n","           1.4457e-02, -4.7772e-02,  8.0685e-02, -1.2034e-03, -6.3235e-02,\n","          -6.4735e-02, -1.1483e-01, -5.8327e-02, -9.2822e-02, -9.2523e-02,\n","          -2.0809e-02,  9.1362e-02,  1.1503e-01, -1.5784e-02,  1.7448e-02,\n","          -4.8917e-03, -8.4148e-02,  1.0982e-01, -9.2562e-02, -1.2438e-01,\n","           5.4479e-04, -1.2841e-02,  3.0322e-02, -8.5117e-02,  1.6209e-02,\n","           2.9063e-02, -1.0799e-01, -6.1734e-02,  5.7621e-02, -5.4767e-02,\n","          -1.0315e-01,  1.0372e-01,  4.7462e-03, -1.2467e-01, -5.5397e-02,\n","          -8.2117e-02, -2.0848e-02, -4.8635e-02, -3.1028e-02,  1.0660e-01,\n","          -1.1701e-01,  1.7695e-02,  4.0041e-02,  4.8725e-02, -2.8303e-02,\n","           3.5062e-02,  1.1813e-01,  5.2624e-02, -8.7026e-02,  3.1113e-02,\n","           1.3229e-02, -4.7381e-02,  1.2243e-01,  8.9198e-02],\n","         [ 6.5145e-02,  2.0186e-02, -3.4206e-02,  1.1788e-01, -9.9343e-03,\n","           1.2493e-01,  1.2330e-01,  8.3833e-02,  8.3766e-02, -3.6103e-02,\n","          -1.0160e-01, -4.6188e-02,  4.1194e-02, -3.8096e-02, -2.0802e-02,\n","          -7.0373e-02, -2.8586e-02, -1.0617e-02,  4.2081e-02, -1.4457e-02,\n","          -1.0771e-02, -9.0150e-02, -4.6419e-02, -7.5706e-03,  2.7141e-02,\n","          -1.4544e-03, -1.4161e-02,  5.8528e-02, -1.2467e-01, -1.1412e-01,\n","           7.4316e-02, -2.4024e-02,  7.6814e-02, -5.3703e-02, -1.1045e-01,\n","          -2.3490e-02,  1.0653e-02, -7.2810e-02,  2.8040e-02,  7.7576e-02,\n","          -1.1552e-01, -1.0024e-01,  9.6889e-02,  4.9525e-02,  9.9233e-02,\n","          -8.7233e-02,  5.4229e-02, -1.0854e-01, -9.6398e-02,  3.7394e-02,\n","           4.4238e-02, -5.1772e-02, -2.2865e-02,  1.0270e-01,  9.7392e-02,\n","          -3.9887e-02,  8.2978e-02,  3.4577e-02, -5.9425e-02, -9.0270e-02,\n","          -2.9902e-02, -1.1630e-02, -2.0373e-03,  3.3237e-02],\n","         [ 1.0776e-01,  6.6817e-02, -5.1478e-02,  8.5380e-02,  9.7128e-02,\n","          -8.3880e-02,  2.2884e-02,  1.1327e-01, -8.4995e-04,  5.2376e-03,\n","          -3.4301e-02,  4.6214e-02,  4.8696e-02,  5.7355e-02, -1.2488e-01,\n","          -9.1788e-02,  7.9220e-02,  2.3566e-02, -1.2381e-01,  2.4150e-02,\n","           1.4672e-02, -2.6277e-02, -3.0841e-02,  1.0820e-01, -6.1053e-02,\n","          -5.8916e-02, -3.3888e-02,  7.0423e-02,  7.1065e-02, -4.6234e-02,\n","          -9.8224e-02,  3.3486e-03,  9.0069e-02,  2.6840e-02,  3.7348e-02,\n","          -2.0355e-02, -1.2771e-02,  3.5835e-02,  6.8662e-02,  3.6179e-02,\n","          -1.2020e-01, -1.1381e-01,  2.0661e-02, -2.5514e-02,  2.1245e-02,\n","           8.2580e-02,  1.8935e-03,  1.1143e-02,  1.1996e-01,  1.1229e-01,\n","           4.0492e-02,  5.5101e-02, -9.7866e-02,  2.9024e-03,  1.4781e-02,\n","          -4.9158e-02, -8.6794e-02, -8.6661e-02, -8.7293e-02, -1.1343e-01,\n","          -5.7178e-02, -7.0990e-02,  2.8147e-02, -3.3631e-02]],\n","        requires_grad=True), Parameter containing:\n"," tensor([ 0.0340, -0.0306, -0.0483,  0.1234,  0.1213, -0.0090,  0.0171,  0.0782,\n","         -0.0512, -0.0829], requires_grad=True)]"]},"metadata":{},"execution_count":88}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"AQ6p4Es3m2A8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"id":"nHCAntTsm7Oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"XBoHvUSom_i9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zE2U5QwTnAsL","executionInfo":{"status":"ok","timestamp":1661190076415,"user_tz":-540,"elapsed":568859,"user":{"displayName":"권수현(상경대학 응용통계학과)","userId":"09851944921108589009"}},"outputId":"d54fc95a-159c-40eb-eba0-affe5ea6d97e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.7838\tTest Loss: 0.4859\tAccuracy: 84.67%\n","[2] Train Loss: 0.2416\tTest Loss: 0.2676\tAccuracy: 91.20%\n","[3] Train Loss: 0.1680\tTest Loss: 0.1622\tAccuracy: 94.99%\n","[4] Train Loss: 0.1304\tTest Loss: 0.1739\tAccuracy: 94.45%\n","[5] Train Loss: 0.1099\tTest Loss: 0.1412\tAccuracy: 95.28%\n","[6] Train Loss: 0.0920\tTest Loss: 0.1131\tAccuracy: 96.52%\n","[7] Train Loss: 0.0757\tTest Loss: 0.1063\tAccuracy: 96.83%\n","[8] Train Loss: 0.0697\tTest Loss: 0.1673\tAccuracy: 94.46%\n","[9] Train Loss: 0.0591\tTest Loss: 0.0758\tAccuracy: 97.54%\n","[10] Train Loss: 0.0544\tTest Loss: 0.1305\tAccuracy: 96.00%\n","[11] Train Loss: 0.0476\tTest Loss: 0.0817\tAccuracy: 97.32%\n","[12] Train Loss: 0.0406\tTest Loss: 0.0777\tAccuracy: 97.55%\n","[13] Train Loss: 0.0359\tTest Loss: 0.1197\tAccuracy: 96.20%\n","[14] Train Loss: 0.0332\tTest Loss: 0.1211\tAccuracy: 96.31%\n","[15] Train Loss: 0.0296\tTest Loss: 0.0740\tAccuracy: 97.73%\n","[16] Train Loss: 0.0250\tTest Loss: 0.0771\tAccuracy: 97.61%\n","[17] Train Loss: 0.0221\tTest Loss: 0.0700\tAccuracy: 97.98%\n","[18] Train Loss: 0.0190\tTest Loss: 0.0840\tAccuracy: 97.54%\n","[19] Train Loss: 4.4233\tTest Loss: 0.5118\tAccuracy: 82.77%\n","[20] Train Loss: 0.2543\tTest Loss: 0.1923\tAccuracy: 94.31%\n","[21] Train Loss: 0.1822\tTest Loss: 0.1672\tAccuracy: 94.94%\n","[22] Train Loss: 0.1494\tTest Loss: 0.2626\tAccuracy: 91.72%\n","[23] Train Loss: 0.1386\tTest Loss: 0.2351\tAccuracy: 92.53%\n","[24] Train Loss: 0.1220\tTest Loss: 0.3943\tAccuracy: 88.80%\n","[25] Train Loss: 0.1213\tTest Loss: 0.1642\tAccuracy: 94.76%\n","[26] Train Loss: 0.1037\tTest Loss: 0.1362\tAccuracy: 95.90%\n","[27] Train Loss: 0.0992\tTest Loss: 0.1338\tAccuracy: 95.88%\n","[28] Train Loss: 0.0930\tTest Loss: 0.8205\tAccuracy: 83.65%\n","[29] Train Loss: 0.8412\tTest Loss: 0.4284\tAccuracy: 85.81%\n","[30] Train Loss: 0.2871\tTest Loss: 0.3869\tAccuracy: 88.44%\n","[31] Train Loss: 0.2439\tTest Loss: 0.2496\tAccuracy: 92.78%\n","[32] Train Loss: 0.1981\tTest Loss: 0.1885\tAccuracy: 94.29%\n","[33] Train Loss: 0.1858\tTest Loss: 0.1984\tAccuracy: 94.29%\n","[34] Train Loss: 0.1693\tTest Loss: 0.4599\tAccuracy: 86.36%\n","[35] Train Loss: 0.1682\tTest Loss: 0.3299\tAccuracy: 89.66%\n","[36] Train Loss: 0.2041\tTest Loss: 0.1930\tAccuracy: 94.20%\n","[37] Train Loss: 0.1472\tTest Loss: 0.1763\tAccuracy: 94.76%\n","[38] Train Loss: 0.1384\tTest Loss: 0.1637\tAccuracy: 95.31%\n","[39] Train Loss: 0.1345\tTest Loss: 0.1552\tAccuracy: 95.51%\n","[40] Train Loss: 0.1251\tTest Loss: 0.1991\tAccuracy: 94.05%\n","[41] Train Loss: 0.1208\tTest Loss: 0.1712\tAccuracy: 94.91%\n","[42] Train Loss: 0.1213\tTest Loss: 0.1782\tAccuracy: 94.72%\n","[43] Train Loss: 0.1106\tTest Loss: 0.1628\tAccuracy: 95.32%\n","[44] Train Loss: 0.1062\tTest Loss: 0.2346\tAccuracy: 93.07%\n","[45] Train Loss: 0.1275\tTest Loss: 0.1666\tAccuracy: 95.19%\n","[46] Train Loss: 0.1016\tTest Loss: 0.1407\tAccuracy: 95.93%\n","[47] Train Loss: 0.0987\tTest Loss: 1.9437\tAccuracy: 68.11%\n","[48] Train Loss: 1.3162\tTest Loss: 0.5893\tAccuracy: 83.58%\n","[49] Train Loss: 0.3684\tTest Loss: 0.5259\tAccuracy: 85.38%\n","[50] Train Loss: 0.3115\tTest Loss: 0.3800\tAccuracy: 89.64%\n","[51] Train Loss: 0.2575\tTest Loss: 0.4356\tAccuracy: 87.67%\n","[52] Train Loss: 0.2423\tTest Loss: 0.5505\tAccuracy: 85.08%\n","[53] Train Loss: 0.2451\tTest Loss: 0.2983\tAccuracy: 92.53%\n","[54] Train Loss: 0.2046\tTest Loss: 0.2706\tAccuracy: 93.56%\n","[55] Train Loss: 0.2027\tTest Loss: 0.2696\tAccuracy: 93.49%\n","[56] Train Loss: 0.2019\tTest Loss: 0.2926\tAccuracy: 92.80%\n","[57] Train Loss: 0.1800\tTest Loss: 0.2967\tAccuracy: 92.89%\n","[58] Train Loss: 0.1728\tTest Loss: 0.3589\tAccuracy: 91.32%\n","[59] Train Loss: 0.1708\tTest Loss: 0.3689\tAccuracy: 91.07%\n","[60] Train Loss: 0.1638\tTest Loss: 0.2269\tAccuracy: 94.58%\n","[61] Train Loss: 0.1609\tTest Loss: 0.2327\tAccuracy: 94.58%\n","[62] Train Loss: 0.1517\tTest Loss: 0.2657\tAccuracy: 93.70%\n","[63] Train Loss: 0.1530\tTest Loss: 0.2540\tAccuracy: 94.11%\n","[64] Train Loss: 0.1436\tTest Loss: 0.2290\tAccuracy: 94.63%\n","[65] Train Loss: 0.1400\tTest Loss: 0.2861\tAccuracy: 93.08%\n","[66] Train Loss: 0.1378\tTest Loss: 0.3796\tAccuracy: 89.80%\n","[67] Train Loss: 0.1369\tTest Loss: 0.2369\tAccuracy: 94.50%\n","[68] Train Loss: 0.1311\tTest Loss: 0.2558\tAccuracy: 93.75%\n","[69] Train Loss: 0.1316\tTest Loss: 0.3170\tAccuracy: 92.24%\n","[70] Train Loss: 0.1307\tTest Loss: 0.8319\tAccuracy: 87.04%\n","[71] Train Loss: 0.4863\tTest Loss: 0.3515\tAccuracy: 91.14%\n","[72] Train Loss: 0.1939\tTest Loss: 0.2565\tAccuracy: 93.63%\n","[73] Train Loss: 0.1751\tTest Loss: 0.2282\tAccuracy: 94.76%\n","[74] Train Loss: 0.1631\tTest Loss: 0.2339\tAccuracy: 94.68%\n","[75] Train Loss: 0.1568\tTest Loss: 0.2303\tAccuracy: 94.41%\n","[76] Train Loss: 0.1581\tTest Loss: 0.2328\tAccuracy: 94.64%\n","[77] Train Loss: 0.1485\tTest Loss: 0.2537\tAccuracy: 93.77%\n","[78] Train Loss: 0.1439\tTest Loss: 0.2223\tAccuracy: 94.75%\n","[79] Train Loss: 0.1394\tTest Loss: 0.2859\tAccuracy: 94.04%\n","[80] Train Loss: 0.1379\tTest Loss: 0.2412\tAccuracy: 94.63%\n","[81] Train Loss: 0.1358\tTest Loss: 0.2142\tAccuracy: 94.71%\n","[82] Train Loss: 0.1352\tTest Loss: 0.2730\tAccuracy: 93.09%\n","[83] Train Loss: 0.1322\tTest Loss: 0.2884\tAccuracy: 92.55%\n","[84] Train Loss: 0.1286\tTest Loss: 0.2418\tAccuracy: 93.98%\n","[85] Train Loss: 0.1268\tTest Loss: 0.2227\tAccuracy: 94.83%\n","[86] Train Loss: 0.1251\tTest Loss: 0.2913\tAccuracy: 92.92%\n","[87] Train Loss: 0.1238\tTest Loss: 0.2133\tAccuracy: 94.88%\n","[88] Train Loss: 0.1204\tTest Loss: 0.2824\tAccuracy: 92.98%\n","[89] Train Loss: 0.1187\tTest Loss: 0.2361\tAccuracy: 94.45%\n","[90] Train Loss: 0.1165\tTest Loss: 0.2005\tAccuracy: 95.28%\n","[91] Train Loss: 0.1159\tTest Loss: 0.2857\tAccuracy: 93.89%\n","[92] Train Loss: 0.1144\tTest Loss: 0.2130\tAccuracy: 95.09%\n","[93] Train Loss: 0.1132\tTest Loss: 0.2287\tAccuracy: 94.76%\n","[94] Train Loss: 0.1116\tTest Loss: 0.2241\tAccuracy: 94.87%\n","[95] Train Loss: 0.1124\tTest Loss: 0.2265\tAccuracy: 94.90%\n","[96] Train Loss: 0.1092\tTest Loss: 0.2951\tAccuracy: 93.06%\n","[97] Train Loss: 0.1123\tTest Loss: 0.2264\tAccuracy: 94.90%\n","[98] Train Loss: 0.1083\tTest Loss: 0.2431\tAccuracy: 94.20%\n","[99] Train Loss: 0.1053\tTest Loss: 0.2386\tAccuracy: 94.38%\n","[100] Train Loss: 0.1070\tTest Loss: 0.4053\tAccuracy: 89.63%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","metadata":{"id":"k6b82DZG6W3j"},"source":["# Assignment 4 구현은 여기서 ()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","        out = self.softmax(out)\n","\n","        return out"],"metadata":{"id":"OajSIGd7nLaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X50G84XroTAM","executionInfo":{"status":"ok","timestamp":1661192630866,"user_tz":-540,"elapsed":3,"user":{"displayName":"권수현(상경대학 응용통계학과)","userId":"09851944921108589009"}},"outputId":"99e5b1ac-b70f-476a-8da8-7d64d32fabbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n","  (softmax): Softmax(dim=1)\n",")"]},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"FmXao0BzoYfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b3dzJz76oj6e","executionInfo":{"status":"ok","timestamp":1661193211978,"user_tz":-540,"elapsed":567187,"user":{"displayName":"권수현(상경대학 응용통계학과)","userId":"09851944921108589009"}},"outputId":"1bed1b05-22ae-40bb-b1c0-ef45fa18fb1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 2.2004\tTest Loss: 1.9302\tAccuracy: 59.94%\n","[2] Train Loss: 1.7552\tTest Loss: 1.7187\tAccuracy: 76.00%\n","[3] Train Loss: 1.6552\tTest Loss: 1.6492\tAccuracy: 82.04%\n","[4] Train Loss: 1.6330\tTest Loss: 1.6284\tAccuracy: 83.98%\n","[5] Train Loss: 1.6122\tTest Loss: 1.5825\tAccuracy: 88.97%\n","[6] Train Loss: 1.5679\tTest Loss: 1.5751\tAccuracy: 89.57%\n","[7] Train Loss: 1.5550\tTest Loss: 1.5595\tAccuracy: 90.84%\n","[8] Train Loss: 1.5465\tTest Loss: 1.5512\tAccuracy: 91.55%\n","[9] Train Loss: 1.5403\tTest Loss: 1.5543\tAccuracy: 91.14%\n","[10] Train Loss: 1.5349\tTest Loss: 1.5404\tAccuracy: 92.48%\n","[11] Train Loss: 1.5300\tTest Loss: 1.5434\tAccuracy: 92.28%\n","[12] Train Loss: 1.5262\tTest Loss: 1.5279\tAccuracy: 93.67%\n","[13] Train Loss: 1.5222\tTest Loss: 1.5234\tAccuracy: 94.29%\n","[14] Train Loss: 1.5196\tTest Loss: 1.5271\tAccuracy: 93.87%\n","[15] Train Loss: 1.5165\tTest Loss: 1.5327\tAccuracy: 93.23%\n","[16] Train Loss: 1.5140\tTest Loss: 1.5168\tAccuracy: 94.79%\n","[17] Train Loss: 1.5113\tTest Loss: 1.5172\tAccuracy: 94.75%\n","[18] Train Loss: 1.5095\tTest Loss: 1.5141\tAccuracy: 95.05%\n","[19] Train Loss: 1.5073\tTest Loss: 1.5092\tAccuracy: 95.42%\n","[20] Train Loss: 1.5054\tTest Loss: 1.5088\tAccuracy: 95.50%\n","[21] Train Loss: 1.5036\tTest Loss: 1.5077\tAccuracy: 95.72%\n","[22] Train Loss: 1.5018\tTest Loss: 1.5075\tAccuracy: 95.59%\n","[23] Train Loss: 1.5005\tTest Loss: 1.5169\tAccuracy: 94.78%\n","[24] Train Loss: 1.4995\tTest Loss: 1.5039\tAccuracy: 95.98%\n","[25] Train Loss: 1.4977\tTest Loss: 1.5042\tAccuracy: 95.96%\n","[26] Train Loss: 1.4963\tTest Loss: 1.5046\tAccuracy: 95.85%\n","[27] Train Loss: 1.4950\tTest Loss: 1.5051\tAccuracy: 95.82%\n","[28] Train Loss: 1.4940\tTest Loss: 1.5010\tAccuracy: 96.27%\n","[29] Train Loss: 1.4927\tTest Loss: 1.5030\tAccuracy: 96.02%\n","[30] Train Loss: 1.4919\tTest Loss: 1.5003\tAccuracy: 96.36%\n","[31] Train Loss: 1.4910\tTest Loss: 1.4976\tAccuracy: 96.59%\n","[32] Train Loss: 1.4895\tTest Loss: 1.4974\tAccuracy: 96.73%\n","[33] Train Loss: 1.4891\tTest Loss: 1.4970\tAccuracy: 96.68%\n","[34] Train Loss: 1.4881\tTest Loss: 1.5071\tAccuracy: 95.67%\n","[35] Train Loss: 1.4874\tTest Loss: 1.4970\tAccuracy: 96.69%\n","[36] Train Loss: 1.4863\tTest Loss: 1.4945\tAccuracy: 96.88%\n","[37] Train Loss: 1.4856\tTest Loss: 1.4961\tAccuracy: 96.79%\n","[38] Train Loss: 1.4852\tTest Loss: 1.5049\tAccuracy: 95.96%\n","[39] Train Loss: 1.4844\tTest Loss: 1.4958\tAccuracy: 96.75%\n","[40] Train Loss: 1.4836\tTest Loss: 1.4935\tAccuracy: 97.04%\n","[41] Train Loss: 1.4829\tTest Loss: 1.4939\tAccuracy: 96.87%\n","[42] Train Loss: 1.4825\tTest Loss: 1.4987\tAccuracy: 96.38%\n","[43] Train Loss: 1.4816\tTest Loss: 1.4923\tAccuracy: 96.97%\n","[44] Train Loss: 1.4812\tTest Loss: 1.4905\tAccuracy: 97.25%\n","[45] Train Loss: 1.4806\tTest Loss: 1.5014\tAccuracy: 96.27%\n","[46] Train Loss: 1.4806\tTest Loss: 1.4905\tAccuracy: 97.22%\n","[47] Train Loss: 1.4799\tTest Loss: 1.4921\tAccuracy: 96.97%\n","[48] Train Loss: 1.4795\tTest Loss: 1.4896\tAccuracy: 97.37%\n","[49] Train Loss: 1.4791\tTest Loss: 1.4894\tAccuracy: 97.37%\n","[50] Train Loss: 1.4786\tTest Loss: 1.4899\tAccuracy: 97.30%\n","[51] Train Loss: 1.4783\tTest Loss: 1.4899\tAccuracy: 97.21%\n","[52] Train Loss: 1.4780\tTest Loss: 1.4953\tAccuracy: 96.79%\n","[53] Train Loss: 1.4776\tTest Loss: 1.4937\tAccuracy: 96.89%\n","[54] Train Loss: 1.4775\tTest Loss: 1.4913\tAccuracy: 97.22%\n","[55] Train Loss: 1.4771\tTest Loss: 1.4890\tAccuracy: 97.38%\n","[56] Train Loss: 1.4768\tTest Loss: 1.4891\tAccuracy: 97.31%\n","[57] Train Loss: 1.4764\tTest Loss: 1.4888\tAccuracy: 97.35%\n","[58] Train Loss: 1.4762\tTest Loss: 1.4935\tAccuracy: 96.92%\n","[59] Train Loss: 1.4760\tTest Loss: 1.4888\tAccuracy: 97.41%\n","[60] Train Loss: 1.4757\tTest Loss: 1.4890\tAccuracy: 97.36%\n","[61] Train Loss: 1.4755\tTest Loss: 1.4917\tAccuracy: 97.09%\n","[62] Train Loss: 1.4755\tTest Loss: 1.4892\tAccuracy: 97.34%\n","[63] Train Loss: 1.4750\tTest Loss: 1.4879\tAccuracy: 97.43%\n","[64] Train Loss: 1.4748\tTest Loss: 1.4879\tAccuracy: 97.48%\n","[65] Train Loss: 1.4747\tTest Loss: 1.4892\tAccuracy: 97.25%\n","[66] Train Loss: 1.4746\tTest Loss: 1.4952\tAccuracy: 96.61%\n","[67] Train Loss: 1.4746\tTest Loss: 1.4882\tAccuracy: 97.42%\n","[68] Train Loss: 1.4744\tTest Loss: 1.4881\tAccuracy: 97.40%\n","[69] Train Loss: 1.4741\tTest Loss: 1.4889\tAccuracy: 97.33%\n","[70] Train Loss: 1.4739\tTest Loss: 1.4887\tAccuracy: 97.19%\n","[71] Train Loss: 1.4738\tTest Loss: 1.4878\tAccuracy: 97.40%\n","[72] Train Loss: 1.4737\tTest Loss: 1.4879\tAccuracy: 97.43%\n","[73] Train Loss: 1.4735\tTest Loss: 1.4879\tAccuracy: 97.36%\n","[74] Train Loss: 1.4733\tTest Loss: 1.4881\tAccuracy: 97.40%\n","[75] Train Loss: 1.4734\tTest Loss: 1.4877\tAccuracy: 97.52%\n","[76] Train Loss: 1.4732\tTest Loss: 1.4885\tAccuracy: 97.33%\n","[77] Train Loss: 1.4731\tTest Loss: 1.4890\tAccuracy: 97.32%\n","[78] Train Loss: 1.4731\tTest Loss: 1.4880\tAccuracy: 97.45%\n","[79] Train Loss: 1.4729\tTest Loss: 1.4870\tAccuracy: 97.50%\n","[80] Train Loss: 1.4727\tTest Loss: 1.4877\tAccuracy: 97.47%\n","[81] Train Loss: 1.4727\tTest Loss: 1.4876\tAccuracy: 97.45%\n","[82] Train Loss: 1.4727\tTest Loss: 1.4901\tAccuracy: 97.13%\n","[83] Train Loss: 1.4727\tTest Loss: 1.4883\tAccuracy: 97.35%\n","[84] Train Loss: 1.4725\tTest Loss: 1.4873\tAccuracy: 97.45%\n","[85] Train Loss: 1.4725\tTest Loss: 1.4866\tAccuracy: 97.56%\n","[86] Train Loss: 1.4724\tTest Loss: 1.4872\tAccuracy: 97.45%\n","[87] Train Loss: 1.4724\tTest Loss: 1.4873\tAccuracy: 97.46%\n","[88] Train Loss: 1.4722\tTest Loss: 1.4867\tAccuracy: 97.51%\n","[89] Train Loss: 1.4721\tTest Loss: 1.4865\tAccuracy: 97.54%\n","[90] Train Loss: 1.4720\tTest Loss: 1.4868\tAccuracy: 97.53%\n","[91] Train Loss: 1.4719\tTest Loss: 1.4865\tAccuracy: 97.49%\n","[92] Train Loss: 1.4720\tTest Loss: 1.4867\tAccuracy: 97.51%\n","[93] Train Loss: 1.4718\tTest Loss: 1.4867\tAccuracy: 97.48%\n","[94] Train Loss: 1.4719\tTest Loss: 1.4866\tAccuracy: 97.53%\n","[95] Train Loss: 1.4716\tTest Loss: 1.4873\tAccuracy: 97.45%\n","[96] Train Loss: 1.4717\tTest Loss: 1.4868\tAccuracy: 97.53%\n","[97] Train Loss: 1.4716\tTest Loss: 1.4865\tAccuracy: 97.53%\n","[98] Train Loss: 1.4715\tTest Loss: 1.4867\tAccuracy: 97.53%\n","[99] Train Loss: 1.4714\tTest Loss: 1.4864\tAccuracy: 97.58%\n","[100] Train Loss: 1.4713\tTest Loss: 1.4911\tAccuracy: 97.16%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"]},{"cell_type":"markdown","source":["softmax 활성화 함수는 다중 분류에 효과적인 활성화 함수이다.\n","\n","이 활성화 함수를 마지막 층에 적용시켰다.\n","\n","그 결과 Accuracy가 97.16%로 향상되었다."],"metadata":{"id":"PBOHopCM1xyn"}}]}