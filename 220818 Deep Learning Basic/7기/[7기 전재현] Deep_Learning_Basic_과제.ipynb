{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[0818]_Deep_Learning_Basic_과제.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_rjuQY9f2mdS"},"source":["## 과제 1\n","ReLu activation function과 derivative function을 구현해보세요\n","- Hint : np.maximum 함수 사용하면 편리합니다\n","- 다른 방법 사용하셔도 무방합니다\n"]},{"cell_type":"code","metadata":{"id":"puH0YVGI2uLz"},"source":["def relu(x):\n","  if x < 0:\n","    return 0\n","  else:\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def d_relu(x):\n","  if x < 0:\n","    return 0\n","  else:\n","    return 1"],"metadata":{"id":"Esm4jmTVijro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["relu(2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pN6bik_AyZam","executionInfo":{"status":"ok","timestamp":1661007947412,"user_tz":-540,"elapsed":7,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"1b38b34b-4878-44b2-9b7f-df0e3489dfbd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["relu(-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwY7vKvbygOk","executionInfo":{"status":"ok","timestamp":1661007960086,"user_tz":-540,"elapsed":5,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"433415d5-5269-405a-bee3-4659b7296334"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"wz8Hi0Rc2-yJ"},"source":["## 과제 2\n","Deep Learning Basic 코드 파일의 MLP implementation with Numpy library using MNIST dataset 코드 참고해서\n","Three layer MLP 일 때의 backward_pass 함수를 완성해주세요.   \n","- Hint : 코드 파일의 예시는 Two layer MLP\n"]},{"cell_type":"code","metadata":{"id":"fusEy49j3uhs"},"source":["def backward_pass(x, y_true, params):\n","\n","  dS3 = params[\"A3\"] - y_true\n","\n","  grads = {}\n","\n","  grads[\"dW3\"] = np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n","  grads[\"db3\"] = (1/x.shape[1])*np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA2 = np.dot(params[\"W3\"].T, dS3)\n","  dS2 = dA2 * d_sigmoid(params[\"S2\"])\n","\n","  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n","  grads[\"db2\"] =  (1/x.shape[1])*np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n","\n","  dA1 = np.dot(params[\"W2\"].T, dS2)\n","  dS1 = dA1 * d_sigmoid(params[\"S1\"])\n","\n","  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n","  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n","\n","  return grads"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twf-R8s-34zT"},"source":["## 과제 3\n","Deep Learning Basic 코드 파일의 MLP implementation with Pytorch library using MNIST dataset 코드 참고해서\n","Three layer MLP를 구한후, 학습을 돌려 보세요\n","\n","hyperparameter는 다음과 같이 설정\n","\n","- epochs : 100\n","- hiddensize : 128, 64 (two layer)\n","- learning_rate : 0.5"]},{"cell_type":"code","source":["from torchvision import transforms, datasets\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"Cx6kbzS1VwLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bxJO249A3jhk"},"source":["transform = transforms.Compose([\n","    transforms.ToTensor()\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = True,\n","    download  = True,\n","    transform = transform\n",")\n","testset = datasets.MNIST(\n","    root      = './.data/', \n","    train     = False,\n","    download  = True,\n","    transform = transform\n",")"],"metadata":{"id":"U3Zn9Us-V49k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainset[0][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91IUptY2WkiC","executionInfo":{"status":"ok","timestamp":1661067754099,"user_tz":-540,"elapsed":6,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"9d1af862-e3b5-4e65-aee3-e63ecb12d240"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 28, 28])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n","test_loader =  DataLoader(testset, batch_size=32, shuffle=False)"],"metadata":{"id":"VPZMd99cWP5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Linear(784,128)\n","        self.layer2 = nn.Linear(128,64)\n","        self.layer3 = nn.Linear(64,10)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        x = x.view(-1, 784)\n","        out = self.layer1(x)\n","        out = self.relu(out)\n","        out = self.layer2(out)\n","        out = self.relu(out)\n","        out = self.layer3(out)\n","\n","        return out"],"metadata":{"id":"AGdVf14aWZla"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Net()\n","model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_f5QLmyPW0CZ","executionInfo":{"status":"ok","timestamp":1661067815138,"user_tz":-540,"elapsed":3,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"439dd18c-1807-4319-d52d-df63c36b9808"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.5)"],"metadata":{"id":"E-MGNdjAW495"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer):\n","    model.train()\n","    # 배치 당 loss 값을 담을 리스트 생성\n","    batch_losses = []\n","\n","    for data, target in train_loader:\n","        # 옵티마이저의 기울기 초기화\n","        optimizer.zero_grad()\n","\n","        # y pred 값 산출\n","        output = model(data)\n","        # loss 계산\n","        # 정답 데이터와의 cross entropy loss 계산\n","        # 이 loss를 배치 당 loss로 보관\n","        loss = criterion(output, target)\n","        batch_losses.append(loss)\n","\n","        # 기울기 계산\n","        loss.backward()\n","\n","        # 가중치 업데이트!\n","        optimizer.step()\n","        \n","    # 배치당 평균 loss 계산\n","    avg_loss = sum(batch_losses) / len(batch_losses)\n","    \n","    return avg_loss"],"metadata":{"id":"qg3SxlC3W9Xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, test_loader):\n","    # 모델을 평가 모드로 전환\n","    model.eval()\n","\n","    batch_losses = []\n","    correct = 0 \n","\n","    with torch.no_grad(): \n","        for data, target in test_loader:\n","            # 예측값 생성\n","            output = model(data)\n","\n","            # loss 계산 (이전과 동일)\n","            loss = criterion(output, target)\n","            batch_losses.append(loss)\n","\n","           # Accuracy 계산\n","           # y pred와 y가 일치하면 correct에 1을 더해주기\n","            pred = output.max(1, keepdim=True)[1]\n","\n","            # eq() 함수는 값이 일치하면 1을, 아니면 0을 출력.\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    # 배치 당 평균 loss 계산 \n","    avg_loss =  sum(batch_losses) / len(batch_losses)\n","\n","    #정확도 계산\n","    accuracy = 100. * correct / len(test_loader.dataset)\n","\n","    return avg_loss, accuracy"],"metadata":{"id":"ELy1XaAuXEaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 100\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model, train_loader, optimizer)\n","    test_loss, test_accuracy = evaluate(model, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_Woo-iuXHbg","executionInfo":{"status":"ok","timestamp":1661068966966,"user_tz":-540,"elapsed":1029243,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"e99f7fc8-02a6-4d33-cf5b-1500904ec9cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 0.3056\tTest Loss: 0.1471\tAccuracy: 95.57%\n","[2] Train Loss: 0.1295\tTest Loss: 0.1210\tAccuracy: 96.36%\n","[3] Train Loss: 0.1019\tTest Loss: 0.1165\tAccuracy: 96.87%\n","[4] Train Loss: 0.0847\tTest Loss: 0.1198\tAccuracy: 96.80%\n","[5] Train Loss: 0.0718\tTest Loss: 0.1215\tAccuracy: 96.86%\n","[6] Train Loss: 0.0649\tTest Loss: 0.1104\tAccuracy: 96.93%\n","[7] Train Loss: 0.0543\tTest Loss: 0.1344\tAccuracy: 96.72%\n","[8] Train Loss: 0.0547\tTest Loss: 0.1619\tAccuracy: 96.11%\n","[9] Train Loss: 0.0506\tTest Loss: 0.1232\tAccuracy: 97.13%\n","[10] Train Loss: 0.0411\tTest Loss: 0.1450\tAccuracy: 96.71%\n","[11] Train Loss: 0.0505\tTest Loss: 0.1335\tAccuracy: 97.00%\n","[12] Train Loss: 0.0412\tTest Loss: 0.1212\tAccuracy: 97.20%\n","[13] Train Loss: 0.0367\tTest Loss: 0.1502\tAccuracy: 97.21%\n","[14] Train Loss: 0.0433\tTest Loss: 0.1434\tAccuracy: 97.19%\n","[15] Train Loss: 0.0376\tTest Loss: 0.1366\tAccuracy: 97.32%\n","[16] Train Loss: 0.0369\tTest Loss: 0.1143\tAccuracy: 97.68%\n","[17] Train Loss: 0.0345\tTest Loss: 0.1551\tAccuracy: 97.31%\n","[18] Train Loss: 0.0377\tTest Loss: 0.2043\tAccuracy: 96.84%\n","[19] Train Loss: 0.0422\tTest Loss: 0.2010\tAccuracy: 96.74%\n","[20] Train Loss: 0.0410\tTest Loss: 0.1398\tAccuracy: 97.39%\n","[21] Train Loss: 0.0326\tTest Loss: 0.1593\tAccuracy: 97.30%\n","[22] Train Loss: 0.0405\tTest Loss: 0.1503\tAccuracy: 97.36%\n","[23] Train Loss: 0.0369\tTest Loss: 0.1470\tAccuracy: 97.43%\n","[24] Train Loss: 0.0235\tTest Loss: 0.1812\tAccuracy: 97.50%\n","[25] Train Loss: 0.0326\tTest Loss: 0.2025\tAccuracy: 97.49%\n","[26] Train Loss: 0.0262\tTest Loss: 0.1904\tAccuracy: 97.55%\n","[27] Train Loss: 0.0300\tTest Loss: 0.1826\tAccuracy: 97.62%\n","[28] Train Loss: 0.0346\tTest Loss: 0.2058\tAccuracy: 97.34%\n","[29] Train Loss: 0.0456\tTest Loss: 0.2024\tAccuracy: 97.42%\n","[30] Train Loss: 0.0306\tTest Loss: 0.2028\tAccuracy: 97.06%\n","[31] Train Loss: 0.0328\tTest Loss: 0.2491\tAccuracy: 96.96%\n","[32] Train Loss: 0.0289\tTest Loss: 0.1919\tAccuracy: 97.77%\n","[33] Train Loss: 0.0362\tTest Loss: 0.2177\tAccuracy: 97.15%\n","[34] Train Loss: 0.0321\tTest Loss: 0.2476\tAccuracy: 97.30%\n","[35] Train Loss: 0.0293\tTest Loss: 0.2206\tAccuracy: 97.28%\n","[36] Train Loss: 0.0250\tTest Loss: 0.2347\tAccuracy: 97.55%\n","[37] Train Loss: 0.0258\tTest Loss: 0.2959\tAccuracy: 96.92%\n","[38] Train Loss: 0.0321\tTest Loss: 0.2555\tAccuracy: 97.14%\n","[39] Train Loss: 0.0511\tTest Loss: 0.2493\tAccuracy: 96.81%\n","[40] Train Loss: 0.0378\tTest Loss: 0.2454\tAccuracy: 97.30%\n","[41] Train Loss: 0.0386\tTest Loss: 0.2741\tAccuracy: 97.11%\n","[42] Train Loss: 0.0375\tTest Loss: 0.2721\tAccuracy: 97.22%\n","[43] Train Loss: 0.0353\tTest Loss: 0.2508\tAccuracy: 97.45%\n","[44] Train Loss: 0.0367\tTest Loss: 0.2616\tAccuracy: 97.17%\n","[45] Train Loss: 0.0352\tTest Loss: 0.2329\tAccuracy: 97.40%\n","[46] Train Loss: 0.0409\tTest Loss: 0.2769\tAccuracy: 97.21%\n","[47] Train Loss: 0.0430\tTest Loss: 0.2970\tAccuracy: 97.25%\n","[48] Train Loss: 0.0282\tTest Loss: 0.2442\tAccuracy: 97.61%\n","[49] Train Loss: 0.0298\tTest Loss: 0.2510\tAccuracy: 97.27%\n","[50] Train Loss: 0.0386\tTest Loss: 0.2734\tAccuracy: 97.14%\n","[51] Train Loss: 0.0459\tTest Loss: 0.3029\tAccuracy: 97.02%\n","[52] Train Loss: 0.0331\tTest Loss: 0.3254\tAccuracy: 97.25%\n","[53] Train Loss: 0.0437\tTest Loss: 0.2964\tAccuracy: 96.98%\n","[54] Train Loss: 0.0362\tTest Loss: 0.3390\tAccuracy: 96.78%\n","[55] Train Loss: 0.0268\tTest Loss: 0.3437\tAccuracy: 97.02%\n","[56] Train Loss: 0.0334\tTest Loss: 0.3976\tAccuracy: 96.62%\n","[57] Train Loss: 0.0468\tTest Loss: 0.2758\tAccuracy: 97.41%\n","[58] Train Loss: 0.0364\tTest Loss: 0.3007\tAccuracy: 97.25%\n","[59] Train Loss: 0.0572\tTest Loss: 0.3741\tAccuracy: 96.96%\n","[60] Train Loss: 0.0536\tTest Loss: 0.3041\tAccuracy: 97.07%\n","[61] Train Loss: 0.0552\tTest Loss: 0.4828\tAccuracy: 96.00%\n","[62] Train Loss: 0.0563\tTest Loss: 0.4813\tAccuracy: 96.82%\n","[63] Train Loss: 0.0491\tTest Loss: 0.3013\tAccuracy: 97.49%\n","[64] Train Loss: 0.0502\tTest Loss: 0.3136\tAccuracy: 97.34%\n","[65] Train Loss: 0.0407\tTest Loss: 0.2751\tAccuracy: 97.50%\n","[66] Train Loss: 0.0432\tTest Loss: 0.3140\tAccuracy: 97.13%\n","[67] Train Loss: 0.0390\tTest Loss: 0.3687\tAccuracy: 96.95%\n","[68] Train Loss: 0.0706\tTest Loss: 0.5886\tAccuracy: 94.88%\n","[69] Train Loss: 0.1020\tTest Loss: 0.3380\tAccuracy: 97.17%\n","[70] Train Loss: 0.0526\tTest Loss: 0.3098\tAccuracy: 97.54%\n","[71] Train Loss: 0.0438\tTest Loss: 0.2745\tAccuracy: 97.34%\n","[72] Train Loss: 0.0456\tTest Loss: 0.3975\tAccuracy: 97.06%\n","[73] Train Loss: 0.0700\tTest Loss: 0.4422\tAccuracy: 96.60%\n","[74] Train Loss: 0.0457\tTest Loss: 0.3649\tAccuracy: 97.01%\n","[75] Train Loss: 0.0402\tTest Loss: 0.4234\tAccuracy: 96.87%\n","[76] Train Loss: 0.0383\tTest Loss: 0.3326\tAccuracy: 97.39%\n","[77] Train Loss: 0.0264\tTest Loss: 0.3258\tAccuracy: 97.68%\n","[78] Train Loss: 0.0236\tTest Loss: 0.3670\tAccuracy: 97.62%\n","[79] Train Loss: 0.0258\tTest Loss: 0.4002\tAccuracy: 97.50%\n","[80] Train Loss: 0.0395\tTest Loss: 0.4139\tAccuracy: 97.48%\n","[81] Train Loss: 0.0279\tTest Loss: 0.4180\tAccuracy: 97.59%\n","[82] Train Loss: 0.0733\tTest Loss: 0.4609\tAccuracy: 96.82%\n","[83] Train Loss: 0.1205\tTest Loss: 0.4429\tAccuracy: 96.72%\n","[84] Train Loss: 0.0811\tTest Loss: 0.3320\tAccuracy: 96.92%\n","[85] Train Loss: 0.0864\tTest Loss: 0.5003\tAccuracy: 96.87%\n","[86] Train Loss: 0.0921\tTest Loss: 0.3425\tAccuracy: 97.36%\n","[87] Train Loss: 0.0552\tTest Loss: 0.3735\tAccuracy: 97.68%\n","[88] Train Loss: 0.0508\tTest Loss: 0.3776\tAccuracy: 96.44%\n","[89] Train Loss: 0.0569\tTest Loss: 0.4209\tAccuracy: 97.03%\n","[90] Train Loss: 0.0645\tTest Loss: 0.4011\tAccuracy: 96.99%\n","[91] Train Loss: 0.0588\tTest Loss: 0.4596\tAccuracy: 97.18%\n","[92] Train Loss: 0.1006\tTest Loss: 0.3670\tAccuracy: 96.94%\n","[93] Train Loss: 0.0902\tTest Loss: 0.4306\tAccuracy: 96.70%\n","[94] Train Loss: 0.0833\tTest Loss: 0.4139\tAccuracy: 97.14%\n","[95] Train Loss: 0.0863\tTest Loss: 0.3508\tAccuracy: 96.67%\n","[96] Train Loss: 0.0776\tTest Loss: 0.4034\tAccuracy: 96.37%\n","[97] Train Loss: 0.0706\tTest Loss: 0.5063\tAccuracy: 97.26%\n","[98] Train Loss: 0.0750\tTest Loss: 0.4961\tAccuracy: 96.82%\n","[99] Train Loss: 0.0710\tTest Loss: 0.4362\tAccuracy: 97.28%\n","[100] Train Loss: 0.0463\tTest Loss: 0.5391\tAccuracy: 97.34%\n"]}]},{"cell_type":"markdown","metadata":{"id":"WaqqRzF73oBu"},"source":["## 과제 4\n","과제 3 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n","\n","- Hint : Activation function, hyperparameter setting"]},{"cell_type":"code","metadata":{"id":"k6b82DZG6W3j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661069592540,"user_tz":-540,"elapsed":318,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"67de60b6-07b8-426d-ef13-415bb8b7483f"},"source":["model2 = Net()\n","model2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (layer1): Linear(in_features=784, out_features=128, bias=True)\n","  (layer2): Linear(in_features=128, out_features=64, bias=True)\n","  (layer3): Linear(in_features=64, out_features=10, bias=True)\n","  (relu): ReLU()\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["optimizer2 = optim.SGD(model2.parameters(), lr=0.01)"],"metadata":{"id":"vn-_q7omZMiB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 50\n","\n","for epoch in range(1, EPOCHS + 1):\n","    train_loss = train(model2, train_loader, optimizer2)\n","    test_loss, test_accuracy = evaluate(model2, test_loader)\n","    \n","    print('[{}] Train Loss: {:.4f}\\tTest Loss: {:.4f}\\tAccuracy: {:.2f}%'.format(\n","          epoch, train_loss, test_loss, test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IQ0Dw_tuZU89","executionInfo":{"status":"ok","timestamp":1661070094815,"user_tz":-540,"elapsed":491116,"user":{"displayName":"전재현","userId":"14279104608242621795"}},"outputId":"edddc1ad-c149-42db-dd36-83da0b25a249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1] Train Loss: 1.1580\tTest Loss: 0.4247\tAccuracy: 88.39%\n","[2] Train Loss: 0.3790\tTest Loss: 0.3206\tAccuracy: 90.82%\n","[3] Train Loss: 0.3145\tTest Loss: 0.2815\tAccuracy: 92.04%\n","[4] Train Loss: 0.2773\tTest Loss: 0.2553\tAccuracy: 92.60%\n","[5] Train Loss: 0.2465\tTest Loss: 0.2273\tAccuracy: 93.64%\n","[6] Train Loss: 0.2196\tTest Loss: 0.2081\tAccuracy: 93.80%\n","[7] Train Loss: 0.1966\tTest Loss: 0.1809\tAccuracy: 94.68%\n","[8] Train Loss: 0.1771\tTest Loss: 0.1668\tAccuracy: 95.02%\n","[9] Train Loss: 0.1608\tTest Loss: 0.1551\tAccuracy: 95.37%\n","[10] Train Loss: 0.1467\tTest Loss: 0.1456\tAccuracy: 95.67%\n","[11] Train Loss: 0.1343\tTest Loss: 0.1350\tAccuracy: 95.90%\n","[12] Train Loss: 0.1240\tTest Loss: 0.1256\tAccuracy: 96.25%\n","[13] Train Loss: 0.1149\tTest Loss: 0.1221\tAccuracy: 96.39%\n","[14] Train Loss: 0.1070\tTest Loss: 0.1128\tAccuracy: 96.59%\n","[15] Train Loss: 0.0998\tTest Loss: 0.1111\tAccuracy: 96.72%\n","[16] Train Loss: 0.0934\tTest Loss: 0.1032\tAccuracy: 97.00%\n","[17] Train Loss: 0.0877\tTest Loss: 0.1060\tAccuracy: 96.88%\n","[18] Train Loss: 0.0830\tTest Loss: 0.0961\tAccuracy: 97.16%\n","[19] Train Loss: 0.0782\tTest Loss: 0.0955\tAccuracy: 97.17%\n","[20] Train Loss: 0.0741\tTest Loss: 0.0911\tAccuracy: 97.33%\n","[21] Train Loss: 0.0703\tTest Loss: 0.0896\tAccuracy: 97.25%\n","[22] Train Loss: 0.0666\tTest Loss: 0.0857\tAccuracy: 97.43%\n","[23] Train Loss: 0.0635\tTest Loss: 0.0849\tAccuracy: 97.45%\n","[24] Train Loss: 0.0601\tTest Loss: 0.0887\tAccuracy: 97.15%\n","[25] Train Loss: 0.0575\tTest Loss: 0.0820\tAccuracy: 97.58%\n","[26] Train Loss: 0.0548\tTest Loss: 0.0821\tAccuracy: 97.65%\n","[27] Train Loss: 0.0520\tTest Loss: 0.0811\tAccuracy: 97.61%\n","[28] Train Loss: 0.0498\tTest Loss: 0.0790\tAccuracy: 97.54%\n","[29] Train Loss: 0.0474\tTest Loss: 0.0794\tAccuracy: 97.57%\n","[30] Train Loss: 0.0453\tTest Loss: 0.0769\tAccuracy: 97.83%\n","[31] Train Loss: 0.0431\tTest Loss: 0.0753\tAccuracy: 97.82%\n","[32] Train Loss: 0.0414\tTest Loss: 0.0772\tAccuracy: 97.63%\n","[33] Train Loss: 0.0393\tTest Loss: 0.0761\tAccuracy: 97.79%\n","[34] Train Loss: 0.0376\tTest Loss: 0.0756\tAccuracy: 97.70%\n","[35] Train Loss: 0.0361\tTest Loss: 0.0757\tAccuracy: 97.79%\n","[36] Train Loss: 0.0344\tTest Loss: 0.0759\tAccuracy: 97.78%\n","[37] Train Loss: 0.0328\tTest Loss: 0.0719\tAccuracy: 97.92%\n","[38] Train Loss: 0.0315\tTest Loss: 0.0719\tAccuracy: 97.88%\n","[39] Train Loss: 0.0299\tTest Loss: 0.0744\tAccuracy: 97.80%\n","[40] Train Loss: 0.0287\tTest Loss: 0.0742\tAccuracy: 97.88%\n","[41] Train Loss: 0.0275\tTest Loss: 0.0736\tAccuracy: 97.88%\n","[42] Train Loss: 0.0261\tTest Loss: 0.0737\tAccuracy: 97.85%\n","[43] Train Loss: 0.0249\tTest Loss: 0.0746\tAccuracy: 97.81%\n","[44] Train Loss: 0.0239\tTest Loss: 0.0723\tAccuracy: 97.94%\n","[45] Train Loss: 0.0229\tTest Loss: 0.0731\tAccuracy: 98.00%\n","[46] Train Loss: 0.0219\tTest Loss: 0.0720\tAccuracy: 97.90%\n","[47] Train Loss: 0.0210\tTest Loss: 0.0722\tAccuracy: 97.97%\n","[48] Train Loss: 0.0200\tTest Loss: 0.0734\tAccuracy: 97.87%\n","[49] Train Loss: 0.0193\tTest Loss: 0.0736\tAccuracy: 97.91%\n","[50] Train Loss: 0.0183\tTest Loss: 0.0733\tAccuracy: 97.87%\n"]}]},{"cell_type":"markdown","metadata":{"id":"pboMIBQq7onH"},"source":["**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**\n","* 우선 learning rate가 0.5로 너무 커서 test loss뿐만 아니라 train loss까지 계속 작아지지 못하고 발산하게 되는 양상을 발견하였다. 따라서 learning rate를 0.01로 두고 재학습을 진행하였다.\n","* 뿐만 아니라 epoch 개수가 100으로 학습 난이도에 비해 너무 높게 설정되어 이거 때문에도 overfitting이 발생했다고 판단해, epoch을 50 정도로 두고 재학습을 진행하였다.\n","* 그 결과, test loss와 train loss가 같이 계속해서 작아지는 좋은 optimization을 만들 수 있었다. 여기서 epoch을 약 40 선에서 끊어 test loss가 가장 작은 곳에서 학습을 종료하는 방안이나 learning rate를 좀더 줄여 더 알맞게 수렴하도록 하는 방안도 있을 것이다."]}]}