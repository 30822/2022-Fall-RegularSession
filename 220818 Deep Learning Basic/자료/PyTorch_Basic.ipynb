{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch Basic",
      "provenance": [],
      "collapsed_sections": [
        "mRwiu2K93KUC",
        "hQrEwOpXb9Gh",
        "zjosrOn8mOMV",
        "OgPaSNS2mVPn",
        "zeH5501nmh7W",
        "uN6FfqU9wFeG"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference  \n",
        "https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/schedule.html"
      ],
      "metadata": {
        "id": "VyKOCHkJ4rtn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Introduction  \n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "dqGZdDRvmLP2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRwiu2K93KUC"
      },
      "source": [
        "## 1.1. Google Colab (구글 코랩)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colaboratory는 클라우드 기반의 Python 3 (iPython Notebook) 개발환경(IDE) 이다.\n",
        "\n",
        "코랩를 사용하면 구글이 무료로 제공하는 클라우드 가상머신 위에서 Python 코드를 개발할 수 있다. 또한 Python 기반의 기계학습 라이브러리들이 (PyTorch, TensorFlow 포함) 이미 가상머신에 설치되어 있어 기계학습 실습을 쉽게 할 수 있다.\n",
        "\n",
        "단 **리소스 제한**이 존재한다. 90분간 코드 실행이 없으면 자동으로 세션이 종료된다 (가상머신이 초기화됨). 연속하여 12시간 구동하면 또한 세션이 종료된다. 월 9.99달러의 구독료를 내면 세션이 더 오래 지속되고 더 고성능 GPU를 사용할 수 있다.\n",
        "\n",
        "아래 코드를 통하여 할당된 가상머신의 정보를 파악할 수 있다."
      ],
      "metadata": {
        "id": "ZbDLhboGnQyK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUmswNX-3X4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d162519-c99b-490c-840d-f9d7c36d47c5"
      },
      "source": [
        "# 가상머신 정보\n",
        "# 출처: https://youngq.tistory.com/category/머신러닝\n",
        "!head /proc/cpuinfo    # CPU\n",
        "!head -n 3 /proc/meminfo  # RAM\n",
        "!df -h      # 하드디스크\n",
        "!nvidia-smi      # GPU"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 63\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.30GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2299.998\n",
            "cache size\t: 46080 KB\n",
            "physical id\t: 0\n",
            "MemTotal:       13298580 kB\n",
            "MemFree:         9976236 kB\n",
            "MemAvailable:   12275984 kB\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         167G   38G  130G  23% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm             5.7G     0  5.7G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  812M  59% /sbin/docker-init\n",
            "/dev/sda1       174G   41G  133G  24% /opt/bin/.nvidia\n",
            "tmpfs           6.4G   28K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "Sun Aug 14 16:54:57 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrEwOpXb9Gh"
      },
      "source": [
        "## 1.2 Python 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAKwfCs_mK3d"
      },
      "source": [
        "If you're unfamiliar with Python 3, here are some of the most common changes from Python 2 to look out for.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjosrOn8mOMV"
      },
      "source": [
        "### Print is a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41SjFuamR7d"
      },
      "source": [
        "print(\"Hello!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEh1swLBmQN-"
      },
      "source": [
        "Without parentheses, printing will not work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgPaSNS2mVPn"
      },
      "source": [
        "### Floating point division by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQKlRZ8KmYDl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "664c8aae-ec3f-407d-989a-c07269d09335"
      },
      "source": [
        "5 / 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOmfK0WWmb2V"
      },
      "source": [
        "To do integer division, we use two backslashes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUg1MjiPmgNX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "85a64713-0fe7-4cc0-a294-308284b46d59"
      },
      "source": [
        "5 // 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeH5501nmh7W"
      },
      "source": [
        "### No xrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wNKyyilmkMy"
      },
      "source": [
        "The xrange from Python 2 is now merged into \"range\" for Python 3 and there is no xrange in Python 3. In Python 3, range(3) does not create a list of 3 elements as it would in Python 2, rather just creates a more memory efficient iterator.\n",
        "\n",
        "Hence,  \n",
        "xrange in Python 3: Does not exist  \n",
        "range in Python 3: Has very similar behavior to Python 2's xrange"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP8Dk9PAmnQh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c689e597-b0f0-4b88-ee5f-1d6224f2967d"
      },
      "source": [
        "for i in range(3):\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SKbKDgLmqd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9758e632-7a3b-4d86-e37c-c0cc7175ab8d"
      },
      "source": [
        "range(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "range(0, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm_VcW3VmsSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2f8f6b-2ed5-457e-a0de-d42d1ce2b25b"
      },
      "source": [
        "# If need be, can use the following to get a similar behavior to Python 2's range:\n",
        "print(list(range(3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEmHrgBsgX4"
      },
      "source": [
        "# 2. PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3e_Nux0siHo"
      },
      "source": [
        "\n",
        "[파이토치(PyTorch)](https://pytorch.org/)는 기계학습을 위한 오픈소스 프레임워크이다. 파이토치의 핵심 기능은 다음과 같다:\n",
        "\n",
        "- 다차원의 **텐서(tensor)** 객체를 지원한다. 파이토치의 텐서 객체는 [numpy](https://numpy.org/)의 텐서 객체와 비슷하나, 추가적으로 ***GPU 가속***을 지원한다!\n",
        "- 최적화된 **autograd** 엔진이 내장되어 있어 미분값을 자동적으로 계산해준다.\n",
        "- **딥러닝 모델**을 빌드하고 배포하는 과정에 필요한 API 및 코드들이 깔끔하면서도 모듈화되어 있다.\n",
        "\n",
        "\n",
        "파이토치에 대한 더 자세한 정보는 [공식 튜토리얼](https://pytorch.org/tutorials/) 이나  [공식 매뉴얼](https://pytorch.org/docs/1.1.0/)을 참조.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiO3_y-vKQ9"
      },
      "source": [
        "파이토치 사용을 위하여는 먼저 `torch` 패키지를 임포트해야 한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sydFm14itrqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56cb5ac-1651-4d4b-999c-1f05e68a9e53"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.1+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(참고) ndarray와 Tensor 속성 비교\n",
        "\n",
        "| ndarray | Tensor |\n",
        "|------|------|\n",
        "| `axis` | `dim` |\n",
        "| `ndarray.ndim` | `Tensor.dim()` |\n",
        "| `ndarray.shape` | `Tensor.size()` |\n",
        "| `np.reshape()` | `Tensor.view()` |\n",
        "| `np.concatenate()` | `Tensor.cat()` |\n",
        "| `np.swapaxes()` | `Tensor.transpose()` |\n",
        "| `np.transpose()` | `Tensor.permute()` |"
      ],
      "metadata": {
        "id": "p_Ipb6K_tI78"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrBSx6hYu8ca"
      },
      "source": [
        "## 2.1. Tensor Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWagwmXuvIle"
      },
      "source": [
        "### Creating and Accessing tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpwfVUvPu_lF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579aa38d-7ad5-416f-ff39-73b9aadf6bdd"
      },
      "source": [
        "# Chunk 2.1.1.\n",
        "\n",
        "# Create a rank 1 tensor from a Python list\n",
        "a = torch.tensor([1, 2, 3])\n",
        "print('Here is a:')\n",
        "print(a)\n",
        "print('type(a): ', type(a))\n",
        "print('rank of a: ', a.dim())  #  1\n",
        "print('a.shape: ', a.shape) # 고양이 이미지: (10000 ,800 , 600 , 3)\n",
        "\n",
        "# Access elements using square brackets\n",
        "print()\n",
        "print('a[0]: ', a[0])\n",
        "print('type(a[0]): ', type(a[0]))\n",
        "print('type(a[0].item()): ', type(a[0].item()))\n",
        "\n",
        "# Mutate elements using square brackets\n",
        "a[1] = 10\n",
        "print()\n",
        "print('a after mutating:')\n",
        "print(a)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a:\n",
            "tensor([1, 2, 3])\n",
            "type(a):  <class 'torch.Tensor'>\n",
            "rank of a:  1\n",
            "a.shape:  torch.Size([3])\n",
            "\n",
            "a[0]:  tensor(1)\n",
            "type(a[0]):  <class 'torch.Tensor'>\n",
            "type(a[0].item()):  <class 'int'>\n",
            "\n",
            "a after mutating:\n",
            "tensor([ 1, 10,  3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZq4zsnLEgXH"
      },
      "source": [
        "The example above shows a one-dimensional tensor; we can similarly create tensors with two or more dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TcvHxpTFUcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada49e61-a1aa-4ede-e5bb-b6f902edea34"
      },
      "source": [
        "# Chunk 2.1.2.\n",
        "\n",
        "# Create a two-dimensional tensor\n",
        "b = torch.tensor([[1, 2, 3], [4, 5, 5]])\n",
        "print('Here is b:')\n",
        "print(b)\n",
        "print('rank of b:', b.dim())  #  2\n",
        "print('b.shape: ', b.shape)\n",
        "\n",
        "# Access elements from a multidimensional tensor\n",
        "print()\n",
        "print('b[0, 1]:', b[0, 1])\n",
        "print('b[1, 2]:', b[1, 2])\n",
        "\n",
        "# Mutate elements of a multidimensional tensor\n",
        "b[1, 1] = 100\n",
        "print()\n",
        "print('b after mutating:')\n",
        "print(b)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is b:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 5]])\n",
            "rank of b: 2\n",
            "b.shape:  torch.Size([2, 3])\n",
            "\n",
            "b[0, 1]: tensor(2)\n",
            "b[1, 2]: tensor(5)\n",
            "\n",
            "b after mutating:\n",
            "tensor([[  1,   2,   3],\n",
            "        [  4, 100,   5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz_VDA3IvP33"
      },
      "source": [
        "### Tensor constructors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoAlslEdwV-k"
      },
      "source": [
        "PyTorch provides many convenience methods for constructing tensors; this avoids the need to use Python lists. For example:\n",
        "\n",
        "- [`torch.zeros`](https://pytorch.org/docs/1.1.0/torch.html#torch.zeros): Creates a tensor of all zeros\n",
        "- [`torch.ones`](https://pytorch.org/docs/1.1.0/torch.html#torch.ones): Creates a tensor of all ones\n",
        "- [`torch.rand`](https://pytorch.org/docs/1.1.0/torch.html#torch.rand): Creates a tensor with uniform random numbers\n",
        "\n",
        "You can find a full list of tensor creation operations [in the documentation](https://pytorch.org/docs/1.1.0/torch.html#creation-ops)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL6DXGXzxHBA"
      },
      "source": [
        "# Chunk 2.1.4.\n",
        "\n",
        "# Create a tensor of all zeros\n",
        "a = torch.zeros(2, 3)\n",
        "print('tensor of zeros:')\n",
        "print(a)\n",
        "\n",
        "# Create a tensor of all ones\n",
        "b = torch.ones(1, 2)\n",
        "print('\\ntensor of ones:')\n",
        "print(b)\n",
        "\n",
        "# Create a 3x3 identity matrix\n",
        "c = torch.eye(3)\n",
        "print('\\nidentity matrix:')\n",
        "print(c)\n",
        "\n",
        "# Tensor of random values\n",
        "d = torch.rand(4, 5)\n",
        "print('\\nrandom tensor:')\n",
        "print(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz_hiJD33fu1"
      },
      "source": [
        "### Datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1xBunZ3ixx"
      },
      "source": [
        "* bit수가 줄어들면 표현할 수 있는 수의 범위가 줄어든다. 대신 메모리 저장공간과 연산속도가 모두 빨라진다. 만일 프로젝트 수행시 계산효율적 고민이 필요하면, 불러온 텐서를 더 낮은 bit의 dtype로 cast하는 방안을 고려해볼 것. \n",
        "* bit의 조합으로 숫자를 표현하는 원리는 [위키](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) 참조. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vREVDf_n31Qz"
      },
      "source": [
        "# Chunk 2.1.6.\n",
        "\n",
        "# Let torch choose the datatype\n",
        "x0 = torch.tensor([1, 2])   # List of integers\n",
        "x1 = torch.tensor([1., 2.]) # List of floats\n",
        "x2 = torch.tensor([1., 2])  # Mixed list\n",
        "print('dtype when torch chooses for us:')\n",
        "print('List of integers:', x0.dtype)\n",
        "print('List of floats:', x1.dtype)\n",
        "print('Mixed list:', x2.dtype)\n",
        "\n",
        "# 이 아래는 복붙\n",
        "# Force a particular datatype\n",
        "y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n",
        "y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n",
        "y2 = torch.tensor([1, 2], dtype=torch.int64)    # 64-bit (signed) integer\n",
        "print('\\ndtype when we force a datatype:')\n",
        "print('32-bit float: ', y0.dtype)\n",
        "print('32-bit integer: ', y1.dtype)\n",
        "print('64-bit integer: ', y2.dtype)\n",
        "\n",
        "# Other creation ops also take a dtype argument\n",
        "z0 = torch.ones(1, 2)  # Let torch choose for us\n",
        "z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n",
        "z2 = torch.ones(1, 2, dtype=torch.uint8) # 8-bit (unsigned) integer\n",
        "print('\\ntorch.ones with different dtypes')\n",
        "print('default dtype:', z0.dtype)\n",
        "print('16-bit integer:', z1.dtype)\n",
        "print('8-bit unsigned integer:', z2.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2reBgQmx_x4"
      },
      "source": [
        "We can **cast** a tensor to another datatype using the [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) method; there are also convenience methods like [`.float()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.float) and [`.long()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.long) that cast to particular datatypes:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAMpwGsdyHAw"
      },
      "source": [
        "# Chunk 2.1.7.\n",
        "\n",
        "x0 = torch.eye(3, dtype=torch.int64)\n",
        "x1 = x0.float()  # Cast to 32-bit float\n",
        "x2 = x0.double() # Cast to 64-bit float\n",
        "x3 = x0.to(torch.float32) # Alternate way to cast to 32-bit float\n",
        "x4 = x0.to(torch.float64) # Alternate way to cast to 64-bit float\n",
        "print('x0:', x0.dtype)\n",
        "print('x1:', x1.dtype)\n",
        "print('x2:', x2.dtype)\n",
        "print('x3:', x3.dtype)\n",
        "print('x4:', x4.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1APDsx54xV6p"
      },
      "source": [
        "# Chunk 2.1.8.\n",
        "\n",
        "x0 = torch.eye(3, dtype=torch.float64)  # Shape (3, 3), dtype torch.float64\n",
        "x1 = torch.zeros_like(x0)               # Shape (3, 3), dtype torch.float64\n",
        "  # x0와 동일한 dtype 및 shape\n",
        "x2 = x0.new_zeros(4, 5)                 # Shape (4, 5), dtype torch.float64\n",
        "  # x0와 동일한 dtype이되 shape은 (4,5)로\n",
        "x3 = torch.ones(6, 7).to(x0)            # Shape (6, 7), dtype torch.float64)\n",
        "  # shape (6,7)의 텐서 torch.ones(6,7)을, x0와 같은 dtype를 취하도록\n",
        "print('x0 shape is %r, dtype is %r' % (x0.shape, x0.dtype))\n",
        "print('x1 shape is %r, dtype is %r' % (x1.shape, x1.dtype))\n",
        "print('x2 shape is %r, dtype is %r' % (x2.shape, x2.dtype))\n",
        "print('x3 shape is %r, dtype is %r' % (x3.shape, x3.dtype))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlANfnILvX3S"
      },
      "source": [
        "## 2.2. Tensor indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-PoTWNvbba"
      },
      "source": [
        "### Slice indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEr5BzdUdCtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a3bf864-dd12-46c4-a7cd-a50064b8a3fe"
      },
      "source": [
        "# Chunk 2.2.1.\n",
        "\n",
        "a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n",
        "print(0, a)        # (0) Original tensor\n",
        "print(1, a[2:5])   # (1) Elements between index 2 and 5 (2, 3, 4번째 elements)\n",
        "print(2, a[2:])    # (2) Elements after index 2\n",
        "print(3, a[:5])    # (3) Elements before index 5\n",
        "print(4, a[:])     # (4) All elements\n",
        "print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5 (1번째, 3번째)\n",
        "print(6, a[:-1])   # (6) All but the last element --> a[:-2], a[-2:]도 해 보세요.\n",
        "print(7, a[-4::2]) # (7) Every second element, starting from the fourth-last"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "1 tensor([22, 33, 44])\n",
            "2 tensor([22, 33, 44, 55, 66])\n",
            "3 tensor([ 0, 11, 22, 33, 44])\n",
            "4 tensor([ 0, 11, 22, 33, 44, 55, 66])\n",
            "5 tensor([11, 33])\n",
            "6 tensor([ 0, 11, 22, 33, 44, 55])\n",
            "7 tensor([33, 55])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5fOdjTUyhNf"
      },
      "source": [
        "# Chunk 2.2.2.\n",
        "\n",
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "print('shape: ', a.shape)\n",
        "\n",
        "# Get row 1, and all columns. \n",
        "print('\\nSingle row:')\n",
        "print(a[1, :])\n",
        "# * 주 : R에서처럼 a[ ,1] 입력하면 에러남. --> a[:, 1]\n",
        "print(a[1])  # Gives the same result; we can omit : for trailing dimensions\n",
        "print('shape: ', a[1].shape)\n",
        "\n",
        "print('\\nSingle column:')\n",
        "print(a[:, 1])\n",
        "print('shape: ', a[:, 1].shape)\n",
        "a\n",
        "# 여기서부터 복붙\n",
        "# Get the first two rows and the last three columns\n",
        "print('\\nFirst two rows, last two columns:')\n",
        "print(a[:2, -3:])\n",
        "print('shape: ', a[:2, -3:].shape)\n",
        "\n",
        "# Get every other row, and columns at index 1 and 2\n",
        "print('\\nEvery other row, middle columns:')\n",
        "print(a[::2, 1:3])\n",
        "print('shape: ', a[::2, 1:3].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1kHcc5jsF-c"
      },
      "source": [
        "# Chunk 2.2.3.\n",
        "\n",
        "\n",
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor')\n",
        "print(a)\n",
        "\n",
        "row_r1 = a[1, :]    # Rank 1 view of the second row of a  \n",
        "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
        "print('\\nTwo ways of accessing a single row:')\n",
        "print(row_r1, row_r1.shape)\n",
        "print(row_r2, row_r2.shape)\n",
        "\n",
        "# We can make the same distinction when accessing columns::\n",
        "col_r1 = a[:, 1]\n",
        "col_r2 = a[:, 1:2]\n",
        "print('\\nTwo ways of accessing a single column:')\n",
        "print(col_r1, col_r1.shape)\n",
        "print(col_r2, col_r2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXbikYPwyxGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f50e0c-7dcd-416f-fffe-2f0e9c2d32b8"
      },
      "source": [
        "# Chunk 2.2.4.\n",
        "\n",
        "# Create a tensor, a slice, and a clone of a slice\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "b = a[0, 1:] # a[0, 1:]와 b는 메모리상의 같은 객체를 지칭하고 있음. (따라서 b의 변경에 따라 a도 바뀜)\n",
        "c = a[0, 1:].clone()  # 메모리에서 a[0, 1:]의 사본을 만든 뒤 그것을 c가 가리키게 함.\n",
        "print('Before mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "a[0, 1] = 20  # a[0, 1] and b[0] point to the same element\n",
        "print(a)\n",
        "print(b)\n",
        "b[1] = 30     # b[1] and a[0, 2] point to the same element\n",
        "print(a)\n",
        "print(b)\n",
        "c[2] = 40     # c is a clone, so it has its own data\n",
        "print('\\nAfter mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before mutating:\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "tensor([2, 3, 4])\n",
            "tensor([2, 3, 4])\n",
            "tensor([[ 1, 20,  3,  4],\n",
            "        [ 5,  6,  7,  8]])\n",
            "tensor([20,  3,  4])\n",
            "tensor([[ 1, 20, 30,  4],\n",
            "        [ 5,  6,  7,  8]])\n",
            "tensor([20, 30,  4])\n",
            "\n",
            "After mutating:\n",
            "tensor([[ 1, 20, 30,  4],\n",
            "        [ 5,  6,  7,  8]])\n",
            "tensor([20, 30,  4])\n",
            "tensor([ 2,  3, 40])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFnky42Rx2I5"
      },
      "source": [
        "# Chunk 2.2.7.\n",
        "\n",
        "a = torch.zeros(2, 4, dtype=torch.int64)\n",
        "a[:, :2] = 1\n",
        "a[:, 2:] = torch.tensor([[2, 3], [4, 5]])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y93rPhGveWw"
      },
      "source": [
        "### Integer tensor indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXePPNkjM_SD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5a7340-5e44-448b-c9ac-dd4bcb043d6e"
      },
      "source": [
        "# Chunk 2.2.9.\n",
        "\n",
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Create a new tensor of shape (5, 4) by reordering rows from a:\n",
        "# - First two rows same as the first row of a\n",
        "# - Third row is the same as the last row of a\n",
        "# - Fourth and fifth rows are the same as the second row from a\n",
        "idx = [0, 0, 2, 1, 1]  # index arrays can be Python lists of integers\n",
        "print('\\nReordered rows:')\n",
        "print(a[idx])\n",
        "\n",
        "# Create a new tensor of shape (3, 4) by reversing the columns from a\n",
        "idx = torch.tensor([3, 2, 1, 0])  # Index arrays can be int64 torch tensors\n",
        "print('\\nReordered columns:')\n",
        "print(a[:, idx])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 9, 10, 11, 12]])\n",
            "\n",
            "Reordered rows:\n",
            "tensor([[ 1,  2,  3,  4],\n",
            "        [ 1,  2,  3,  4],\n",
            "        [ 9, 10, 11, 12],\n",
            "        [ 5,  6,  7,  8],\n",
            "        [ 5,  6,  7,  8]])\n",
            "\n",
            "Reordered columns:\n",
            "tensor([[ 4,  3,  2,  1],\n",
            "        [ 8,  7,  6,  5],\n",
            "        [12, 11, 10,  9]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocIR8R5ZSEaP"
      },
      "source": [
        "# Chunk 2.2.10.\n",
        "\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "idx = [0, 1, 2]\n",
        "print('\\nGet the diagonal:')\n",
        "print(a[idx, idx])\n",
        "\n",
        "# * 주의. R의 행렬에서도 에서도 a[idx, idx] 식의 문법을 지원하나 결과가 다름.\n",
        "\n",
        "# Modify the diagonal\n",
        "a[idx, idx] = torch.tensor([11, 22, 33])\n",
        "print('\\nAfter setting the diagonal:')\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWA8E8iI0x17"
      },
      "source": [
        "# Create a new tensor from which we will select elements\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Take on element from each row of a:\n",
        "# from row 0, take element 1;\n",
        "# from row 1, take element 2;\n",
        "# from row 2, take element 1;\n",
        "# from row 3, take element 0\n",
        "idx0 = torch.arange(4)  # Quick way to build [0, 1, 2, 3]\n",
        "idx1 = torch.tensor([1, 2, 1, 0])\n",
        "print('\\nSelect one element from each row:')\n",
        "print(a[idx0, idx1])\n",
        "\n",
        "# Now set each of those elements to zero\n",
        "a[idx0, idx1] = 0\n",
        "print('\\nAfter modifying one element from each row:')\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGt8ZPb_vixw"
      },
      "source": [
        "### Boolean tensor indexing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Zf7rb82Dkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a785eee-69d7-46ef-9ce3-0ef0b09c4d17"
      },
      "source": [
        "# Chunk 2.2.12.\n",
        "\n",
        "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Find the elements of a that are bigger than 3. The mask has the same shape as\n",
        "# a, where each element of mask tells whether the corresponding element of a\n",
        "# is greater than three.\n",
        "mask = (a > 3)\n",
        "print('\\nMask tensor:')\n",
        "print(mask)\n",
        "\n",
        "# We can use the mask to construct a rank-1 tensor containing the elements of a\n",
        "# that are selected by the mask\n",
        "print('\\nSelecting elements with the mask:')\n",
        "print(a[mask])\n",
        "\n",
        "# We can also use boolean masks to modify tensors; for example this sets all\n",
        "# elements <= 3 to zero:\n",
        "a[a <= 3] = 0\n",
        "print('\\nAfter modifying with a mask:')\n",
        "print(a)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "\n",
            "Mask tensor:\n",
            "tensor([[False, False],\n",
            "        [False,  True],\n",
            "        [ True,  True]])\n",
            "\n",
            "Selecting elements with the mask:\n",
            "tensor([4, 5, 6])\n",
            "\n",
            "After modifying with a mask:\n",
            "tensor([[0, 0],\n",
            "        [0, 4],\n",
            "        [5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-xqELwyqpN"
      },
      "source": [
        "## 2.3. Reshaping operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9_eXuU4OG8"
      },
      "source": [
        "### View"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw-M7C_61FZK"
      },
      "source": [
        "# Chunk 2.3.1.\n",
        "\n",
        "x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Flatten x0 into a rank 1 vector of shape (8,)\n",
        "x1 = x0.reshape(8)\n",
        "print('\\nFlattened tensor:')\n",
        "print(x1)\n",
        "print('shape:', x1.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"row vector\" of shape (1, 8)\n",
        "x2 = x1.view(1, 8)\n",
        "print('\\nRow vector:')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"column vector\" of shape (8, 1)\n",
        "x3 = x1.view(8, 1)\n",
        "print('\\nColumn vector:')\n",
        "print(x3)\n",
        "print('shape:', x3.shape)\n",
        "\n",
        "# Convert x1 to a rank 3 tensor of shape (2, 2, 2):\n",
        "x4 = x1.view(2, 2, 2)\n",
        "print('\\nRank 3 tensor:')\n",
        "print(x4)\n",
        "print('shape:', x4.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNWu-R_J2qFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564fd1c8-80cf-4374-d756-50c972686875"
      },
      "source": [
        "# Chunk 2.3.2.\n",
        "\n",
        "# 복붙\n",
        "# We can reuse these functions for tensors of different shapes\n",
        "def flatten(x):\n",
        "  return x.view(-1)\n",
        "\n",
        "def make_row_vec(x):\n",
        "  return x.view(1, -1)\n",
        "\n",
        "x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x0_flat = flatten(x0)\n",
        "x0_row = make_row_vec(x0)\n",
        "print('x0:')\n",
        "print(x0)\n",
        "print('x0_flat:')\n",
        "print(x0_flat)\n",
        "print('x0_row:')\n",
        "print(x0_row)\n",
        "\n",
        "x1 = torch.tensor([[1, 2], [3, 4]])\n",
        "x1_flat = flatten(x1)\n",
        "x1_row = make_row_vec(x1)\n",
        "print('\\nx1:')\n",
        "print(x1)\n",
        "print('x1_flat:')\n",
        "print(x1_flat)\n",
        "print('x1_row:')\n",
        "print(x1_row)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "x0_flat:\n",
            "tensor([1, 2, 3, 4, 5, 6])\n",
            "x0_row:\n",
            "tensor([[1, 2, 3, 4, 5, 6]])\n",
            "\n",
            "x1:\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "x1_flat:\n",
            "tensor([1, 2, 3, 4])\n",
            "x1_row:\n",
            "tensor([[1, 2, 3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebT99rUo2McN"
      },
      "source": [
        "# Chunk 2.3.3.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x_flat = x.view(-1)\n",
        "print('x before modifying:')\n",
        "print(x)\n",
        "print('x_flat before modifying:')\n",
        "print(x_flat)\n",
        "\n",
        "x[0, 0] = 10   # x[0, 0] and x_flat[0] point to the same data\n",
        "x_flat[1] = 20 # x_flat[1] and x[0, 1] point to the same data\n",
        "\n",
        "print('\\nx after modifying:')\n",
        "print(x)\n",
        "print('x_flat after modifying:')\n",
        "print(x_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z150qBob4Wkz"
      },
      "source": [
        "### Swapping axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMDxbyBys78"
      },
      "source": [
        "* 부가설명: 예를 들어 shape (2,2,2) tensor를 `.view`를 통하여 shape (4,2) tensor로 변환하면, 다음 순서에 따라 성분들이 재배정됨 : \n",
        "```\n",
        "[0,0,0] 번째 성분 --> [0,0]번째 성분\n",
        "[0,0,1] 번째 성분 --> [0,1]번째 성분\n",
        "[0,1,0] 번째 성분 --> [1,0]번째 성분\n",
        "[0,1,1] 번째 성분 --> [1,1]번째 성분\n",
        "[1,0,0] 번째 성분 --> [2,0]번째 성분\n",
        "...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_B4NuX6zQm-"
      },
      "source": [
        "# Chunk 2.3.4.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print('Original matrix:')\n",
        "print(x)\n",
        "print('\\nTransposing with view DOES NOT WORK!')\n",
        "print(x.view(3, 2))\n",
        "print('\\nTransposed matrix:')\n",
        "print(torch.t(x))\n",
        "print(x.t())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN93xo98zn0v"
      },
      "source": [
        "For tensors with more than two dimensions, we can use the function [`torch.transpose`](https://pytorch.org/docs/1.1.0/torch.html#torch.transpose) to swap arbitrary dimensions, or the [`.permute`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.permute) method to arbitrarily permute dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgN7YB8YzzkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "543c9e9d-5b0d-47b4-95bd-3cedd462bc40"
      },
      "source": [
        "# Chunk 2.3.5.\n",
        "\n",
        "# Create a tensor of shape (2, 3, 4)\n",
        "x0 = torch.tensor([\n",
        "     [[1,  2,  3,  4],\n",
        "      [5,  6,  7,  8],\n",
        "      [9, 10, 11, 12]],\n",
        "     [[13, 14, 15, 16],\n",
        "      [17, 18, 19, 20],\n",
        "      [21, 22, 23, 24]]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Swap axes 1 and 2; shape is (2, 4, 3)\n",
        "# * (0번째 축은 그대로 남김)\n",
        "x1 = x0.transpose(0, 1)\n",
        "# * Mathematically x1[i,j,k] = x0[j,i,k]\n",
        "print('\\nSwap axes 1 and 2:')\n",
        "print(x1)\n",
        "print(x1.shape)\n",
        "\n",
        "# Permute axes; the argument (1, 2, 0) means:\n",
        "# * Mathematically x2[j,k,i] = x0[i,k,j]\n",
        "# - Make the old dimension 1 appear at dimension 0;\n",
        "# - Make the old dimension 2 appear at dimension 1;\n",
        "# - Make the old dimension 0 appear at dimension 2\n",
        "# This results in a tensor of shape (3, 4, 2)\n",
        "x2 = x0.permute(1, 2, 0)\n",
        "print('\\nPermute axes')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [ 5,  6,  7,  8],\n",
            "         [ 9, 10, 11, 12]],\n",
            "\n",
            "        [[13, 14, 15, 16],\n",
            "         [17, 18, 19, 20],\n",
            "         [21, 22, 23, 24]]])\n",
            "shape: torch.Size([2, 3, 4])\n",
            "\n",
            "Swap axes 1 and 2:\n",
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [13, 14, 15, 16]],\n",
            "\n",
            "        [[ 5,  6,  7,  8],\n",
            "         [17, 18, 19, 20]],\n",
            "\n",
            "        [[ 9, 10, 11, 12],\n",
            "         [21, 22, 23, 24]]])\n",
            "torch.Size([3, 2, 4])\n",
            "\n",
            "Permute axes\n",
            "tensor([[[ 1, 13],\n",
            "         [ 2, 14],\n",
            "         [ 3, 15],\n",
            "         [ 4, 16]],\n",
            "\n",
            "        [[ 5, 17],\n",
            "         [ 6, 18],\n",
            "         [ 7, 19],\n",
            "         [ 8, 20]],\n",
            "\n",
            "        [[ 9, 21],\n",
            "         [10, 22],\n",
            "         [11, 23],\n",
            "         [12, 24]]])\n",
            "shape: torch.Size([3, 4, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4SJCVbf-bZ0"
      },
      "source": [
        "### Contiguous tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGC6NERq_CT9"
      },
      "source": [
        "# Chunk 2.3.6.\n",
        "\n",
        "x0 = torch.randn(2, 3, 4)\n",
        "\n",
        "try:\n",
        "  # This sequence of reshape operations will crash\n",
        "  x1 = x0.transpose(1, 2).view(8, 3)\n",
        "except RuntimeError as e:\n",
        "  print(type(e), e)\n",
        "  \n",
        "# We can solve the problem using either .contiguous() or .reshape()\n",
        "x1 = x0.transpose(1, 2).contiguous().view(8, 3)\n",
        "x2 = x0.transpose(1, 2).reshape(8, 3)\n",
        "print('x1 shape: ', x1.shape)\n",
        "print('x2 shape: ', x2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgcdvD1evxTQ"
      },
      "source": [
        "## 2.4. Tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BCVlPHZ4_Qz"
      },
      "source": [
        "### Elementwise operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrMkbk535KRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78e76219-b288-4b22-9a93-bfb4494d2f16"
      },
      "source": [
        "# Chunk 2.4.1.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n",
        "\n",
        "# * Operator overload만 해봅니다.\n",
        "\n",
        "# Elementwise sum; all give the same result\n",
        "print('Elementwise sum:')\n",
        "print(x + y)\n",
        "print(torch.add(x, y))\n",
        "print(x.add(y))\n",
        "\n",
        "# Elementwise difference\n",
        "print('\\nElementwise difference:')\n",
        "print(x - y)\n",
        "print(torch.sub(x, y))\n",
        "print(x.sub(y))\n",
        "\n",
        "# Elementwise product\n",
        "print('\\nElementwise product:')\n",
        "print(x * y)\n",
        "print(torch.mul(x, y))\n",
        "print(x.mul(y))\n",
        "\n",
        "# Elementwise division\n",
        "print('\\nElementwise division')\n",
        "print(x / y)\n",
        "print(torch.div(x, y))\n",
        "print(x.div(y))\n",
        "\n",
        "# Elementwise power\n",
        "print('\\nElementwise power')\n",
        "print(x ** y)\n",
        "print(torch.pow(x, y))\n",
        "print(x.pow(y))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elementwise sum:\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "tensor([[ 6.,  8., 10., 12.]])\n",
            "\n",
            "Elementwise difference:\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "tensor([[-4., -4., -4., -4.]])\n",
            "\n",
            "Elementwise product:\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "tensor([[ 5., 12., 21., 32.]])\n",
            "\n",
            "Elementwise division\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "tensor([[0.2000, 0.3333, 0.4286, 0.5000]])\n",
            "\n",
            "Elementwise power\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n",
            "tensor([[1.0000e+00, 6.4000e+01, 2.1870e+03, 6.5536e+04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s87mjsnG58vR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27653e21-128e-4e14-8238-45b24a4984cf"
      },
      "source": [
        "# Chunk 2.4.2.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "\n",
        "print('Square root:')\n",
        "print(torch.sqrt(x))\n",
        "print(x.sqrt())\n",
        "\n",
        "print('\\nTrig functions:')\n",
        "print(torch.sin(x))\n",
        "print(x.sin())\n",
        "print(torch.cos(x))\n",
        "print(x.cos())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Square root:\n",
            "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
            "tensor([[1.0000, 1.4142, 1.7321, 2.0000]])\n",
            "\n",
            "Trig functions:\n",
            "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
            "tensor([[ 0.8415,  0.9093,  0.1411, -0.7568]])\n",
            "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n",
            "tensor([[ 0.5403, -0.4161, -0.9900, -0.6536]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDyH9USAuyZ-"
      },
      "source": [
        "### Reduction operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlmsYJWUE2r3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc46fc9-3749-4524-a1a9-b3c812113360"
      },
      "source": [
        "# Chunk 2.4.3.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], \n",
        "                  [4, 5, 6]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x)\n",
        "\n",
        "print('\\nSum over entire tensor:')\n",
        "print(torch.sum(x))\n",
        "print(x.sum())\n",
        "\n",
        "# We can sum over each row:\n",
        "# * 0번째 dimension에 대하여 모으기\n",
        "# * y[j] = sum_i x[i,j]\n",
        "print('\\nSum of each row:')\n",
        "y = torch.sum(x, dim=0)\n",
        "print(y)\n",
        "y = x.sum(dim=0)\n",
        "print(y)\n",
        "print(y.shape)\n",
        "\n",
        "# Sum over each column:\n",
        "# * 1번째 dimension에 대하여 모으기\n",
        "# * y[i] = sum_j x[i,j]\n",
        "print('\\nSum of each column:')\n",
        "y = torch.sum(x, dim=1)\n",
        "print(y)\n",
        "y = x.sum(dim=1)\n",
        "print(y)\n",
        "print(y.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "Sum over entire tensor:\n",
            "tensor(21.)\n",
            "tensor(21.)\n",
            "\n",
            "Sum of each row:\n",
            "tensor([5., 7., 9.])\n",
            "tensor([5., 7., 9.])\n",
            "torch.Size([3])\n",
            "\n",
            "Sum of each column:\n",
            "tensor([ 6., 15.])\n",
            "tensor([ 6., 15.])\n",
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFD7aT54H4ik"
      },
      "source": [
        "# Chunk 2.4.4.\n",
        "\n",
        "\n",
        "x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x, x.shape)\n",
        "\n",
        "# Finding the overall minimum only returns a single value\n",
        "print('\\nOverall minimum: ', x.min())\n",
        "\n",
        "# Compute the minimum along each column; we get both the value and location:\n",
        "# The minimum of the first column is 2, and it appears at index 0;\n",
        "# the minimum of the second column is 3 and it appears at index 1; etc\n",
        "col_min_vals, col_min_idxs = x.min(dim=0)\n",
        "# * col_min_vals[j] = min_i x[i,j].\n",
        "# * col_min_vals는 각 column마다 최솟값을 저장하고 있음.\n",
        "\n",
        "print('\\nMinimum along each column:')\n",
        "print('values:', col_min_vals)\n",
        "print('idxs:', col_min_idxs)\n",
        "\n",
        "# Compute the minimum along each row; we get both the value and the minimum\n",
        "row_min_vals, row_min_idxs = x.min(dim=1)\n",
        "print('\\nMinimum along each row:')\n",
        "print('values:', row_min_vals)\n",
        "print('idxs:', row_min_idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjcAveyJFqm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba675a36-810f-40ee-c043-8a61e334ebe8"
      },
      "source": [
        "# Chunk 2.4.5.\n",
        "\n",
        "# Create a tensor of shape (128, 10, 3, 64, 64)\n",
        "x = torch.randn(128, 10, 3, 64, 64)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1; shape is now (128, 3, 64, 64)\n",
        "x = x.mean(dim=1)\n",
        "# x[i,k,l,m] <- (1/10) * sum_j x[i,j,k,l,m]\n",
        "print(x.shape)\n",
        "\n",
        "# Take the sum over dimension 2; shape is now (128, 3, 64)\n",
        "x = x.sum(dim=2)\n",
        "# x[i,k,m] <- sum_l x[i,k,l,m]\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1, but keep the dimension from being eliminated\n",
        "# by passing keepdim=True; shape is now (128, 1, 64)\n",
        "x = x.mean(dim=1, keepdim=True)\n",
        "# x[i,0,m] <- sum_k x[i,k,m]\n",
        "print(x.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 10, 3, 64, 64])\n",
            "torch.Size([128, 3, 64, 64])\n",
            "torch.Size([128, 3, 64])\n",
            "torch.Size([128, 1, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyLyXU2u29N"
      },
      "source": [
        "### Matrix operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwjbapG6MM_"
      },
      "source": [
        "Note that unlike MATLAB, __* is elementwise multiplication, not matrix multiplication__. (주. R과 동일) PyTorch provides a number of linear algebra functions that compute different types of vector and matrix products. The most commonly used are:\n",
        "\n",
        "- [`torch.dot`](https://pytorch.org/docs/1.1.0/torch.html#blas-and-lapack-operations): Computes inner product of vectors\n",
        "- [`torch.mm`](https://pytorch.org/docs/1.1.0/torch.html#torch.mm): Computes matrix-matrix products\n",
        "- [`torch.mv`](https://pytorch.org/docs/1.1.0/torch.html#torch.mv): Computes matrix-vector products\n",
        "- [`torch.addmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmm) / [`torch.addmv`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmv): Computes matrix-matrix and matrix-vector multiplications plus a bias\n",
        "- [`torch.bmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.addmv) / [`torch.baddmm`](https://pytorch.org/docs/1.1.0/torch.html#torch.baddbmm): Batched versions of `torch.mm` and `torch.addmm`, respectively\n",
        "- [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul): General matrix product that performs different operations depending on the rank of the inputs; this is similar to `np.dot` in numpy.\n",
        "\n",
        "You can find a full list of the available linear algebra operators [in the documentation](https://pytorch.org/docs/1.1.0/torch.html#blas-and-lapack-operations).\n",
        "\n",
        "Here is an example of using `torch.dot` to compute inner products. Like the other mathematical operators we've seen, most linear algebra operators are available both as functions in the `torch` module (예. `torch.dot(x,y)`) and as instance methods of tensors (예. `x.dot(y)`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRUYW2as6ZCh"
      },
      "source": [
        "# Chunk 2.4.8.\n",
        "\n",
        "v = torch.tensor([9,10], dtype=torch.float32)\n",
        "w = torch.tensor([11, 12], dtype=torch.float32)\n",
        "\n",
        "# Inner product of vectors\n",
        "print('Dot products:')\n",
        "print(torch.dot(v, w))\n",
        "print(v.dot(w))\n",
        "\n",
        "# dot only works for vectors -- it will give an error for tensors of rank > 1\n",
        "x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n",
        "try:\n",
        "  print(x.dot(y))\n",
        "except RuntimeError as e:\n",
        "  print(e)\n",
        "  \n",
        "# Instead we use mm for matrix-matrix products:\n",
        "print('\\nMatrix-matrix product:')\n",
        "print(torch.mm(x, y))\n",
        "print(x.mm(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRxK34KdHm3"
      },
      "source": [
        "알고리즘 개발시 흔한 오류 중 하나가 rank/dimension/shape 안 맞는 행렬-벡터 간의 연산에서 발생합니다. 아래 결과가 각각 rank가 다름을 주지하시고, 코드 개발할 때도 수시로 shape와 rank를 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqEzcnHkdRYA"
      },
      "source": [
        "# Chunk 2.4.9.\n",
        "\n",
        "print('Here is x (rank 2):')\n",
        "print(x)\n",
        "print('\\nHere is v (rank 1):')\n",
        "print(v)\n",
        "\n",
        "# Matrix-vector multiply with torch.mv produces a rank-1 output\n",
        "print('\\nMatrix-vector product with torch.mv (rank 1 output)')\n",
        "print(torch.mv(x, v))\n",
        "print(x.mv(v))\n",
        "print(x.mv(v).shape)\n",
        "\n",
        "# We can reshape the vector to have rank 2 and use torch.mm to perform\n",
        "# matrix-vector products, but the result will have rank 2\n",
        "print('\\nMatrix-vector product with torch.mm (rank 2 output)')\n",
        "print(torch.mm(x, v.view(2, 1)))\n",
        "print(x.mm(v.view(2, 1)))\n",
        "print(x.mm(v.view(2, 1)).shape)\n",
        "\n",
        "print('\\nMatrix-vector product with torch.matmul (rank 1 output)')\n",
        "print(torch.matmul(x, v))\n",
        "print(x.matmul(v))\n",
        "print(x.matmul(v).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograd"
      ],
      "metadata": {
        "id": "2J2g2C4A6gza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `requires_grad=True`로 텐서를 생성하면 autograd가 활성화된다.  \n",
        "    - 모든 연산 추적  \n",
        "- `requires_grad=True`가 있는 텐서에 대한 연산(operation)은 pytorch가 계산 그래프(computational graph)를 빌드한다.  \n",
        "- `torch.randn(requires_grad=True)` : autograd가 반환된 텐서의 연산을 기록해야 하는 경우 쓰인다.\n",
        "- `torch.Tensor.backward` : 현재 w,r,t 그래프가 남아있는 텐서의 그레이디언트를 계산한다.\n",
        "    - 텐서에 대한 그레이디언트는 .grad 속성에 누적된다.\n",
        "- `torch.autograd.grad` : 입력에 대한 출력의 그레이디언트 합계를 계산하고 반환한다.\n",
        "- `torch.no_grad` : 이 부분에서는 계산 그래프를 만들지 말라는 의미이다.\n",
        "- `grad.zero_` : _로 끝나는 파이토치 메서드는 텐서를 제자리에서 수정하고, 새로운 텐서를 반환하지 않는 메서드이다."
      ],
      "metadata": {
        "id": "fTugNa8GH1K8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAwlqcGQ6lcR",
        "outputId": "703dfaff-38fe-4a20-d550-1f9aa2bfbfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "print(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbWP2eE8QzTZ",
        "outputId": "42022fd3-ce28-467b-c4ad-2f87f9da201e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UISn2pcf9QjY"
      },
      "source": [
        "## 2.5. Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTj6f8VN9UZg"
      },
      "source": [
        "예. `torch.tensor([1,2,2]) + 1`은 원칙적으로는 벡터와 스칼라의 덧셈이므로, 정의되지 않는 계산입니다. 그렇지만 Broadcasting 덕분에 잘 작동합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF0Dhzlu9fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5391248c-c4bf-4aa4-8806-4c918e109f59"
      },
      "source": [
        "# Chunk 2.5.1.\n",
        "\n",
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "print(x.shape)\n",
        "print(v.shape)\n",
        "y = torch.zeros_like(x)   # Create an empty matrix with the same shape as x\n",
        "\n",
        "# Add the vector v to each row of the matrix x with an explicit loop\n",
        "for i in range(4):\n",
        "    y[i, :] = x[i, :] + v\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 3])\n",
            "torch.Size([3])\n",
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2_5cKeu94c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4c45bd-3f8a-4490-9677-688b3750f642"
      },
      "source": [
        "# Chunk 2.5.2.\n",
        "\n",
        "vv = v.repeat((4, 1))  # Stack 4 copies of v on top of each other\n",
        "print(vv)              # Prints \"[[1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]]\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1],\n",
            "        [1, 0, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KiRj23p-QIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785d51e8-041c-48d8-b544-293171be75c6"
      },
      "source": [
        "y = x + vv  # Add x and vv elementwise\n",
        "print(y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jIiZc-ABBnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad5b0a2-62c9-46e1-8dc9-2752fc9a7370"
      },
      "source": [
        "# Chunk 2.5.3.\n",
        "\n",
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]]) #(4,3)\n",
        "v = torch.tensor([1, 0, 1]) #(3,)\n",
        "y = x + v  # Add v to each row of x using broadcasting\n",
        "y2 = x + v.reshape(1,3) #(1,3)\n",
        "print(y)\n",
        "print(y2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n",
            "tensor([[ 2,  2,  4],\n",
            "        [ 5,  5,  7],\n",
            "        [ 8,  8, 10],\n",
            "        [11, 11, 13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuUBX8YnBSIG"
      },
      "source": [
        "\n",
        "\n",
        "주. Broadcasting을 실수 없이 사용하고 싶으면, 먼저 연산 대상의 두 텐서를 compatible하게 만든 뒤 연산하길 합니다. 예를 들어, 아래 3 by 4 tensor (`x`)와 3 by 5 by 4 tensor (`y`)를 더하여 `z`를 만들되 `z[i,j,k] = x[i,k] + y[i,j,k]`를 만족하고 싶다면?  \n",
        "```\n",
        "x = torch.randn(3,4)\n",
        "y = torch.randn(3,5,4)\n",
        "x + y             # 에러\n",
        "x.view(3,1,4) + y # ok\n",
        "``` \n",
        "\n",
        "Broadcasting can let us easily implement many different operations. For example we can compute an outer product of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W-k7-hpCwlT"
      },
      "source": [
        "# Chunk 2.5.4.\n",
        "\n",
        "# Compute outer product of vectors\n",
        "v = torch.tensor([1, 2, 3])  # v has shape (3,)\n",
        "w = torch.tensor([4, 5])     # w has shape (2,)\n",
        "print(v.view(3, 1) * w.view(1, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bhmBiwcDF1B"
      },
      "source": [
        "# Chunk 2.5.5.\n",
        "\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "v = torch.tensor([1, 2, 3])               # v has shape (3,)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(v)\n",
        "\n",
        "# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n",
        "# giving the following matrix:\n",
        "print('\\nAdd the vector to each row of the matrix:')\n",
        "print(x + v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN6FfqU9wFeG"
      },
      "source": [
        "## 2.6. Running on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds6SDTbrwOc1"
      },
      "source": [
        "One of the most important features of PyTorch is that it can use graphics processing units (GPUs) to accelerate its tensor operations.\n",
        "\n",
        "We can easily check whether PyTorch is configured to use GPUs:\n",
        "\n",
        "Tensors can be moved onto any device using the .to method."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPU : 병렬 처리 방식, 여러 개 동시에 -> 쉬운 문제를 동시에 푸는데 특화\n",
        "\n",
        "- CPU : 직렬 처리 방식, 하나씩 -> 어려운 문제를 푸는데 특화 "
      ],
      "metadata": {
        "id": "BtQLX0FaFIVc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RkoFEVVKWlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c8b7b66-7a63-4de1-c247-06b26c77ece1"
      },
      "source": [
        "# Chunk 2.6.1.\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch can use GPUs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i_5n_XuKr5k"
      },
      "source": [
        "CUDA: GPU에서의 연산을 산업표준 언어(이를테면 C)로 구현하도록 돕는 기술 중 하나. CUDA는 엔비디아가 개발해오고 있으며, CUDA를 사용하려면 엔비디아의 GPU 하드웨어가 필요하다. [위키백과](https://ko.wikipedia.org/wiki/CUDA))\n",
        "\n",
        "\n",
        "(런타임 -> 런타임 유형 변경 -> 하드웨어 가속기 -> GPU)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D03s614dMCvy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1f6e9f-081f-4ac2-f00d-7f797c21c1b6"
      },
      "source": [
        "# Chunk 2.6.2.\n",
        "\n",
        "# Construct a tensor on the CPU\n",
        "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print('x0 device:', x0.device)\n",
        "\n",
        "# Move it to the GPU using .to()\n",
        "x1 = x0.to('cuda')\n",
        "print('x1 device:', x1.device)\n",
        "\n",
        "# Move it to the GPU using .cuda()\n",
        "x2 = x0.cuda()\n",
        "print('x2 device:', x2.device)\n",
        "\n",
        "# Move it back to the CPU using .to()\n",
        "x3 = x1.to('cpu')\n",
        "print('x3 device:', x3.device)\n",
        "\n",
        "# Move it back to the CPU using .cpu()\n",
        "x4 = x2.cpu()\n",
        "print('x4 device:', x4.device)\n",
        "\n",
        "# We can construct tensors directly on the GPU as well\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
        "print('y device / dtype:', y.device, y.dtype)\n",
        "\n",
        "# Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
        "# device and dtype as y\n",
        "x5 = x0.to(y)\n",
        "print('x5 device / dtype:', x5.device, x5.dtype)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x0 device: cpu\n",
            "x1 device: cuda:0\n",
            "x2 device: cuda:0\n",
            "x3 device: cpu\n",
            "x4 device: cpu\n",
            "y device / dtype: cuda:0 torch.float64\n",
            "x5 device / dtype: cuda:0 torch.float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW14ZF-_PK7t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232d23e6-d0a6-4161-befc-da5831853b45"
      },
      "source": [
        "# Chunk 2.6.3.\n",
        "\n",
        "import time\n",
        "\n",
        "a_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "b_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "\n",
        "a_gpu = a_cpu.cuda()\n",
        "b_gpu = b_cpu.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "t0 = time.time()\n",
        "c_cpu = a_cpu + b_cpu\n",
        "t1 = time.time()\n",
        "c_gpu = a_gpu + b_gpu\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "# Check that they computed the same thing\n",
        "diff = (c_gpu.cpu() - c_cpu).abs().max().item()\n",
        "print('Max difference between c_gpu and c_cpu:', diff)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max difference between c_gpu and c_cpu: 0.0\n",
            "CPU time: 215.92 ms\n",
            "GPU time: 11.96 ms\n",
            "GPU speedup: 18.05 x\n"
          ]
        }
      ]
    }
  ]
}