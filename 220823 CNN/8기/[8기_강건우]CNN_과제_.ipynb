{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[8기 강건우]CNN_과제.",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XNHYwgkCwcC-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNXLvm82ynQV",
        "outputId": "1f2865ac-a91c-4592-abe8-eae70f8ed71f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#압축풀기\n",
        "%cd /content/drive/MyDrive/Colab Notebooks\n",
        "!unzip -qq \"/content/drive/MyDrive/Colab Notebooks/animals10.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp1EDvk_y6H0",
        "outputId": "59813d85-c8c5-4ff8-ef72-a84d070703dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "replace Animals-10/butterfly/butterfly (1).jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A,n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "butterfly1 = sorted(glob.glob('./Animals-10/butterfly/*.jpg'))\n",
        "butterfly2 = sorted(glob.glob('./Animals-10/butterfly/*.jpeg'))\n",
        "butterfly3 = sorted(glob.glob('./Animals-10/butterfly/*.png'))\n",
        "Bu=butterfly1+butterfly2+butterfly3\n",
        "\n",
        "cat1= sorted(glob.glob('./Animals-10/cat/*.jpg'))\n",
        "cat2= sorted(glob.glob('./Animals-10/cat/*.jpeg'))\n",
        "cat3= sorted(glob.glob('./Animals-10/cat/*.png'))\n",
        "Ca=cat1+cat2+cat3\n",
        "\n",
        "chicken1= sorted(glob.glob('./Animals-10/chicken/*.jpg'))\n",
        "chicken2= sorted(glob.glob('./Animals-10/chicken/*.jpeg'))\n",
        "chicken3= sorted(glob.glob('./Animals-10/chicken/*.png'))\n",
        "Chi=chicken1+chicken2+chicken3\n",
        "\n",
        "cow1= sorted(glob.glob('./Animals-10/cow/*.jpg'))\n",
        "cow2= sorted(glob.glob('./Animals-10/cow/*.jpeg'))\n",
        "cow3= sorted(glob.glob('./Animals-10/cow/*.png'))\n",
        "Co=cow1+cow2+cow3\n",
        "\n",
        "dog1= sorted(glob.glob('./Animals-10/dog/*.jpg'))\n",
        "dog2= sorted(glob.glob('./Animals-10/dog/*.jpeg'))\n",
        "dog3= sorted(glob.glob('./Animals-10/dog/*.png'))\n",
        "Do=dog1+dog2+dog3\n",
        "\n",
        "elephant1= sorted(glob.glob('./Animals-10/elephant/*.jpg'))\n",
        "elephant2= sorted(glob.glob('./Animals-10/elephant/*.jpeg'))\n",
        "elephant3= sorted(glob.glob('./Animals-10/elephant/*.png'))\n",
        "El=elephant1+elephant2+elephant3\n",
        "\n",
        "horse1= sorted(glob.glob('./Animals-10/horse/*.jpg'))\n",
        "horse2= sorted(glob.glob('./Animals-10/horse/*.jpeg'))\n",
        "horse3= sorted(glob.glob('./Animals-10/horse/*.png'))\n",
        "Ho=horse1+horse2+horse3\n",
        "\n",
        "sheep1= sorted(glob.glob('./Animals-10/sheep/*.jpg'))\n",
        "sheep2= sorted(glob.glob('./Animals-10/sheep/*.jpeg'))\n",
        "sheep3= sorted(glob.glob('./Animals-10/sheep/*.png'))\n",
        "Sh=sheep1+sheep2+sheep3\n",
        "\n",
        "spider1= sorted(glob.glob('./Animals-10/spider/*.jpg'))\n",
        "spider2= sorted(glob.glob('./Animals-10/spider/*.jpeg'))\n",
        "spider3= sorted(glob.glob('./Animals-10/spider/*.png'))\n",
        "Sp=spider1+spider2+spider3\n",
        "\n",
        "squirrel1= sorted(glob.glob('./Animals-10/squirrel/*.jpg'))\n",
        "squirrel2= sorted(glob.glob('./Animals-10/squirrel/*.jpeg'))\n",
        "squirrel3= sorted(glob.glob('./Animals-10/squirrel/*.png'))\n",
        "Sq=squirrel1+squirrel2+squirrel3\n",
        "\n",
        "print(len(Bu))\n",
        "print(len(Ca))\n",
        "print(len(Chi))\n",
        "print(len(Co))\n",
        "print(len(Do))\n",
        "print(len(El))\n",
        "print(len(Ho))\n",
        "print(len(Sh))\n",
        "print(len(Sp))\n",
        "print(len(Sq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn0e1fDU3gHc",
        "outputId": "bb8ccc58-c48a-4d71-a27e-192d9e68c55a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2130\n",
            "1668\n",
            "3098\n",
            "1866\n",
            "4863\n",
            "1446\n",
            "2623\n",
            "1820\n",
            "4821\n",
            "1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Bu_test_count = round(len(Bu)*0.3)\n",
        "Ca_test_count = round(len(Ca)*0.3)\n",
        "Chi_test_count = round(len(Chi)*0.3)\n",
        "Co_test_count = round(len(Co)*0.3)\n",
        "Do_test_count = round(len(Do)*0.3)\n",
        "El_test_count = round(len(El)*0.3)\n",
        "Ho_test_count = round(len(Ho)*0.3)\n",
        "Sh_test_count = round(len(Sh)*0.3)\n",
        "Sp_test_count = round(len(Sp)*0.3)\n",
        "Sq_test_count = round(len(Sq)*0.3)\n",
        "\n",
        "print(Bu_test_count)\n",
        "print(Ca_test_count)\n",
        "print(Chi_test_count)\n",
        "print(Co_test_count)\n",
        "print(Do_test_count)\n",
        "print(El_test_count)\n",
        "print(Ho_test_count)\n",
        "print(Sh_test_count)\n",
        "print(Sp_test_count)\n",
        "print(Sq_test_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGm7y4UM_jDY",
        "outputId": "4bb5589a-ec02-404e-a721-a008a594156d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "639\n",
            "500\n",
            "929\n",
            "560\n",
            "1459\n",
            "434\n",
            "787\n",
            "546\n",
            "1446\n",
            "559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def split(img_list, test_count, train_path, test_path):\n",
        "  test_files=[]\n",
        "  for i in random.sample(img_list, test_count):\n",
        "    test_files.append(i)\n",
        "  train_files = [x for x in img_list if x not in test_files]\n",
        "  for k in train_files:\n",
        "    shutil.copy(k, train_path)\n",
        "  for c in test_files:\n",
        "    shutil.copy(c, test_path)\n",
        "  print('train 폴더 이미지 개수 : {}\\ntest 폴더 이미지 개수 : {}'.format(len(glob.glob(train_path+'/*')),len(glob.glob(test_path+'/*'))))"
      ],
      "metadata": {
        "id": "GcKFlkxOIy4q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Bu_train_path='./train/Bu'\n",
        "Bu_test_path='./test/Bu'\n",
        "os.makedirs(Bu_train_path, exist_ok=True)\n",
        "os.makedirs(Bu_test_path, exist_ok=True)\n",
        "\n",
        "Ca_train_path='./train/Ca'\n",
        "Ca_test_path='./test/Ca'\n",
        "os.makedirs(Ca_train_path, exist_ok=True)\n",
        "os.makedirs(Ca_test_path, exist_ok=True)\n",
        "\n",
        "Chi_train_path='./train/Chi'\n",
        "Chi_test_path='./test/Chi'\n",
        "os.makedirs(Chi_train_path, exist_ok=True)\n",
        "os.makedirs(Chi_test_path, exist_ok=True)\n",
        "\n",
        "Co_train_path='./train/Co'\n",
        "Co_test_path='./test/Co'\n",
        "os.makedirs(Co_train_path, exist_ok=True)\n",
        "os.makedirs(Co_test_path, exist_ok=True)\n",
        "\n",
        "Do_train_path='./train/Do'\n",
        "Do_test_path='./test/Do'\n",
        "os.makedirs(Do_train_path, exist_ok=True)\n",
        "os.makedirs(Do_test_path, exist_ok=True)\n",
        "\n",
        "El_train_path='./train/El'\n",
        "El_test_path='./test/El'\n",
        "os.makedirs(El_train_path, exist_ok=True)\n",
        "os.makedirs(El_test_path, exist_ok=True)\n",
        "\n",
        "Ho_train_path='./train/Ho'\n",
        "Ho_test_path='./test/Ho'\n",
        "os.makedirs(Ho_train_path, exist_ok=True)\n",
        "os.makedirs(Ho_test_path, exist_ok=True)\n",
        "\n",
        "Sh_train_path='./train/Sh'\n",
        "Sh_test_path='./test/Sh'\n",
        "os.makedirs(Sh_train_path, exist_ok=True)\n",
        "os.makedirs(Sh_test_path, exist_ok=True)\n",
        "\n",
        "Sp_train_path='./train/Sp'\n",
        "Sp_test_path='./test/Sp'\n",
        "os.makedirs(Sp_train_path, exist_ok=True)\n",
        "os.makedirs(Sp_test_path, exist_ok=True)\n",
        "\n",
        "Sq_train_path='./train/Sq'\n",
        "Sq_test_path='./test/Sq'\n",
        "os.makedirs(Sq_train_path, exist_ok=True)\n",
        "os.makedirs(Sq_test_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "jbObywUrRS1r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split(Bu, Bu_test_count, Bu_train_path, Bu_test_path)\n",
        "split(Ca, Ca_test_count, Ca_train_path, Ca_test_path)\n",
        "split(Chi, Chi_test_count, Chi_train_path, Chi_test_path)\n",
        "split(Co, Co_test_count, Co_train_path, Co_test_path)\n",
        "split(Do, Do_test_count, Do_train_path, Do_test_path)\n",
        "split(El, El_test_count, El_train_path, El_test_path)\n",
        "split(Ho, Ho_test_count, Ho_train_path, Ho_test_path)\n",
        "split(Sh, Sh_test_count, Sh_train_path, Sh_test_path)\n",
        "split(Sp, Sp_test_count, Sp_train_path, Sp_test_path)\n",
        "split(Sq, Sq_test_count, Sq_train_path, Sq_test_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "RUzE3ZH1Rk4C",
        "outputId": "1cbe52de-d415-4f18-e0c5-58e54667e07d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1017a219c2d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBu_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBu_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBu_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCa_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCa_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCa_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChi_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChi_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChi_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCo_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCo_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCo_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDo_test_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDo_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDo_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-6d1409f92bd2>\u001b[0m in \u001b[0;36msplit\u001b[0;34m(img_list, test_count, train_path, test_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtrain_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mSameFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{!r} and {!r} are the same file\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36m_samefile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'samefile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36msamefile\u001b[0;34m(f1, f2)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \"\"\"\n\u001b[1;32m    100\u001b[0m     \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msamestat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "upOnt9y6Y8wQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_animals_train = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(256),\n",
        "    transforms.ColorJitter(brightness=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "iwnqKFLMeBPV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_animals_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "khxVr1qkeMAL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder(\n",
        "    root = './train',                      # 불러들인 데이터가 저장되는 디렉터리 위치\n",
        "                             \n",
        "    transform = transform_animals_train,    # 적용할 transform function\n",
        "    \n",
        ")"
      ],
      "metadata": {
        "id": "1BneHZnusVka"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -R test/.ipynb_checkpoints\n",
        "!ls test/train/ -a\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoxOkGthwH8o",
        "outputId": "f3d195b8-22d8-4c72-e1fb-2572ec2bc2f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'test/.ipynb_checkpoints': No such file or directory\n",
            "ls: cannot access 'test/train/': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = torchvision.datasets.ImageFolder(\n",
        "    root = './test',\n",
        "    \n",
        "    transform = transform_animals_test, )\n",
        "    "
      ],
      "metadata": {
        "id": "aBiUMstdwgDo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbfRmPrRw1rc",
        "outputId": "6c924d37-0f5b-4ed4-f2e3-495c9939610c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 36159\n",
              "    Root location: ./train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
              "               CenterCrop(size=(256, 256))\n",
              "               ColorJitter(brightness=[0.5, 1.5], contrast=None, saturation=None, hue=None)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = 32,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=True,         # train dataloader는 epoch마다 데이터 다시 섞어서 batch 만들고 학습! - 학습 효율 up\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "9MfWKKU2w8FD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = 32,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=False,        # test dataloader는 데이터 매번 섞을 이유가 없음 - 어차피 확인 용도\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "bEo7N61QxPKe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "yBadhpi_4goR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),  # [batch_size,3,256,256] -> [batch_size,16,256,256] \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1), # [batch_size,16,256,256] -> [batch_size,16,256,256]\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2), # 0.2 확률로 Dropout\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                # [batch_size,16,256,256] -> [batch_size,16,128,128]\n",
        "\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1), # [batch_size,16,128,128] -> [batch_size,32,128,128] \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1), # [batch_size,32,128,128] -> [batch_size,32,128,128] \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2), # 0.2 확률로 Dropout\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                # [batch_size,32,128,128] -> [batch_size,32,64,64] \n",
        "\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), # [batch_size,32,64,64]  -> [batch_size,64,64,64] \n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1), # [batch_size,64,64,64]  -> [batch_size,64,64,64] \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                 # [batch_size,64,64,64] -> [batch_size,64,32,32]\n",
        "\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1), # [batch_size,64,32,32] -> [batch_size,128,32,32]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), # [batch_size,128,32,32] -> [batch_size,128,32,32]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1), # [batch_size,128,32,32] -> [batch_size,128,32,32]\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                 # [batch_size,128,32,32] -> [batch_size,128,16,16]\n",
        "\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1), # [batch_size,128,16,16] -> [batch_size,256,16,16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), # [batch_size,256,16,16] -> [batch_size,256,16,16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), # [batch_size,256,16,16] -> [batch_size,256,16,16]\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1), # [batch_size,256,16,16] -> [batch_size,256,16,16]\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)                                  #[batch_size,256,16,16]-> [batch_size,256,8,8]\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(256*8*8,1000), \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000,100),                                               \n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # forward pass 과정\n",
        "        out = self.layer(x)\n",
        "        out = out.view(out.size(0),-1)  # FC layer에 들어가기전, flatten! - view 함수 활용\n",
        "                                        # 가장 첫 차원인 batch_size는 유지하고 나머지 차원들을 하나로 합치기 - [batch_size,16,5,5] -> [batch_size,16*5*5]\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "OUmqjSaW5Dr6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCo8P-W5EGYe",
        "outputId": "4a478f1e-07ab-4d4b-da13-f99c216ae668"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "test_model = MyModel() \n",
        "summary(test_model, (3,256,256), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZfGRUuEJmL",
        "outputId": "b360a8d1-a858-4452-bee0-1620aab2e204"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [32, 16, 256, 256]             448\n",
            "              ReLU-2         [32, 16, 256, 256]               0\n",
            "            Conv2d-3         [32, 16, 256, 256]           2,320\n",
            "              ReLU-4         [32, 16, 256, 256]               0\n",
            "           Dropout-5         [32, 16, 256, 256]               0\n",
            "         MaxPool2d-6         [32, 16, 128, 128]               0\n",
            "            Conv2d-7         [32, 32, 128, 128]           4,640\n",
            "              ReLU-8         [32, 32, 128, 128]               0\n",
            "            Conv2d-9         [32, 32, 128, 128]           9,248\n",
            "             ReLU-10         [32, 32, 128, 128]               0\n",
            "          Dropout-11         [32, 32, 128, 128]               0\n",
            "        MaxPool2d-12           [32, 32, 64, 64]               0\n",
            "           Conv2d-13           [32, 64, 64, 64]          18,496\n",
            "             ReLU-14           [32, 64, 64, 64]               0\n",
            "           Conv2d-15           [32, 64, 64, 64]          36,928\n",
            "             ReLU-16           [32, 64, 64, 64]               0\n",
            "          Dropout-17           [32, 64, 64, 64]               0\n",
            "        MaxPool2d-18           [32, 64, 32, 32]               0\n",
            "           Conv2d-19          [32, 128, 32, 32]          73,856\n",
            "             ReLU-20          [32, 128, 32, 32]               0\n",
            "           Conv2d-21          [32, 128, 32, 32]         147,584\n",
            "             ReLU-22          [32, 128, 32, 32]               0\n",
            "           Conv2d-23          [32, 128, 32, 32]         147,584\n",
            "             ReLU-24          [32, 128, 32, 32]               0\n",
            "          Dropout-25          [32, 128, 32, 32]               0\n",
            "        MaxPool2d-26          [32, 128, 16, 16]               0\n",
            "           Conv2d-27          [32, 256, 16, 16]         295,168\n",
            "             ReLU-28          [32, 256, 16, 16]               0\n",
            "           Conv2d-29          [32, 256, 16, 16]         590,080\n",
            "             ReLU-30          [32, 256, 16, 16]               0\n",
            "           Conv2d-31          [32, 256, 16, 16]         590,080\n",
            "             ReLU-32          [32, 256, 16, 16]               0\n",
            "           Conv2d-33          [32, 256, 16, 16]         590,080\n",
            "             ReLU-34          [32, 256, 16, 16]               0\n",
            "          Dropout-35          [32, 256, 16, 16]               0\n",
            "        MaxPool2d-36            [32, 256, 8, 8]               0\n",
            "           Linear-37                 [32, 1000]      16,385,000\n",
            "             ReLU-38                 [32, 1000]               0\n",
            "           Linear-39                  [32, 100]         100,100\n",
            "             ReLU-40                  [32, 100]               0\n",
            "           Linear-41                   [32, 10]           1,010\n",
            "================================================================\n",
            "Total params: 18,992,622\n",
            "Trainable params: 18,992,622\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 24.00\n",
            "Forward/backward pass size (MB): 2732.54\n",
            "Params size (MB): 72.45\n",
            "Estimated Total Size (MB): 2828.99\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBpuNtMJENuq",
        "outputId": "73dbbd0f-825d-4753-fe8b-7e2d1e9e6cfa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = MyModel().to(device)\n",
        "mymodel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kr3sACgERGJ",
        "outputId": "42a1b0f6-71f6-44e4-ab63-e9a87a48e36a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (layer): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU()\n",
              "    (10): Dropout(p=0.2, inplace=False)\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU()\n",
              "    (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU()\n",
              "    (16): Dropout(p=0.2, inplace=False)\n",
              "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (18): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): ReLU()\n",
              "    (20): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): ReLU()\n",
              "    (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): ReLU()\n",
              "    (24): Dropout(p=0.2, inplace=False)\n",
              "    (25): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (26): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU()\n",
              "    (28): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU()\n",
              "    (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU()\n",
              "    (32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU()\n",
              "    (34): Dropout(p=0.2, inplace=False)\n",
              "    (35): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layer): Sequential(\n",
              "    (0): Linear(in_features=16384, out_features=1000, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=1000, out_features=100, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device) \n",
        "optimizer = torch.optim.Adam(mymodel.parameters(), lr=0.001, weight_decay=0.0001)"
      ],
      "metadata": {
        "id": "C0FpBj_aEU_e"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        print(inputs.shape)  # batch_size개의 이미지\n",
        "        print(targets.shape) # batch_size개의 클래스\n",
        "\n",
        "        outputs = mymodel(inputs)\n",
        "        print(outputs.shape) # outputs는 32, 10 차원 (32개 data의 확률 10개씩)\n",
        "        print(targets.shape) # targets는 32차원 (32개 data의 실제 클래스)\n",
        "\n",
        "        loss = criterion(outputs, targets)  # -> cross entropy 식에 넣을때 이런 형태로 넣으면됨 (outputs, targets의 차원을 맞출 필요가 없음)\n",
        "        print(loss.item())\n",
        "\n",
        "        print(outputs.max(1)[1]) # 예측한 클래스\n",
        "        print(targets) # 실제 클래스\n",
        "        correct = (outputs.max(1)[1] == targets).sum().item() # 맞게 예측한 클래스 개수\n",
        "        print(correct)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPmQRzj5EYBN",
        "outputId": "a6c88b47-f926-4b83-ed28-4f951cecc2d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 256, 256])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 10])\n",
            "torch.Size([32])\n",
            "2.302565336227417\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor([5, 0, 2, 6, 0, 0, 0, 0, 4, 0, 0, 0, 5, 0, 7, 0, 0, 0, 3, 0, 2, 2, 0, 1,\n",
            "        0, 2, 0, 0, 9, 6, 4, 0])\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train_loss = []\n",
        "seq_test_loss = []\n",
        "seq_train_acc = []\n",
        "seq_test_acc = []"
      ],
      "metadata": {
        "id": "b1tvS4mfEisr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    print(f'\\n[ Train epoch: {epoch+1} ]')\n",
        "\n",
        "    mymodel.train() # train은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하고 진행하기 위함\n",
        "\n",
        "    running_loss = 0.0\n",
        "    batch_losses = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # dataloader의 input image와 label도 device에 넣어줘야됨\n",
        "        \n",
        "        # DL 학습 기본 코드\n",
        "        optimizer.zero_grad() # gradient 초기화\n",
        "        outputs = mymodel(inputs) # 현재 batch의 inputs을 모델에 넣어 outputs 추출 (확률값)\n",
        "        loss = criterion(outputs, targets) # 추출한 outputs와 원래 label인 targets 사이 loss 계산\n",
        "        loss.backward() # 계산한 loss 기반으로 gradient 값 계산\n",
        "        optimizer.step() # weight parameter update\n",
        "\n",
        "        total += targets.size(0) # batch 데이터 개수 더하기\n",
        "        running_loss += loss.item()\n",
        "        batch_losses.append(loss.item())\n",
        "        \n",
        "        _, predicted = outputs.max(1) # 확률값 가장 높게 나타난 클래스\n",
        "        correct += (predicted == targets).sum().item() # 현재 batch 내에서 알맞게 분류한 이미지 개수 더하기\n",
        "        \n",
        "        if batch_idx % 300 == 299:\n",
        "            print(f'\\nCurrent batch: {str(batch_idx+1)}')\n",
        "            print(f'Average train loss of recent 300 batches: {running_loss / 300}') # 이렇게 출력하는 것이 꼭 필요한 것은 아니지만, 중간중간 확인을 위해 매우 권장\n",
        "            running_loss = 0.0\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_train_loss.append(avg_loss)\n",
        "    seq_train_acc.append(100*correct/total)\n",
        "    print('\\nTotal train accuarcy:', 100. * correct / total) # 전체 데이터 개수에서 맞게 예측한 비율\n",
        "    print('Total train loss:', avg_loss)"
      ],
      "metadata": {
        "id": "g73jkkWiEna5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(epoch):\n",
        "    print(f'\\n[ Test epoch: {epoch+1} ]')\n",
        "\n",
        "    mymodel.eval() # eval은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하지 않기 위함!\n",
        "                   # ex. evaluation 할때는 Dropout 없이 지금까지 학습한 모든 node를 활용해서 진행해야됨\n",
        "\n",
        "    loss = 0\n",
        "    batch_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # gradient update 안함 - eval과 torch.no_grad는 하나의 세트\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = mymodel(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            total += targets.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_test_loss.append(avg_loss)\n",
        "    seq_test_acc.append(100 * correct / total)\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', avg_loss)"
      ],
      "metadata": {
        "id": "zakvcnbaEsai"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 8\n",
        "for epoch in range(0, num_epoch):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "# model 저장! - parameter값 저장\n",
        "torch.save(mymodel.state_dict(), '/content/drive/MyDrive/Colab Notebooks/mymodel_Animals-10.pt')\n",
        "print('Model Saved!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6NAQ0ebEyob",
        "outputId": "152ff941-0287-4d02-e991-1c3fec3a9b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.7090615916252136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1시간이 넘도록 epoch1을 벗어나지 못하는 이유로 더 이상의 진행이 어렵다고 느껴졌습니다. 문제를 해결하고 다시 시도해볼 생각입니다 ㅠㅠ"
      ],
      "metadata": {
        "id": "8jfH_hzkq1Cy"
      }
    }
  ]
}