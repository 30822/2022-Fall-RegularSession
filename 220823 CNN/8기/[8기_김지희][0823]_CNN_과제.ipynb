{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 폴더 정리\n",
        "\n",
        "* 제공해드린 animals10.zip의 압축을 풀어 그 내용을 살펴보시고, 폴더 구조를 학습에 알맞도록 재구성해주세요\n",
        "* 특히 각 클래스마다 약 30% 정도의 이미지를 test 폴더에 할당해주세요\n",
        "* 중간중간 헷갈리다면 이것저것 확인하는 코드를 거쳐보세요 (ex. flat_test[:5]로 앞의 다섯값 확인)\n",
        "* 궁금한 점이 있을 경우, 슬랙 질문 채널 활용을 적극 권장합니다."
      ],
      "metadata": {
        "id": "d2ENRS4E9_xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# colab 사용할 경우\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o31UBqgoDVuc",
        "outputId": "fd3fab94-e51e-496b-8552-7e2a65c9a7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 현재 디렉터리 위치 확인\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ35ka_UDWiL",
        "outputId": "82e28e60-e86d-4f14-afbb-e8c03cfdbe85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# python에서 파일 관리해주는 라이브러리\n",
        "import os\n",
        "import glob\n",
        "import shutil"
      ],
      "metadata": {
        "id": "hWkGcz3lDqFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7tlWm3L9KoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05af13c5-d496-4c16-f9be-b10af6a04d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace Animals-10/butterfly/butterfly (1).jpeg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "### Guideline for convenience ###\n",
        "# 압축 풀기\n",
        "!unzip -qq \"/content/drive/MyDrive/animals10.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Djpz6BOE_Jh",
        "outputId": "c6c59c2b-cfa6-4aca-966d-01d7812f5a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Animals-10  drive  sample_data\ttest  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 클래스별 파일 개수 확인 -> 변수로 해당 개수 저장\n",
        "# Tip) 클래스가 10개이므로 각 클래스마다 똑같은 내용의 코드를 계속 써야할까요? 클래스 이름을 리스트로 저장해 for문을 돌리면서 코드를 재사용하는건 어떨까요?\n",
        "\n",
        "animals = ['butterfly', 'cat', 'chicken', 'cow', 'dog', 'elephant', 'horse', 'sheep', 'spider', 'squirrel']\n",
        "for i in range(10):\n",
        "    globals()[animals[i]] = sorted(glob.glob(f'./Animals-10/{animals[i]}/*'))\n"
      ],
      "metadata": {
        "id": "RPXh3srt-1Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(len(globals()[animals[i]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8fdPXJBKqpa",
        "outputId": "df2e8f91-1291-4c5a-fc78-3c6f8cd13ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2112\n",
            "1668\n",
            "3098\n",
            "1866\n",
            "4863\n",
            "1446\n",
            "2623\n",
            "1820\n",
            "4821\n",
            "1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX6x50etK7Ps",
        "outputId": "9c584b68-0193-493a-89d5-c5cb0b8530cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./Animals-10/cat/cat (1).jpeg',\n",
              " './Animals-10/cat/cat (1).jpg',\n",
              " './Animals-10/cat/cat (1).png',\n",
              " './Animals-10/cat/cat (10).jpeg',\n",
              " './Animals-10/cat/cat (10).jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test에 넣을 이미지 개수 계산\n",
        "for i in range(10):\n",
        "    globals()[f'{animals[i]}_test_count'] = round(len(globals()[animals[i]])*0.3)\n"
      ],
      "metadata": {
        "id": "9Jikw6Rp_BQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split(img_list, test_count, train_path, test_path):\n",
        "  # img_list : 이미지 경로 리스트\n",
        "  # test_count : test에 할당할 이미지 개수\n",
        "  # train_path : train 데이터 넣을 경로\n",
        "  # test_path : test 데이터 넣을 경로\n",
        "  \n",
        "  # 랜덤으로 test_count만큼 이미지 img_list에서 고르기\n",
        "  # test 담을 이미지 리스트 저장\n",
        "  test_files=[]\n",
        "  for i in random.sample(img_list, test_count):\n",
        "    test_files.append(i)\n",
        "\n",
        "  # 위에서 고르지 않은 이미지들을 train 담을 이미지 리스트로 저장\n",
        "  train_files = [x for x in img_list if x not in test_files]\n",
        "\n",
        "  # 고른 이미지를 train_path, test_path폴더에 폭사\n",
        "  for k in train_files:\n",
        "    shutil.copy(k, train_path)\n",
        "  \n",
        "  for c in test_files:\n",
        "    shutil.copy(c, test_path)\n",
        "\n",
        "  print('train 폴더 이미지 개수 : {}\\ntest 폴더 이미지 개수 : {}'.format(len(glob.glob(train_path+'/*')),len(glob.glob(test_path+'/*'))))"
      ],
      "metadata": {
        "id": "PJfpCS2RSSLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 폴더 경로 선언 & 만들기\n",
        "\n",
        "for i in range(10):\n",
        "    globals()[f'{animals[i]}_train_path'] = f'./train/{animals[i]}/'\n",
        "    globals()[f'{animals[i]}_test_path'] = f'./test/{animals[i]}/'\n",
        "    os.makedirs(globals()[f'{animals[i]}_train_path'], exist_ok=True)\n",
        "    os.makedirs(globals()[f'{animals[i]}_test_path'], exist_ok=True)"
      ],
      "metadata": {
        "id": "bb2XMrAe_Fl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 폴더로 이미지 split해서 알맞게 집어넣기\n",
        "\n",
        "for i in range(10):\n",
        "    split(globals()[animals[i]], globals()[f'{animals[i]}_test_count'], globals()[f'{animals[i]}_train_path'],  globals()[f'{animals[i]}_test_path'])"
      ],
      "metadata": {
        "id": "BaAXh57G_srE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e9b58a-184d-46c7-8632-cad2581c9cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 폴더 이미지 개수 : 1922\n",
            "test 폴더 이미지 개수 : 1078\n",
            "train 폴더 이미지 개수 : 1516\n",
            "test 폴더 이미지 개수 : 848\n",
            "train 폴더 이미지 개수 : 2808\n",
            "test 폴더 이미지 개수 : 1568\n",
            "train 폴더 이미지 개수 : 1701\n",
            "test 폴더 이미지 개수 : 955\n",
            "train 폴더 이미지 개수 : 4406\n",
            "test 폴더 이미지 개수 : 2461\n",
            "train 폴더 이미지 개수 : 1318\n",
            "test 폴더 이미지 개수 : 740\n",
            "train 폴더 이미지 개수 : 2400\n",
            "test 폴더 이미지 개수 : 1351\n",
            "train 폴더 이미지 개수 : 1645\n",
            "test 폴더 이미지 개수 : 917\n",
            "train 폴더 이미지 개수 : 4389\n",
            "test 폴더 이미지 개수 : 2460\n",
            "train 폴더 이미지 개수 : 1692\n",
            "test 폴더 이미지 개수 : 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. CNN 모델\n",
        "수업 코드에 '모델 성능에 대한 고민!!' 부분을 참고해서 **test 이미지 기준 최소 80% 이상**의 모델이 나올 수 있도록 학습을 진행해주세요.\n",
        "* 중간중간 헷갈리다면 이것저것 확인하는 코드를 거쳐보세요\n",
        "* 90% 이상의 모델이 이상적이긴 합니다\n",
        "* 궁금한 점이 있을 경우, 슬랙 질문 채널 활용을 적극 권장합니다."
      ],
      "metadata": {
        "id": "UKa7LZ4RAXxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "전반적으로 수업 코드와 비슷하게 짜면 되는데, 생각해봐야 할 것은\n",
        "* 모든 이미지를 내가 정의하는 하나의 모델에 넣어야함\n",
        "* 그말은 input 데이터의 차원이 항상 동일해야 된다는 말\n",
        "* 그런데 과연 내가 가지고 있는 이미지들의 사이즈가 모두 같을까? - 대략 얼마정도 사이즈 가지고 있는지 코드로 확인해보면 더 좋음 (shape 확인)\n",
        "* 이미지 사이즈를 통일시키기 위해서는 어떻게 해야할까? - transforms의 Resize, RandomCrop 써볼까?\n",
        "* 동물 이미지에 적합한 Augmentation은 무엇이 있을까?"
      ],
      "metadata": {
        "id": "31hFiKNBD48l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "kFe_FuSzZErv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 이미지를 위한 transforms function 정의\n",
        "# 위에서 말한 포인트들 생각해보기 - 차원에 유의하자!\n",
        "\n",
        "trans_tr = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ColorJitter(contrast=0.5),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "UM1CiMcdAfas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_te = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "d-qkZ8MDdiXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageFolder와 앞서 정의한 transforms function을 활용해 Dataset 객체 (train, test에 대해) 선언\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(\n",
        "    root = './train', \n",
        "    transform = trans_tr \n",
        ")\n",
        "test_data = torchvision.datasets.ImageFolder(\n",
        "    root = './test',\n",
        "    transform = trans_te\n",
        ")"
      ],
      "metadata": {
        "id": "QAYc35vdAlHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIAQ1onW8A5S",
        "outputId": "01043c3c-3537-42f7-c99f-6bb58c7efc15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# train, test에 대해 DataLoader 정의\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size = 32,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=True,         # train dataloader는 epoch마다 데이터 다시 섞어서 batch 만들고 학습! - 학습 효율 up\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size = 32,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=False,        # test dataloader는 데이터 매번 섞을 이유가 없음 - 어차피 확인 용도\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "gHF4PaUQAvzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "er13t-tYflBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Model 클래스 정의 - 차원에 유의하자!\n",
        "# 10개의 클래스를 분류해야 하는 꽤나 복잡한 task - 모델 구조를 어느 정도로 복잡하게 짜볼까?\n",
        "# 가장 마지막 Linear의 out_features는 얼마로 해야할까?\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        ##### Layer 정의 #####\n",
        "        self.layer = nn.Sequential(\n",
        "            # 맨처음 RGB 채널 3개이므로 가장 처음 in_channels = 3\n",
        "            # img의 가장 첫 차원이 batch_size 값은 계속해서 유지\n",
        "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5),             # [batch_size,3,224,224] -> [batch_size,16,220,220] -> same with? - using 16 filters\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5),            # [batch_size,16,220,220] -> [batch_size,32,216,216]\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2), # 0.2 확률로 Dropout\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                # [batch_size,32,216,216] -> [batch_size,32,108,108]\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), # [batch_size,32,108,108] -> [batch_size,32,108,108] -> padding*2 주의!\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5),            # [batch_size,32,108,108] -> [batch_size,32,104,104]\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)                                 # [batch_size,32,104,104] -> [batch_size,32,52,52]\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(32*52*52,100),                                                # [batch_size,32*52*52] -> [batch_size,100]\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass 과정\n",
        "        out = self.layer(x)\n",
        "        out = out.view(out.size(0),-1)  # FC layer에 들어가기전, flatten! - view 함수 활용\n",
        "                                        # 가장 첫 차원인 batch_size는 유지하고 나머지 차원들을 하나로 합치기 - [batch_size,16,5,5] -> [batch_size,16*5*5]\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Kpvu7SJtAyUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummary로 모델 차원 어떻게 나오는지 돌려보기\n",
        "\n",
        "from torchsummary import summary\n",
        "model = MyModel().cuda() # 모델 객체 선언\n",
        "summary(model, (3,224,224), batch_size=32)"
      ],
      "metadata": {
        "id": "K_AvhGgTCXO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2833e43-290e-4e80-cca0-a8c43271b150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [32, 16, 220, 220]           1,216\n",
            "              ReLU-2         [32, 16, 220, 220]               0\n",
            "            Conv2d-3         [32, 32, 216, 216]          12,832\n",
            "              ReLU-4         [32, 32, 216, 216]               0\n",
            "           Dropout-5         [32, 32, 216, 216]               0\n",
            "         MaxPool2d-6         [32, 32, 108, 108]               0\n",
            "            Conv2d-7         [32, 32, 108, 108]          25,632\n",
            "              ReLU-8         [32, 32, 108, 108]               0\n",
            "            Conv2d-9         [32, 32, 104, 104]          25,632\n",
            "             ReLU-10         [32, 32, 104, 104]               0\n",
            "          Dropout-11         [32, 32, 104, 104]               0\n",
            "        MaxPool2d-12           [32, 32, 52, 52]               0\n",
            "           Linear-13                  [32, 100]       8,652,900\n",
            "             ReLU-14                  [32, 100]               0\n",
            "           Linear-15                   [32, 10]           1,010\n",
            "================================================================\n",
            "Total params: 8,719,222\n",
            "Trainable params: 8,719,222\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 18.38\n",
            "Forward/backward pass size (MB): 2019.68\n",
            "Params size (MB): 33.26\n",
            "Estimated Total Size (MB): 2071.31\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device 선언 (GPU 권장) => 코랩 사용중이어서\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "A99ALViSCbmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78c7ba9-8c4f-4ad0-95ba-0f1abc200165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss func, optimizer 정의\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device) # criterion (loss func)도 device 위에서\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001) # 보통 Adam의 learning rate로 0.001 사용\n",
        "                                                                                  # weight_decay = L2 Regularization의 lambda값 (가중치 제한 정도)"
      ],
      "metadata": {
        "id": "CFk7o_fuCgc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "        print(inputs.shape)  # batch_size개의 이미지\n",
        "        print(targets.shape) # batch_size개의 클래스\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        print(outputs.shape) # outputs는 32, 10 차원 (32개 data의 확률 10개씩)\n",
        "        print(targets.shape) # targets는 32차원 (32개 data의 실제 클래스)\n",
        "\n",
        "        loss = criterion(outputs, targets)  # -> cross entropy 식에 넣을때 이런 형태로 넣으면됨 (outputs, targets의 차원을 맞출 필요가 없음)\n",
        "        print(loss.item())\n",
        "\n",
        "        print(outputs.max(1)[1]) # 예측한 클래스\n",
        "        print(targets) # 실제 클래스\n",
        "        correct = (outputs.max(1)[1] == targets).sum().item() # 맞게 예측한 클래스 개수\n",
        "        print(correct)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq_Vz368oYwk",
        "outputId": "80576bbd-7419-4760-c9a1-d8ffa1fd70b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 224, 224])\n",
            "torch.Size([32])\n",
            "torch.Size([32, 10])\n",
            "torch.Size([32])\n",
            "2.291752815246582\n",
            "tensor([7, 9, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 7, 7, 7, 7, 7,\n",
            "        7, 7, 7, 7, 7, 7, 7, 7], device='cuda:0')\n",
            "tensor([0, 9, 7, 2, 4, 8, 8, 6, 1, 8, 2, 6, 6, 4, 9, 8, 9, 4, 0, 4, 1, 7, 8, 0,\n",
            "        4, 3, 7, 6, 7, 0, 9, 8], device='cuda:0')\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 추후 시각화를 위해 매 epoch마다 loss, accuracy 값을 저장할 리스트\n",
        "seq_train_loss = []\n",
        "seq_test_loss = []\n",
        "seq_train_acc = []\n",
        "seq_test_acc = []"
      ],
      "metadata": {
        "id": "Nk2vPDpbkQAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 함수 정의\n",
        "\n",
        "def train(epoch):\n",
        "    print(f'\\n[ Train epoch: {epoch+1} ]')\n",
        "\n",
        "    model.train() # train은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하고 진행하기 위함\n",
        "\n",
        "    running_loss = 0.0\n",
        "    batch_losses = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # dataloader의 input image와 label도 device에 넣어줘야됨\n",
        "        \n",
        "        # DL 학습 기본 코드\n",
        "        optimizer.zero_grad() # gradient 초기화\n",
        "        outputs = model(inputs) # 현재 batch의 inputs을 모델에 넣어 outputs 추출 (확률값)\n",
        "        loss = criterion(outputs, targets) # 추출한 outputs와 원래 label인 targets 사이 loss 계산\n",
        "        loss.backward() # 계산한 loss 기반으로 gradient 값 계산\n",
        "        optimizer.step() # weight parameter update\n",
        "\n",
        "        total += targets.size(0) # batch 데이터 개수 더하기\n",
        "        running_loss += loss.item()\n",
        "        batch_losses.append(loss.item())\n",
        "        \n",
        "        _, predicted = outputs.max(1) # 확률값 가장 높게 나타난 클래스\n",
        "        correct += (predicted == targets).sum().item() # 현재 batch 내에서 알맞게 분류한 이미지 개수 더하기\n",
        "        \n",
        "        if batch_idx % 300 == 299:\n",
        "            print(f'\\nCurrent batch: {str(batch_idx+1)}')\n",
        "            print(f'Average train loss of recent 300 batches: {running_loss / 300}') # 이렇게 출력하는 것이 꼭 필요한 것은 아니지만, 중간중간 확인을 위해 매우 권장\n",
        "            running_loss = 0.0\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_train_loss.append(avg_loss)\n",
        "    seq_train_acc.append(100*correct/total)\n",
        "    print('\\nTotal train accuarcy:', 100. * correct / total) # 전체 데이터 개수에서 맞게 예측한 비율\n",
        "    print('Total train loss:', avg_loss)"
      ],
      "metadata": {
        "id": "-ENGsLI3CksA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 함수 정의\n",
        "def test(epoch):\n",
        "    print(f'\\n[ Test epoch: {epoch+1} ]')\n",
        "\n",
        "    model.eval() # eval은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하지 않기 위함!\n",
        "                   # ex. evaluation 할때는 Dropout 없이 지금까지 학습한 모든 node를 활용해서 진행해야됨\n",
        "\n",
        "    loss = 0\n",
        "    batch_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # gradient update 안함 - eval과 torch.no_grad는 하나의 세트\n",
        "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            total += targets.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_test_loss.append(avg_loss)\n",
        "    seq_test_acc.append(100 * correct / total)\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', avg_loss)"
      ],
      "metadata": {
        "id": "zxKTZ7oACnGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "num_epoch = 40\n",
        "\n",
        "for epoch in range(0, num_epoch):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "\n",
        "# model 저장! - parameter값 저장\n",
        "torch.save(model.state_dict(), './mymodel.pt')\n",
        "print('Model Saved!')"
      ],
      "metadata": {
        "id": "Y2AB6X97Cx4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dd7de0-d01b-4e39-c832-337e1c6999d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 2.2158764922618865\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 2.0948877942562105\n",
            "\n",
            "Total train accuarcy: 22.910450897171913\n",
            "Total train loss: 2.137061884166092\n",
            "\n",
            "[ Test epoch: 1 ]\n",
            "\n",
            "Test accuarcy: 27.100054628734714\n",
            "Test average loss: 2.0576680318642686\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 2.05214643796285\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 2.0004034388065337\n",
            "\n",
            "Total train accuarcy: 28.516199520948017\n",
            "Total train loss: 2.0142397588940075\n",
            "\n",
            "[ Test epoch: 2 ]\n",
            "\n",
            "Test accuarcy: 31.545993192419214\n",
            "Test average loss: 1.950546895944944\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.91959849913915\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.8613915689786276\n",
            "\n",
            "Total train accuarcy: 33.907635416228935\n",
            "Total train loss: 1.882201093980061\n",
            "\n",
            "[ Test epoch: 3 ]\n",
            "\n",
            "Test accuarcy: 35.86166323486154\n",
            "Test average loss: 1.8503447029218878\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.7961817908287048\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.7784442897637684\n",
            "\n",
            "Total train accuarcy: 37.643400428625455\n",
            "Total train loss: 1.7871743724230797\n",
            "\n",
            "[ Test epoch: 4 ]\n",
            "\n",
            "Test accuarcy: 37.84930873639534\n",
            "Test average loss: 1.803888461442404\n",
            "\n",
            "[ Train epoch: 5 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.760376029809316\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.715317989985148\n",
            "\n",
            "Total train accuarcy: 40.13951338404001\n",
            "Total train loss: 1.7371053649212724\n",
            "\n",
            "[ Test epoch: 5 ]\n",
            "\n",
            "Test accuarcy: 41.274110181955706\n",
            "Test average loss: 1.7224951171746818\n",
            "\n",
            "[ Train epoch: 6 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.7066454116503398\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.698656458457311\n",
            "\n",
            "Total train accuarcy: 41.2152792368786\n",
            "Total train loss: 1.6969143954976913\n",
            "\n",
            "[ Test epoch: 6 ]\n",
            "\n",
            "Test accuarcy: 40.48409463377737\n",
            "Test average loss: 1.7054247784037744\n",
            "\n",
            "[ Train epoch: 7 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.6636454753081005\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.6589367230733236\n",
            "\n",
            "Total train accuarcy: 42.45072908349792\n",
            "Total train loss: 1.663209082779064\n",
            "\n",
            "[ Test epoch: 7 ]\n",
            "\n",
            "Test accuarcy: 43.715594402655796\n",
            "Test average loss: 1.6440680980361917\n",
            "\n",
            "[ Train epoch: 8 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.6832174774010975\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.6505264075597128\n",
            "\n",
            "Total train accuarcy: 42.92557885447746\n",
            "Total train loss: 1.6561866479535257\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 43.202924738412406\n",
            "Test average loss: 1.6520688539390922\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.6324466594060263\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.5943199038505553\n",
            "\n",
            "Total train accuarcy: 44.8249779383956\n",
            "Total train loss: 1.6056957401896035\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 44.50560995083414\n",
            "Test average loss: 1.615228075974731\n",
            "\n",
            "[ Train epoch: 10 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.5736675143241883\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.5879906062285105\n",
            "\n",
            "Total train accuarcy: 45.446905072067906\n",
            "Total train loss: 1.5747101853291194\n",
            "\n",
            "[ Test epoch: 10 ]\n",
            "\n",
            "Test accuarcy: 46.91347648863302\n",
            "Test average loss: 1.5581163054191938\n",
            "\n",
            "[ Train epoch: 11 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.5354434390862783\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.5865948295593262\n",
            "\n",
            "Total train accuarcy: 46.66134386687398\n",
            "Total train loss: 1.5513450726065585\n",
            "\n",
            "[ Test epoch: 11 ]\n",
            "\n",
            "Test accuarcy: 47.54380804303064\n",
            "Test average loss: 1.5212230330673597\n",
            "\n",
            "[ Train epoch: 12 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.521258189280828\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.5119142844279607\n",
            "\n",
            "Total train accuarcy: 47.69928982644871\n",
            "Total train loss: 1.5118124904812023\n",
            "\n",
            "[ Test epoch: 12 ]\n",
            "\n",
            "Test accuarcy: 49.34655628860781\n",
            "Test average loss: 1.4822986776469855\n",
            "\n",
            "[ Train epoch: 13 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.525634340842565\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4988049113750457\n",
            "\n",
            "Total train accuarcy: 47.989242341471616\n",
            "Total train loss: 1.504586753265191\n",
            "\n",
            "[ Test epoch: 13 ]\n",
            "\n",
            "Test accuarcy: 49.0061772492331\n",
            "Test average loss: 1.4724987974410415\n",
            "\n",
            "[ Train epoch: 14 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.4711618347962698\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4564490008354187\n",
            "\n",
            "Total train accuarcy: 49.594486700004204\n",
            "Total train loss: 1.4673842194900717\n",
            "\n",
            "[ Test epoch: 14 ]\n",
            "\n",
            "Test accuarcy: 45.484724965331765\n",
            "Test average loss: 1.5768237456839571\n",
            "\n",
            "[ Train epoch: 15 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.5060621472199758\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4565186337629954\n",
            "\n",
            "Total train accuarcy: 48.783460100012604\n",
            "Total train loss: 1.4792496843203422\n",
            "\n",
            "[ Test epoch: 15 ]\n",
            "\n",
            "Test accuarcy: 50.174391730050004\n",
            "Test average loss: 1.4587229664127033\n",
            "\n",
            "[ Train epoch: 16 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.4646550935506821\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4520798567930857\n",
            "\n",
            "Total train accuarcy: 49.901248056477705\n",
            "Total train loss: 1.4475600900509025\n",
            "\n",
            "[ Test epoch: 16 ]\n",
            "\n",
            "Test accuarcy: 48.90112199016683\n",
            "Test average loss: 1.4991350140302413\n",
            "\n",
            "[ Train epoch: 17 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.4334459728002549\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4349362456798553\n",
            "\n",
            "Total train accuarcy: 50.32567130310543\n",
            "Total train loss: 1.4354875270397431\n",
            "\n",
            "[ Test epoch: 17 ]\n",
            "\n",
            "Test accuarcy: 50.28364919947892\n",
            "Test average loss: 1.4280658672733972\n",
            "\n",
            "[ Train epoch: 18 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.414427825808525\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4263483915726345\n",
            "\n",
            "Total train accuarcy: 50.90137412278859\n",
            "Total train loss: 1.4235666744330877\n",
            "\n",
            "[ Test epoch: 18 ]\n",
            "\n",
            "Test accuarcy: 52.59906710929949\n",
            "Test average loss: 1.4130200515350988\n",
            "\n",
            "[ Train epoch: 19 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.4224234515428542\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4143180253108343\n",
            "\n",
            "Total train accuarcy: 51.191326637811486\n",
            "Total train loss: 1.4171310257847591\n",
            "\n",
            "[ Test epoch: 19 ]\n",
            "\n",
            "Test accuarcy: 52.3343278564525\n",
            "Test average loss: 1.3964306717758537\n",
            "\n",
            "[ Train epoch: 20 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.3926375478506088\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.4246536445617677\n",
            "\n",
            "Total train accuarcy: 51.19552884817414\n",
            "Total train loss: 1.413186001441171\n",
            "\n",
            "[ Test epoch: 20 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss, accuracy 추이 확인 with plt.plot\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ZwJHGOY9Czzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(seq_train_loss, label='Train')\n",
        "plt.plot(seq_test_loss, label='Test')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sxhrIi08kihY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "2eb1e9f3-1061-44c2-8075-9a3745560de8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a4b95ba1b43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_train_loss' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.plot(seq_train_acc, label='Train')\n",
        "plt.plot(seq_test_acc, label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s7FWJKUqkjtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLO_cvPwXRUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}