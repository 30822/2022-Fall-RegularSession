{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[0823] CNN 과제.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 폴더 정리\n",
        "\n",
        "* 제공해드린 animals10.zip의 압축을 풀어 그 내용을 살펴보시고, 폴더 구조를 학습에 알맞도록 재구성해주세요\n",
        "* 특히 각 클래스마다 약 30% 정도의 이미지를 test 폴더에 할당해주세요\n",
        "* 중간중간 헷갈리다면 이것저것 확인하는 코드를 거쳐보세요 (ex. flat_test[:5]로 앞의 다섯값 확인)\n",
        "* 궁금한 점이 있을 경우, 슬랙 질문 채널 활용을 적극 권장합니다."
      ],
      "metadata": {
        "id": "d2ENRS4E9_xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwymfLZxHqeX",
        "outputId": "ad187aee-13aa-402b-b2a8-6d9f44000752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO-xnUKMH5GK",
        "outputId": "c67ca268-93c5-435b-b951-8981c670dedd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil"
      ],
      "metadata": {
        "id": "df_tolvxH_zr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u7tlWm3L9KoV"
      },
      "outputs": [],
      "source": [
        "### Guideline for convenience ###\n",
        "# 압축 풀기\n",
        "!unzip -qq \"./drive/MyDrive/dsl/animals10.zip\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikDg--UXIs_s",
        "outputId": "3b89e52a-feeb-4753-e4fe-97d198cc6fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Animals-10  drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 클래스별 파일 개수 확인 -> 변수로 해당 개수 저장\n",
        "# Tip) 클래스가 10개이므로 각 클래스마다 똑같은 내용의 코드를 계속 써야할까요? 클래스 이름을 리스트로 저장해 for문을 돌리면서 코드를 재사용하는건 어떨까요?\n",
        "animals = [\"butterfly\",'cat','chicken','cow','dog','elephant','horse','sheep','spider','squirrel']\n",
        "for anim in animals:\n",
        "    animal = sorted(glob.glob(f'./Animals-10/{anim}/*'))\n",
        "    print(f'name: {anim}\\t 개수: {len(animal)}')"
      ],
      "metadata": {
        "id": "RPXh3srt-1Rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0c6a75d-e0cc-435b-f7a6-5ec4b1505bda"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: butterfly\t 개수: 2112\n",
            "name: cat\t 개수: 1668\n",
            "name: chicken\t 개수: 3098\n",
            "name: cow\t 개수: 1866\n",
            "name: dog\t 개수: 4863\n",
            "name: elephant\t 개수: 1446\n",
            "name: horse\t 개수: 2623\n",
            "name: sheep\t 개수: 1820\n",
            "name: spider\t 개수: 4821\n",
            "name: squirrel\t 개수: 1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test에 넣을 이미지 개수 계산\n",
        "test_cnt = []\n",
        "for anim in animals:\n",
        "    animal = sorted(glob.glob(f'./Animals-10/{anim}/*'))\n",
        "    test_count = round(len(animal)*0.3)\n",
        "    test_cnt.append(test_count)\n",
        "    print(f'name: {anim}\\t test 수: {test_count}')"
      ],
      "metadata": {
        "id": "9Jikw6Rp_BQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fa46bc-59e3-49ac-e743-b4a9e2b1cc06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name: butterfly\t test 수: 634\n",
            "name: cat\t test 수: 500\n",
            "name: chicken\t test 수: 929\n",
            "name: cow\t test 수: 560\n",
            "name: dog\t test 수: 1459\n",
            "name: elephant\t test 수: 434\n",
            "name: horse\t test 수: 787\n",
            "name: sheep\t test 수: 546\n",
            "name: spider\t test 수: 1446\n",
            "name: squirrel\t test 수: 559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 폴더 경로 선언 & 만들기\n",
        "import random\n",
        "\n",
        "def split(img_list, test_count, train_path, test_path):\n",
        "  # img_list : 이미지 경로 리스트\n",
        "  # test_count : test에 할당할 이미지 개수\n",
        "  # train_path : train 데이터 넣을 경로\n",
        "  # test_path : test 데이터 넣을 경로\n",
        "  \n",
        "  # 랜덤으로 test_count만큼 이미지 img_list에서 고르기\n",
        "  # test 담을 이미지 리스트 저장\n",
        "  test_files=[]\n",
        "  for i in random.sample(img_list, test_count):\n",
        "    test_files.append(i)\n",
        "\n",
        "  # 위에서 고르지 않은 이미지들을 train 담을 이미지 리스트로 저장\n",
        "  train_files = [x for x in img_list if x not in test_files]\n",
        "\n",
        "  # 고른 이미지를 train_path, test_path폴더에 폭사\n",
        "  for k in train_files:\n",
        "    shutil.copy(k, train_path)\n",
        "  \n",
        "  for c in test_files:\n",
        "    shutil.copy(c, test_path)\n",
        "\n",
        "  print('train 폴더 이미지 개수 : {}\\ntest 폴더 이미지 개수 : {}'.format(len(glob.glob(train_path+'/*')),len(glob.glob(test_path+'/*'))))"
      ],
      "metadata": {
        "id": "bb2XMrAe_Fl8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for anim in animals:\n",
        "    train_path = f'./train/{anim}'\n",
        "    test_path = f'./test/{anim}'\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "    os.makedirs(test_path, exist_ok=True)"
      ],
      "metadata": {
        "id": "DIAckGeDLork"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 폴더로 이미지 split해서 알맞게 집어넣기\n",
        "for anim in animals:\n",
        "    train_path = f'./train/{anim}'\n",
        "    test_path = f'./test/{anim}'\n",
        "    animal = sorted(glob.glob(f'./Animals-10/{anim}/*'))\n",
        "    a = round(len(animal)*0.3)\n",
        "    split(animal,a,train_path,test_path)"
      ],
      "metadata": {
        "id": "BaAXh57G_srE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6daaa55f-21c9-4ba4-f29f-6c23ab8393c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 폴더 이미지 개수 : 1478\n",
            "test 폴더 이미지 개수 : 634\n",
            "train 폴더 이미지 개수 : 1168\n",
            "test 폴더 이미지 개수 : 500\n",
            "train 폴더 이미지 개수 : 2169\n",
            "test 폴더 이미지 개수 : 929\n",
            "train 폴더 이미지 개수 : 1306\n",
            "test 폴더 이미지 개수 : 560\n",
            "train 폴더 이미지 개수 : 3404\n",
            "test 폴더 이미지 개수 : 1459\n",
            "train 폴더 이미지 개수 : 1012\n",
            "test 폴더 이미지 개수 : 434\n",
            "train 폴더 이미지 개수 : 1836\n",
            "test 폴더 이미지 개수 : 787\n",
            "train 폴더 이미지 개수 : 1274\n",
            "test 폴더 이미지 개수 : 546\n",
            "train 폴더 이미지 개수 : 3375\n",
            "test 폴더 이미지 개수 : 1446\n",
            "train 폴더 이미지 개수 : 1303\n",
            "test 폴더 이미지 개수 : 559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for anim in animals:\n",
        "    train_path = sorted(glob.glob(f'./train/{anim}/*'))\n",
        "    test_path = sorted(glob.glob(f'./test/{anim}/*'))\n",
        "    train = len(train_path)\n",
        "    test = len(test_path)\n",
        "    total = train+test\n",
        "    print(f'{anim[:3]}\\t train 개수: {train}\\ttest개수: {test}\\t총: {total}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5ajbWJdPt0H",
        "outputId": "bb2278aa-da06-4a7b-d87c-3695ff7eaefe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "but\t train 개수: 1478\ttest개수: 634\t총: 2112\n",
            "cat\t train 개수: 1168\ttest개수: 500\t총: 1668\n",
            "chi\t train 개수: 2169\ttest개수: 929\t총: 3098\n",
            "cow\t train 개수: 1306\ttest개수: 560\t총: 1866\n",
            "dog\t train 개수: 3404\ttest개수: 1459\t총: 4863\n",
            "ele\t train 개수: 1012\ttest개수: 434\t총: 1446\n",
            "hor\t train 개수: 1836\ttest개수: 787\t총: 2623\n",
            "she\t train 개수: 1274\ttest개수: 546\t총: 1820\n",
            "spi\t train 개수: 3375\ttest개수: 1446\t총: 4821\n",
            "squ\t train 개수: 1303\ttest개수: 559\t총: 1862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. CNN 모델\n",
        "수업 코드에 '모델 성능에 대한 고민!!' 부분을 참고해서 **test 이미지 기준 최소 80% 이상**의 모델이 나올 수 있도록 학습을 진행해주세요.\n",
        "* 중간중간 헷갈리다면 이것저것 확인하는 코드를 거쳐보세요\n",
        "* 90% 이상의 모델이 이상적이긴 합니다\n",
        "* 궁금한 점이 있을 경우, 슬랙 질문 채널 활용을 적극 권장합니다."
      ],
      "metadata": {
        "id": "UKa7LZ4RAXxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "전반적으로 수업 코드와 비슷하게 짜면 되는데, 생각해봐야 할 것은\n",
        "* 모든 이미지를 내가 정의하는 하나의 모델에 넣어야함\n",
        "* 그말은 input 데이터의 차원이 항상 동일해야 된다는 말\n",
        "* 그런데 과연 내가 가지고 있는 이미지들의 사이즈가 모두 같을까? - 대략 얼마정도 사이즈 가지고 있는지 코드로 확인해보면 더 좋음 (shape 확인)\n",
        "* 이미지 사이즈를 통일시키기 위해서는 어떻게 해야할까? - transforms의 Resize, RandomCrop 써볼까?\n",
        "* 동물 이미지에 적합한 Augmentation은 무엇이 있을까?"
      ],
      "metadata": {
        "id": "31hFiKNBD48l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "MLMp9MB2Ulc9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test 이미지를 위한 transforms function 정의\n",
        "# 위에서 말한 포인트들 생각해보기 - 차원에 유의하자!\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),  \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "UM1CiMcdAfas"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "JY08MELSV22U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageFolder와 앞서 정의한 transforms function을 활용해 Dataset 객체 (train, test에 대해) 선언\n",
        "train_data = torchvision.datasets.ImageFolder(\n",
        "    root = './train', # 바로 train 폴더 지정\n",
        "    transform = transform_train # transform은 그냥 편의상 cifar에서 썼던거 그대로\n",
        ")\n",
        "test_data = torchvision.datasets.ImageFolder(\n",
        "    root = './test',\n",
        "    transform = transform_test\n",
        ")"
      ],
      "metadata": {
        "id": "QAYc35vdAlHk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data),len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtGl0BrhWORR",
        "outputId": "19a8c7ba-46ea-4496-e96a-67c5612296a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18325 7854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test에 대해 DataLoader 정의\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    train_data,\n",
        "    batch_size = 16,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=True,         # train dataloader는 epoch마다 데이터 다시 섞어서 batch 만들고 학습! - 학습 효율 up\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    test_data,\n",
        "    batch_size = 16,      # 일반적으로 batch size = 16, 32\n",
        "    shuffle=False,        # test dataloader는 데이터 매번 섞을 이유가 없음 - 어차피 확인 용도\n",
        "    num_workers=2\n",
        ")"
      ],
      "metadata": {
        "id": "gHF4PaUQAvzJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9EexGaAYZsb",
        "outputId": "7e5be16e-06d2-46ff-8cbb-c065aa342f7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1146"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0Kh7hQpglQ-",
        "outputId": "1e260bba-770a-4519-f9e4-8fa548662cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f9d0a63d350>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Model 클래스 정의 - 차원에 유의하자!\n",
        "# 10개의 클래스를 분류해야 하는 꽤나 복잡한 task - 모델 구조를 어느 정도로 복잡하게 짜볼까?\n",
        "# 가장 마지막 Linear의 out_features는 얼마로 해야할까?\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        ##### Layer 정의 #####\n",
        "        self.layer = nn.Sequential(\n",
        "            # 맨처음 RGB 채널 3개이므로 가장 처음 in_channels = 3\n",
        "            # img의 가장 첫 차원이 batch_size 값은 계속해서 유지\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride = 1, padding=1), # [batch_size,3,224,224] -> [batch_size,32,224,224]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride = 1, padding=1),  # [batch_size,32,224,224] -> [batch_size,32,224,224]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                                # [batch_size,32,224,224] -> [batch_size,32,112,112]\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2, padding=1), # [batch_size,32,112,112] -> [batch_size,32,56,56]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, stride=2, padding=1),  # [batch_size,32,56,56] -> [batch_size,32,28,28]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),   \n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),    # 32, 14, 14\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),  # [batch_size,32,14,14] -> [batch_size,32,14,14]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),  # [batch_size,32,14,14] -> [batch_size,32,14,14]\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)                              # [batch_size,32,14,14] -> [batch_size,32,7,7]\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(32*7*7,100),                                                # [batch_size,32*7*7] -> [batch_size,100]\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward pass 과정\n",
        "        out = self.layer(x)\n",
        "        out = out.view(out.size(0),-1)  # FC layer에 들어가기전, flatten! - view 함수 활용\n",
        "                                        # 가장 첫 차원인 batch_size는 유지하고 나머지 차원들을 하나로 합치기 - [batch_size,16,5,5] -> [batch_size,16*5*5]\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Kpvu7SJtAyUH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8GiN7eccJQ1",
        "outputId": "c7c91e08-fa7d-46b4-c26a-658f977efcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torchsummary로 모델 차원 어떻게 나오는지 돌려보기\n",
        "from torchsummary import summary\n",
        "test_model = MyModel() # 모델 객체 선언\n",
        "summary(test_model, (3,224,224), batch_size=16)"
      ],
      "metadata": {
        "id": "K_AvhGgTCXO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "6a343695-b872-498f-e6fe-33cad88b7a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1e6f9765d370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 모델 객체 선언\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-aca139054bb2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# forward pass 과정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FC layer에 들어가기전, flatten! - view 함수 활용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                                         \u001b[0;31m# 가장 첫 차원인 batch_size는 유지하고 나머지 차원들을 하나로 합치기 - [batch_size,16,5,5] -> [batch_size,16*5*5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [2, 16, 3, 224, 224]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device 선언 (GPU 권장)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "A99ALViSCbmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "533528b4-a39e-48a9-90fc-aab711859241"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 객체 선언\n",
        "mymodel = MyModel().to(device)\n",
        "mymodel"
      ],
      "metadata": {
        "id": "FmZYSSBFCfOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a912182-6eb2-4122-caf1-4dc544814c7e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (layer): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU()\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
              "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU()\n",
              "    (10): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
              "    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU()\n",
              "    (13): Dropout(p=0.2, inplace=False)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (17): ReLU()\n",
              "    (18): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (19): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (20): ReLU()\n",
              "    (21): Dropout(p=0.2, inplace=False)\n",
              "    (22): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc_layer): Sequential(\n",
              "    (0): Linear(in_features=1568, out_features=100, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=100, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss func, optimizer 정의\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(mymodel.parameters(), lr=0.001) "
      ],
      "metadata": {
        "id": "CFk7o_fuCgc7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_train_loss = []\n",
        "seq_test_loss = []\n",
        "seq_train_acc = []\n",
        "seq_test_acc = []"
      ],
      "metadata": {
        "id": "8ADnEb1gc9rD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 함수 정의\n",
        "def train(epoch):\n",
        "    print(f'\\n[ Train epoch: {epoch+1} ]')\n",
        "\n",
        "    mymodel.train() # train은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하고 진행하기 위함\n",
        "\n",
        "    running_loss = 0.0\n",
        "    batch_losses = []\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device) # dataloader의 input image와 label도 device에 넣어줘야됨\n",
        "        \n",
        "        # DL 학습 기본 코드\n",
        "        optimizer.zero_grad() # gradient 초기화\n",
        "        outputs = mymodel(inputs) # 현재 batch의 inputs을 모델에 넣어 outputs 추출 (확률값)\n",
        "        loss = criterion(outputs, targets) # 추출한 outputs와 원래 label인 targets 사이 loss 계산\n",
        "        loss.backward() # 계산한 loss 기반으로 gradient 값 계산\n",
        "        optimizer.step() # weight parameter update\n",
        "\n",
        "        total += targets.size(0) # batch 데이터 개수 더하기\n",
        "        running_loss += loss.item()\n",
        "        batch_losses.append(loss.item())\n",
        "        \n",
        "        _, predicted = outputs.max(1) # 확률값 가장 높게 나타난 클래스\n",
        "        correct += (predicted == targets).sum().item() # 현재 batch 내에서 알맞게 분류한 이미지 개수 더하기\n",
        "        \n",
        "        if batch_idx % 300 == 299:\n",
        "            print(f'\\nCurrent batch: {str(batch_idx+1)}')\n",
        "            print(f'Average train loss of recent 300 batches: {running_loss / 300}') # 이렇게 출력하는 것이 꼭 필요한 것은 아니지만, 중간중간 확인을 위해 매우 권장\n",
        "            running_loss = 0.0\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_train_loss.append(avg_loss)\n",
        "    seq_train_acc.append(100*correct/total)\n",
        "    print('\\nTotal train accuarcy:', 100. * correct / total) # 전체 데이터 개수에서 맞게 예측한 비율\n",
        "    print('Total train loss:', avg_loss)"
      ],
      "metadata": {
        "id": "-ENGsLI3CksA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 함수 정의\n",
        "def test(epoch):\n",
        "    print(f'\\n[ Test epoch: {epoch+1} ]')\n",
        "\n",
        "    mymodel.eval() # eval은 항상 이걸 지정하고 시작! - Dropout, Batch Normalization 등의 효과를 적용하지 않기 위함!\n",
        "                   # ex. evaluation 할때는 Dropout 없이 지금까지 학습한 모든 node를 활용해서 진행해야됨\n",
        "\n",
        "    loss = 0\n",
        "    batch_losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad(): # gradient update 안함 - eval과 torch.no_grad는 하나의 세트\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            outputs = mymodel(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "            total += targets.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
        "    seq_test_loss.append(avg_loss)\n",
        "    seq_test_acc.append(100 * correct / total)\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', avg_loss)"
      ],
      "metadata": {
        "id": "zxKTZ7oACnGs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "num_epoch = 50\n",
        "for epoch in range(0, num_epoch):\n",
        "    train(epoch)\n",
        "    test(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2o4nJ7lwe-u",
        "outputId": "96431080-651b-455f-abce-7a68db3d4e02"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.9781371470292408\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.7287604860464731\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.618582063317299\n",
            "\n",
            "Total train accuarcy: 39.86357435197817\n",
            "Total train loss: 1.7241366910268707\n",
            "\n",
            "[ Test epoch: 1 ]\n",
            "\n",
            "Test accuarcy: 46.58772599949071\n",
            "Test average loss: 1.5647484424643507\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.4708119682470957\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.3943337879578273\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.3815425010522207\n",
            "\n",
            "Total train accuarcy: 51.84720327421555\n",
            "Total train loss: 1.3961592069054982\n",
            "\n",
            "[ Test epoch: 2 ]\n",
            "\n",
            "Test accuarcy: 53.01757066462949\n",
            "Test average loss: 1.3715255894449723\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.2680657853682835\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.27839280128479\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.271529924472173\n",
            "\n",
            "Total train accuarcy: 56.2837653478854\n",
            "Total train loss: 1.2625076887807296\n",
            "\n",
            "[ Test epoch: 3 ]\n",
            "\n",
            "Test accuarcy: 56.81181563534505\n",
            "Test average loss: 1.2655054916250243\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.1900635941823323\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.2035928304990133\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.1593541049957274\n",
            "\n",
            "Total train accuarcy: 58.941336971350616\n",
            "Total train loss: 1.180255021100269\n",
            "\n",
            "[ Test epoch: 4 ]\n",
            "\n",
            "Test accuarcy: 57.66488413547237\n",
            "Test average loss: 1.224866551622111\n",
            "\n",
            "[ Train epoch: 5 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.114505639076233\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.120924250582854\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.1042939683794974\n",
            "\n",
            "Total train accuarcy: 61.82810368349249\n",
            "Total train loss: 1.1067697271886712\n",
            "\n",
            "[ Test epoch: 5 ]\n",
            "\n",
            "Test accuarcy: 63.16526610644258\n",
            "Test average loss: 1.0926419507765235\n",
            "\n",
            "[ Train epoch: 6 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.0434639991323154\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.049095710515976\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.0516229838132858\n",
            "\n",
            "Total train accuarcy: 63.950886766712145\n",
            "Total train loss: 1.0502635898754442\n",
            "\n",
            "[ Test epoch: 6 ]\n",
            "\n",
            "Test accuarcy: 63.610898905016555\n",
            "Test average loss: 1.058401182809818\n",
            "\n",
            "[ Train epoch: 7 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 1.0044300638635952\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 1.0152225581804912\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 1.0237082954247791\n",
            "\n",
            "Total train accuarcy: 65.51159618008185\n",
            "Total train loss: 1.0079533343331768\n",
            "\n",
            "[ Test epoch: 7 ]\n",
            "\n",
            "Test accuarcy: 64.40030557677616\n",
            "Test average loss: 1.0502793876920844\n",
            "\n",
            "[ Train epoch: 8 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.9804587601621946\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.9689200874169668\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.9633844817678133\n",
            "\n",
            "Total train accuarcy: 66.1718963165075\n",
            "Total train loss: 0.971724965783522\n",
            "\n",
            "[ Test epoch: 8 ]\n",
            "\n",
            "Test accuarcy: 65.87725999490705\n",
            "Test average loss: 0.9951559572865667\n",
            "\n",
            "[ Train epoch: 9 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.9501143407821655\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.9378953609863917\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.9254888878266017\n",
            "\n",
            "Total train accuarcy: 67.62892223738062\n",
            "Total train loss: 0.9400987681554042\n",
            "\n",
            "[ Test epoch: 9 ]\n",
            "\n",
            "Test accuarcy: 68.67838044308633\n",
            "Test average loss: 0.9357980169856621\n",
            "\n",
            "[ Train epoch: 10 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.91312946220239\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.9104588211576143\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.8766513050595919\n",
            "\n",
            "Total train accuarcy: 68.93860845839018\n",
            "Total train loss: 0.9033057918385269\n",
            "\n",
            "[ Test epoch: 10 ]\n",
            "\n",
            "Test accuarcy: 69.2768016297428\n",
            "Test average loss: 0.9290428223987461\n",
            "\n",
            "[ Train epoch: 11 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.8892053396999836\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.8927980349461238\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.8735046596825122\n",
            "\n",
            "Total train accuarcy: 69.95361527967258\n",
            "Total train loss: 0.8796424510487295\n",
            "\n",
            "[ Test epoch: 11 ]\n",
            "\n",
            "Test accuarcy: 69.82429335370512\n",
            "Test average loss: 0.8971460432785349\n",
            "\n",
            "[ Train epoch: 12 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.8295480062067508\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.8594332736730576\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.8547872914373875\n",
            "\n",
            "Total train accuarcy: 70.8431105047749\n",
            "Total train loss: 0.8506260604108399\n",
            "\n",
            "[ Test epoch: 12 ]\n",
            "\n",
            "Test accuarcy: 68.24548001018589\n",
            "Test average loss: 0.9303981919152674\n",
            "\n",
            "[ Train epoch: 13 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.8259551515678565\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.8366910605629285\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.8464516221483549\n",
            "\n",
            "Total train accuarcy: 71.78717598908595\n",
            "Total train loss: 0.8342751454542444\n",
            "\n",
            "[ Test epoch: 13 ]\n",
            "\n",
            "Test accuarcy: 70.35905271199388\n",
            "Test average loss: 0.8774559333647342\n",
            "\n",
            "[ Train epoch: 14 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.8086404299239317\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.8171231490870317\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.8139364422361056\n",
            "\n",
            "Total train accuarcy: 72.27285129604365\n",
            "Total train loss: 0.8144623669706298\n",
            "\n",
            "[ Test epoch: 14 ]\n",
            "\n",
            "Test accuarcy: 71.70868347338936\n",
            "Test average loss: 0.8447866890799489\n",
            "\n",
            "[ Train epoch: 15 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.8164817390839259\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7920094598829747\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7872161650160948\n",
            "\n",
            "Total train accuarcy: 72.70395634379263\n",
            "Total train loss: 0.8007888370315442\n",
            "\n",
            "[ Test epoch: 15 ]\n",
            "\n",
            "Test accuarcy: 72.03972498090145\n",
            "Test average loss: 0.8444639363016956\n",
            "\n",
            "[ Train epoch: 16 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7639883042375246\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7710103042423725\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7864828147490819\n",
            "\n",
            "Total train accuarcy: 73.50068212824011\n",
            "Total train loss: 0.7800660052224604\n",
            "\n",
            "[ Test epoch: 16 ]\n",
            "\n",
            "Test accuarcy: 72.79093455564043\n",
            "Test average loss: 0.8202113350910227\n",
            "\n",
            "[ Train epoch: 17 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7428169637918473\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7502885743975639\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7600144708156585\n",
            "\n",
            "Total train accuarcy: 74.37380627557981\n",
            "Total train loss: 0.7538813564269746\n",
            "\n",
            "[ Test epoch: 17 ]\n",
            "\n",
            "Test accuarcy: 70.6009676597912\n",
            "Test average loss: 0.8544843049981686\n",
            "\n",
            "[ Train epoch: 18 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7718237141271432\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7264396082361539\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7553827275335788\n",
            "\n",
            "Total train accuarcy: 74.33015006821283\n",
            "Total train loss: 0.7520930438609648\n",
            "\n",
            "[ Test epoch: 18 ]\n",
            "\n",
            "Test accuarcy: 71.00840336134453\n",
            "Test average loss: 0.8665990782275705\n",
            "\n",
            "[ Train epoch: 19 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7368390250205994\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.736533947835366\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7178194586684307\n",
            "\n",
            "Total train accuarcy: 74.95225102319236\n",
            "Total train loss: 0.7399976629744337\n",
            "\n",
            "[ Test epoch: 19 ]\n",
            "\n",
            "Test accuarcy: 72.01426024955437\n",
            "Test average loss: 0.8410638031187465\n",
            "\n",
            "[ Train epoch: 20 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7294222978750865\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7303247339030107\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7210752474019925\n",
            "\n",
            "Total train accuarcy: 75.26330150068213\n",
            "Total train loss: 0.7269972878301851\n",
            "\n",
            "[ Test epoch: 20 ]\n",
            "\n",
            "Test accuarcy: 74.84084542908073\n",
            "Test average loss: 0.7612333078460878\n",
            "\n",
            "[ Train epoch: 21 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7145961385468642\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7083814069628716\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6870157459378242\n",
            "\n",
            "Total train accuarcy: 75.95634379263302\n",
            "Total train loss: 0.7119818106423705\n",
            "\n",
            "[ Test epoch: 21 ]\n",
            "\n",
            "Test accuarcy: 74.34428316781258\n",
            "Test average loss: 0.7749522315630359\n",
            "\n",
            "[ Train epoch: 22 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.7132045640051365\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.7129895160595576\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7112094811101755\n",
            "\n",
            "Total train accuarcy: 75.80900409276944\n",
            "Total train loss: 0.7036674846789391\n",
            "\n",
            "[ Test epoch: 22 ]\n",
            "\n",
            "Test accuarcy: 72.19251336898395\n",
            "Test average loss: 0.823741173216136\n",
            "\n",
            "[ Train epoch: 23 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6696925111363331\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6763598971068859\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.7004320161044597\n",
            "\n",
            "Total train accuarcy: 76.6712141882674\n",
            "Total train loss: 0.6877223497897023\n",
            "\n",
            "[ Test epoch: 23 ]\n",
            "\n",
            "Test accuarcy: 74.81538069773364\n",
            "Test average loss: 0.7573348160400653\n",
            "\n",
            "[ Train epoch: 24 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6935515674948692\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6814966436723868\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6790295956035455\n",
            "\n",
            "Total train accuarcy: 76.4856753069577\n",
            "Total train loss: 0.6882749301808875\n",
            "\n",
            "[ Test epoch: 24 ]\n",
            "\n",
            "Test accuarcy: 74.70078940667176\n",
            "Test average loss: 0.7723168039736158\n",
            "\n",
            "[ Train epoch: 25 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6775394595662753\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6785595675309499\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6662153322249651\n",
            "\n",
            "Total train accuarcy: 76.72032742155525\n",
            "Total train loss: 0.6829652106470671\n",
            "\n",
            "[ Test epoch: 25 ]\n",
            "\n",
            "Test accuarcy: 76.2541380188439\n",
            "Test average loss: 0.7357908190662409\n",
            "\n",
            "[ Train epoch: 26 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6676735080778599\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6592691701650619\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6672388074795405\n",
            "\n",
            "Total train accuarcy: 77.46793997271487\n",
            "Total train loss: 0.6660840253867404\n",
            "\n",
            "[ Test epoch: 26 ]\n",
            "\n",
            "Test accuarcy: 74.01324166030048\n",
            "Test average loss: 0.7720679128637382\n",
            "\n",
            "[ Train epoch: 27 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6570246759553751\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6628248499333859\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6859038925667604\n",
            "\n",
            "Total train accuarcy: 77.51705320600273\n",
            "Total train loss: 0.6650794393677154\n",
            "\n",
            "[ Test epoch: 27 ]\n",
            "\n",
            "Test accuarcy: 73.77132671250318\n",
            "Test average loss: 0.7718995449317205\n",
            "\n",
            "[ Train epoch: 28 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6363972990463177\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6472463535269102\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6589959002037843\n",
            "\n",
            "Total train accuarcy: 77.83356070941338\n",
            "Total train loss: 0.6540316921410136\n",
            "\n",
            "[ Test epoch: 28 ]\n",
            "\n",
            "Test accuarcy: 73.33842627960276\n",
            "Test average loss: 0.8004043748993009\n",
            "\n",
            "[ Train epoch: 29 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6277668778598309\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6315117343266805\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6671966359515985\n",
            "\n",
            "Total train accuarcy: 78.09549795361528\n",
            "Total train loss: 0.6428067814136155\n",
            "\n",
            "[ Test epoch: 29 ]\n",
            "\n",
            "Test accuarcy: 75.71937866055514\n",
            "Test average loss: 0.7261943486338963\n",
            "\n",
            "[ Train epoch: 30 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.630312692373991\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6356963064273199\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6680599461992581\n",
            "\n",
            "Total train accuarcy: 78.12278308321964\n",
            "Total train loss: 0.6424585324021326\n",
            "\n",
            "[ Test epoch: 30 ]\n",
            "\n",
            "Test accuarcy: 75.99949070537306\n",
            "Test average loss: 0.7249457676820988\n",
            "\n",
            "[ Train epoch: 31 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.628110333532095\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6309966486692429\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6025304786860943\n",
            "\n",
            "Total train accuarcy: 78.80491132332878\n",
            "Total train loss: 0.6275817503076677\n",
            "\n",
            "[ Test epoch: 31 ]\n",
            "\n",
            "Test accuarcy: 75.59205500381971\n",
            "Test average loss: 0.7174016886073982\n",
            "\n",
            "[ Train epoch: 32 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6081576191385587\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.60389768751959\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6386654962102573\n",
            "\n",
            "Total train accuarcy: 78.88130968622102\n",
            "Total train loss: 0.6237632993277142\n",
            "\n",
            "[ Test epoch: 32 ]\n",
            "\n",
            "Test accuarcy: 75.54112554112554\n",
            "Test average loss: 0.7401093006467868\n",
            "\n",
            "[ Train epoch: 33 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6138866768280665\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6294048122068246\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6305172753334045\n",
            "\n",
            "Total train accuarcy: 78.54843110504775\n",
            "Total train loss: 0.6259740405239776\n",
            "\n",
            "[ Test epoch: 33 ]\n",
            "\n",
            "Test accuarcy: 75.38833715304304\n",
            "Test average loss: 0.7327869039317194\n",
            "\n",
            "[ Train epoch: 34 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.629706176345547\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5841980593899886\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6104802160213391\n",
            "\n",
            "Total train accuarcy: 79.01227830832197\n",
            "Total train loss: 0.6179587698783758\n",
            "\n",
            "[ Test epoch: 34 ]\n",
            "\n",
            "Test accuarcy: 75.0445632798574\n",
            "Test average loss: 0.7409955394850729\n",
            "\n",
            "[ Train epoch: 35 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.6037689998249213\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6191427378356457\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6178658863405386\n",
            "\n",
            "Total train accuarcy: 78.78308321964529\n",
            "Total train loss: 0.6153348989355627\n",
            "\n",
            "[ Test epoch: 35 ]\n",
            "\n",
            "Test accuarcy: 75.24828113063407\n",
            "Test average loss: 0.7351507957258439\n",
            "\n",
            "[ Train epoch: 36 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5847980821381012\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6246653227508068\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5965077842275301\n",
            "\n",
            "Total train accuarcy: 79.24147339699863\n",
            "Total train loss: 0.6069876312572399\n",
            "\n",
            "[ Test epoch: 36 ]\n",
            "\n",
            "Test accuarcy: 75.12095747389866\n",
            "Test average loss: 0.7372813543452751\n",
            "\n",
            "[ Train epoch: 37 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5828291892508666\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.6095702396333218\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6080131109555562\n",
            "\n",
            "Total train accuarcy: 79.0231923601637\n",
            "Total train loss: 0.6071056420240311\n",
            "\n",
            "[ Test epoch: 37 ]\n",
            "\n",
            "Test accuarcy: 75.6175197351668\n",
            "Test average loss: 0.7230129119107412\n",
            "\n",
            "[ Train epoch: 38 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5806440505882104\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.596505343541503\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6031705477337043\n",
            "\n",
            "Total train accuarcy: 79.65075034106412\n",
            "Total train loss: 0.5977832644748751\n",
            "\n",
            "[ Test epoch: 38 ]\n",
            "\n",
            "Test accuarcy: 75.78304048892284\n",
            "Test average loss: 0.7157766639791778\n",
            "\n",
            "[ Train epoch: 39 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5795792289078235\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5842304187019666\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6020149463911851\n",
            "\n",
            "Total train accuarcy: 79.62346521145976\n",
            "Total train loss: 0.5931203653467055\n",
            "\n",
            "[ Test epoch: 39 ]\n",
            "\n",
            "Test accuarcy: 75.64298446651388\n",
            "Test average loss: 0.7214261706000433\n",
            "\n",
            "[ Train epoch: 40 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5566225028286378\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5984501937776804\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5991608823339144\n",
            "\n",
            "Total train accuarcy: 79.98362892223739\n",
            "Total train loss: 0.5908676151179726\n",
            "\n",
            "[ Test epoch: 40 ]\n",
            "\n",
            "Test accuarcy: 74.64985994397759\n",
            "Test average loss: 0.7649163142180492\n",
            "\n",
            "[ Train epoch: 41 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5496649876733621\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5934540718793869\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5850843341648578\n",
            "\n",
            "Total train accuarcy: 80.49113233287858\n",
            "Total train loss: 0.5769426615658438\n",
            "\n",
            "[ Test epoch: 41 ]\n",
            "\n",
            "Test accuarcy: 75.0445632798574\n",
            "Test average loss: 0.7341749533070081\n",
            "\n",
            "[ Train epoch: 42 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5527319717158874\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5979747066895167\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5659429943064849\n",
            "\n",
            "Total train accuarcy: 80.55115961800819\n",
            "Total train loss: 0.5745239032084614\n",
            "\n",
            "[ Test epoch: 42 ]\n",
            "\n",
            "Test accuarcy: 75.23554876496053\n",
            "Test average loss: 0.7526507149668612\n",
            "\n",
            "[ Train epoch: 43 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5497065785030524\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5598324096699556\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5875366531312466\n",
            "\n",
            "Total train accuarcy: 80.71487039563438\n",
            "Total train loss: 0.5707100264298979\n",
            "\n",
            "[ Test epoch: 43 ]\n",
            "\n",
            "Test accuarcy: 76.64884135472371\n",
            "Test average loss: 0.6949342589222214\n",
            "\n",
            "[ Train epoch: 44 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5515487227092186\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.572500626432399\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.6064638598263263\n",
            "\n",
            "Total train accuarcy: 80.27285129604365\n",
            "Total train loss: 0.5734971774576222\n",
            "\n",
            "[ Test epoch: 44 ]\n",
            "\n",
            "Test accuarcy: 77.34912146676852\n",
            "Test average loss: 0.6765497126853636\n",
            "\n",
            "[ Train epoch: 45 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5487367447714011\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5810569595793883\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5534464414417744\n",
            "\n",
            "Total train accuarcy: 80.68212824010914\n",
            "Total train loss: 0.5660503665294114\n",
            "\n",
            "[ Test epoch: 45 ]\n",
            "\n",
            "Test accuarcy: 75.84670231729055\n",
            "Test average loss: 0.7264306229935644\n",
            "\n",
            "[ Train epoch: 46 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5564510142058134\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.545143995086352\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5488744279990594\n",
            "\n",
            "Total train accuarcy: 81.19508867667122\n",
            "Total train loss: 0.5574862752077259\n",
            "\n",
            "[ Test epoch: 46 ]\n",
            "\n",
            "Test accuarcy: 73.18563789152024\n",
            "Test average loss: 0.7863095766711077\n",
            "\n",
            "[ Train epoch: 47 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5376907344410817\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5788169612238805\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.561666482364138\n",
            "\n",
            "Total train accuarcy: 80.69849931787176\n",
            "Total train loss: 0.558985487926901\n",
            "\n",
            "[ Test epoch: 47 ]\n",
            "\n",
            "Test accuarcy: 76.39419404125286\n",
            "Test average loss: 0.704920329098531\n",
            "\n",
            "[ Train epoch: 48 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.555811517201364\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5333810712148745\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5818955072015524\n",
            "\n",
            "Total train accuarcy: 80.93860845839018\n",
            "Total train loss: 0.5581023910848414\n",
            "\n",
            "[ Test epoch: 48 ]\n",
            "\n",
            "Test accuarcy: 76.24140565317036\n",
            "Test average loss: 0.7117760185675805\n",
            "\n",
            "[ Train epoch: 49 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5309376249959071\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.547514853750666\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5563454239815474\n",
            "\n",
            "Total train accuarcy: 81.31514324693042\n",
            "Total train loss: 0.5486887705952813\n",
            "\n",
            "[ Test epoch: 49 ]\n",
            "\n",
            "Test accuarcy: 76.83982683982684\n",
            "Test average loss: 0.6945310816654363\n",
            "\n",
            "[ Train epoch: 50 ]\n",
            "\n",
            "Current batch: 300\n",
            "Average train loss of recent 300 batches: 0.5266907844195763\n",
            "\n",
            "Current batch: 600\n",
            "Average train loss of recent 300 batches: 0.5495866756637892\n",
            "\n",
            "Current batch: 900\n",
            "Average train loss of recent 300 batches: 0.5222055202474197\n",
            "\n",
            "Total train accuarcy: 81.5443383356071\n",
            "Total train loss: 0.5413340623011036\n",
            "\n",
            "[ Test epoch: 50 ]\n",
            "\n",
            "Test accuarcy: 77.4891774891775\n",
            "Test average loss: 0.673409035960459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "dNikljYS19pP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss, accuracy 추이 확인 with plt.plot\n",
        "plt.plot(range(num_epoch), seq_train_loss, label=\"Train loss\")\n",
        "plt.plot(range(num_epoch), seq_test_loss, label=\"Test loss\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZwJHGOY9Czzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "f5994bf8-0d00-4492-d697-ffda9e49e4fb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(num_epoch), seq_train_acc, label=\"Train Accuracy\")\n",
        "plt.plot(range(num_epoch), seq_test_acc, label=\"Test Accuracy\")\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cp9e_BTt2FiA",
        "outputId": "492a0933-fac7-4d5d-a338-fba2b319e064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6kZr7JA2HSr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}